{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 26/09/2023 at 12:11:37.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/protanjung/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/protanjung/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/protanjung/icar-ng-data/model\n",
      "Model file: pixel2cm_model.pt\n",
      "Model path: /home/protanjung/icar-ng-data/model/pixel2cm_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'pixel2cm_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([ 30., 260.]), max_x: tensor([1268.,  692.])\n",
      "min_y: tensor([   0., -300.]), max_y: tensor([450., 300.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 0:2].values\n",
    "        y = self.dataframe.iloc[:, 2:4].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 91\n",
      "dataset_test: 23\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.fc5 = nn.Linear(16, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/protanjung/icar-ng-data/model/pixel2cm_model.pt\n",
      "Loaded model: /home/protanjung/icar-ng-data/model/pixel2cm_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 82969.531250, test_loss: 31843.570312 (Saved)\n",
      "epoch: 100, train_loss: 70820.970703, test_loss: 27318.500000 (Saved)\n",
      "epoch: 200, train_loss: 62231.236328, test_loss: 24863.410156 (Saved)\n",
      "epoch: 300, train_loss: 62416.578125, test_loss: 22909.375000 (Saved)\n",
      "epoch: 400, train_loss: 53197.500000, test_loss: 21184.654297 (Saved)\n",
      "epoch: 500, train_loss: 57136.605469, test_loss: 19707.726562 (Saved)\n",
      "epoch: 600, train_loss: 49580.169922, test_loss: 18230.568359 (Saved)\n",
      "epoch: 700, train_loss: 49283.458984, test_loss: 17022.142578 (Saved)\n",
      "epoch: 800, train_loss: 45944.542969, test_loss: 15972.717773 (Saved)\n",
      "epoch: 900, train_loss: 45616.275391, test_loss: 14928.288086 (Saved)\n",
      "epoch: 1000, train_loss: 40483.275391, test_loss: 13960.291016 (Saved)\n",
      "epoch: 1100, train_loss: 35591.335938, test_loss: 13134.852539 (Saved)\n",
      "epoch: 1200, train_loss: 36202.275391, test_loss: 12441.505859 (Saved)\n",
      "epoch: 1300, train_loss: 31636.343750, test_loss: 11675.589844 (Saved)\n",
      "epoch: 1400, train_loss: 33551.501953, test_loss: 10957.427734 (Saved)\n",
      "epoch: 1500, train_loss: 30398.311523, test_loss: 10338.169922 (Saved)\n",
      "epoch: 1600, train_loss: 27250.782227, test_loss: 9737.177734 (Saved)\n",
      "epoch: 1700, train_loss: 27959.766602, test_loss: 9160.365234 (Saved)\n",
      "epoch: 1800, train_loss: 27361.310547, test_loss: 8700.439453 (Saved)\n",
      "epoch: 1900, train_loss: 26471.276367, test_loss: 8116.990723 (Saved)\n",
      "epoch: 2000, train_loss: 24735.843750, test_loss: 7673.564453 (Saved)\n",
      "epoch: 2100, train_loss: 21237.608398, test_loss: 7188.788086 (Saved)\n",
      "epoch: 2200, train_loss: 24441.250000, test_loss: 6737.936035 (Saved)\n",
      "epoch: 2300, train_loss: 19165.579102, test_loss: 6312.739258 (Saved)\n",
      "epoch: 2400, train_loss: 18319.085938, test_loss: 5881.766602 (Saved)\n",
      "epoch: 2500, train_loss: 18309.934570, test_loss: 5478.253418 (Saved)\n",
      "epoch: 2600, train_loss: 17446.409180, test_loss: 5120.181641 (Saved)\n",
      "epoch: 2700, train_loss: 16119.037109, test_loss: 4718.870117 (Saved)\n",
      "epoch: 2800, train_loss: 14702.153809, test_loss: 4338.417480 (Saved)\n",
      "epoch: 2900, train_loss: 13319.682617, test_loss: 3991.427490 (Saved)\n",
      "epoch: 3000, train_loss: 12442.110352, test_loss: 3655.414551 (Saved)\n",
      "epoch: 3100, train_loss: 10889.463379, test_loss: 3314.095215 (Saved)\n",
      "epoch: 3200, train_loss: 10924.860352, test_loss: 2987.287354 (Saved)\n",
      "epoch: 3300, train_loss: 9801.675781, test_loss: 2687.424316 (Saved)\n",
      "epoch: 3400, train_loss: 8947.733887, test_loss: 2402.381348 (Saved)\n",
      "epoch: 3500, train_loss: 8967.133789, test_loss: 2136.370361 (Saved)\n",
      "epoch: 3600, train_loss: 7794.464355, test_loss: 1888.764771 (Saved)\n",
      "epoch: 3700, train_loss: 7239.966064, test_loss: 1668.272461 (Saved)\n",
      "epoch: 3800, train_loss: 6249.156006, test_loss: 1468.532959 (Saved)\n",
      "epoch: 3900, train_loss: 5542.920898, test_loss: 1286.217529 (Saved)\n",
      "epoch: 4000, train_loss: 5831.965576, test_loss: 1137.893799 (Saved)\n",
      "epoch: 4100, train_loss: 4821.770020, test_loss: 1004.326782 (Saved)\n",
      "epoch: 4200, train_loss: 4579.049316, test_loss: 892.798767 (Saved)\n",
      "epoch: 4300, train_loss: 4319.432373, test_loss: 797.795166 (Saved)\n",
      "epoch: 4400, train_loss: 3562.440063, test_loss: 724.747314 (Saved)\n",
      "epoch: 4500, train_loss: 3604.656982, test_loss: 665.397278 (Saved)\n",
      "epoch: 4600, train_loss: 3304.597412, test_loss: 624.783997 (Saved)\n",
      "epoch: 4700, train_loss: 2857.384033, test_loss: 588.034058 (Saved)\n",
      "epoch: 4800, train_loss: 2753.681885, test_loss: 570.862427 (Saved)\n",
      "epoch: 4900, train_loss: 2848.369751, test_loss: 552.673523 (Saved)\n",
      "epoch: 5000, train_loss: 2401.471436, test_loss: 544.464905 (Saved)\n",
      "epoch: 5100, train_loss: 2725.842407, test_loss: 536.189697 (Saved)\n",
      "epoch: 5200, train_loss: 2355.258423, test_loss: 541.362183\n",
      "epoch: 5300, train_loss: 2326.642883, test_loss: 544.111145\n",
      "epoch: 5400, train_loss: 2223.699219, test_loss: 538.340515\n",
      "epoch: 5500, train_loss: 2295.510986, test_loss: 530.221497 (Saved)\n",
      "epoch: 5600, train_loss: 2096.436035, test_loss: 528.191406 (Saved)\n",
      "epoch: 5700, train_loss: 2140.073425, test_loss: 532.390320\n",
      "epoch: 5800, train_loss: 2499.198486, test_loss: 519.127747 (Saved)\n",
      "epoch: 5900, train_loss: 2238.486694, test_loss: 518.298645 (Saved)\n",
      "epoch: 6000, train_loss: 2279.494629, test_loss: 516.328369 (Saved)\n",
      "epoch: 6100, train_loss: 2118.350769, test_loss: 517.046387\n",
      "epoch: 6200, train_loss: 2369.949829, test_loss: 496.933594 (Saved)\n",
      "epoch: 6300, train_loss: 2112.022461, test_loss: 501.478180\n",
      "epoch: 6400, train_loss: 2382.229187, test_loss: 480.461578 (Saved)\n",
      "epoch: 6500, train_loss: 2314.611816, test_loss: 479.705841 (Saved)\n",
      "epoch: 6600, train_loss: 2021.326233, test_loss: 460.945801 (Saved)\n",
      "epoch: 6700, train_loss: 2023.474426, test_loss: 457.899963 (Saved)\n",
      "epoch: 6800, train_loss: 2156.854614, test_loss: 451.371307 (Saved)\n",
      "epoch: 6900, train_loss: 1788.508118, test_loss: 450.456696 (Saved)\n",
      "epoch: 7000, train_loss: 1952.848633, test_loss: 437.281342 (Saved)\n",
      "epoch: 7100, train_loss: 2014.344482, test_loss: 440.597961\n",
      "epoch: 7200, train_loss: 1899.853027, test_loss: 428.127563 (Saved)\n",
      "epoch: 7300, train_loss: 1600.541443, test_loss: 430.847717\n",
      "epoch: 7400, train_loss: 2118.840820, test_loss: 418.661438 (Saved)\n",
      "epoch: 7500, train_loss: 1636.271912, test_loss: 420.087952\n",
      "epoch: 7600, train_loss: 1879.668152, test_loss: 413.655396 (Saved)\n",
      "epoch: 7700, train_loss: 1466.760925, test_loss: 400.731659 (Saved)\n",
      "epoch: 7800, train_loss: 1600.832275, test_loss: 400.691071 (Saved)\n",
      "epoch: 7900, train_loss: 1584.467651, test_loss: 389.153412 (Saved)\n",
      "epoch: 8000, train_loss: 1601.509888, test_loss: 382.697784 (Saved)\n",
      "epoch: 8100, train_loss: 1709.419312, test_loss: 376.692444 (Saved)\n",
      "epoch: 8200, train_loss: 1712.162109, test_loss: 375.218445 (Saved)\n",
      "epoch: 8300, train_loss: 1499.280518, test_loss: 366.223480 (Saved)\n",
      "epoch: 8400, train_loss: 1428.037292, test_loss: 367.302032\n",
      "epoch: 8500, train_loss: 1461.485535, test_loss: 361.237915 (Saved)\n",
      "epoch: 8600, train_loss: 1481.658691, test_loss: 348.530029 (Saved)\n",
      "epoch: 8700, train_loss: 1539.414246, test_loss: 339.504211 (Saved)\n",
      "epoch: 8800, train_loss: 1594.720520, test_loss: 342.009277\n",
      "epoch: 8900, train_loss: 1432.452881, test_loss: 337.626648 (Saved)\n",
      "epoch: 9000, train_loss: 1624.405457, test_loss: 340.721375\n",
      "epoch: 9100, train_loss: 1401.158264, test_loss: 324.234711 (Saved)\n",
      "epoch: 9200, train_loss: 1345.073730, test_loss: 324.548676\n",
      "epoch: 9300, train_loss: 1470.202637, test_loss: 329.376465\n",
      "epoch: 9400, train_loss: 1260.757385, test_loss: 321.921173 (Saved)\n",
      "epoch: 9500, train_loss: 1397.692810, test_loss: 321.618774 (Saved)\n",
      "epoch: 9600, train_loss: 1195.101318, test_loss: 307.905548 (Saved)\n",
      "epoch: 9700, train_loss: 1580.673615, test_loss: 312.832733\n",
      "epoch: 9800, train_loss: 1491.758057, test_loss: 301.211365 (Saved)\n",
      "epoch: 9900, train_loss: 1486.701294, test_loss: 299.596588 (Saved)\n",
      "epoch: 10000, train_loss: 1277.673096, test_loss: 295.305939 (Saved)\n",
      "epoch: 10100, train_loss: 1334.013489, test_loss: 293.491272 (Saved)\n",
      "epoch: 10200, train_loss: 1285.293396, test_loss: 293.114990 (Saved)\n",
      "epoch: 10300, train_loss: 1340.884216, test_loss: 285.238190 (Saved)\n",
      "epoch: 10400, train_loss: 1279.563599, test_loss: 289.515625\n",
      "epoch: 10500, train_loss: 1084.908905, test_loss: 280.348755 (Saved)\n",
      "epoch: 10600, train_loss: 1312.963013, test_loss: 280.334106 (Saved)\n",
      "epoch: 10700, train_loss: 1129.219971, test_loss: 269.484406 (Saved)\n",
      "epoch: 10800, train_loss: 1222.778381, test_loss: 273.129425\n",
      "epoch: 10900, train_loss: 1247.058350, test_loss: 269.223969 (Saved)\n",
      "epoch: 11000, train_loss: 1238.895935, test_loss: 260.938751 (Saved)\n",
      "epoch: 11100, train_loss: 1072.373413, test_loss: 260.255981 (Saved)\n",
      "epoch: 11200, train_loss: 1085.727600, test_loss: 260.215454 (Saved)\n",
      "epoch: 11300, train_loss: 1084.456421, test_loss: 249.749008 (Saved)\n",
      "epoch: 11400, train_loss: 1000.158569, test_loss: 258.686707\n",
      "epoch: 11500, train_loss: 1173.028778, test_loss: 253.591553\n",
      "epoch: 11600, train_loss: 1037.630920, test_loss: 244.534164 (Saved)\n",
      "epoch: 11700, train_loss: 1233.217590, test_loss: 250.023422\n",
      "epoch: 11800, train_loss: 1088.173767, test_loss: 244.683640\n",
      "epoch: 11900, train_loss: 962.854523, test_loss: 244.326172 (Saved)\n",
      "epoch: 12000, train_loss: 985.211212, test_loss: 234.311356 (Saved)\n",
      "epoch: 12100, train_loss: 1042.616058, test_loss: 236.350372\n",
      "epoch: 12200, train_loss: 921.278412, test_loss: 231.652557 (Saved)\n",
      "epoch: 12300, train_loss: 948.289215, test_loss: 226.567657 (Saved)\n",
      "epoch: 12400, train_loss: 1053.027588, test_loss: 224.521530 (Saved)\n",
      "epoch: 12500, train_loss: 950.526794, test_loss: 219.657150 (Saved)\n",
      "epoch: 12600, train_loss: 967.573853, test_loss: 225.840988\n",
      "epoch: 12700, train_loss: 962.620361, test_loss: 215.695892 (Saved)\n",
      "epoch: 12800, train_loss: 859.186066, test_loss: 213.543335 (Saved)\n",
      "epoch: 12900, train_loss: 1015.595581, test_loss: 209.035248 (Saved)\n",
      "epoch: 13000, train_loss: 926.306183, test_loss: 213.237259\n",
      "epoch: 13100, train_loss: 772.560547, test_loss: 209.228683\n",
      "epoch: 13200, train_loss: 841.673065, test_loss: 206.879990 (Saved)\n",
      "epoch: 13300, train_loss: 1019.603729, test_loss: 201.767410 (Saved)\n",
      "epoch: 13400, train_loss: 762.001953, test_loss: 195.846573 (Saved)\n",
      "epoch: 13500, train_loss: 960.102722, test_loss: 201.281738\n",
      "epoch: 13600, train_loss: 792.536072, test_loss: 196.274353\n",
      "epoch: 13700, train_loss: 783.700104, test_loss: 196.262833\n",
      "epoch: 13800, train_loss: 880.014771, test_loss: 190.040421 (Saved)\n",
      "epoch: 13900, train_loss: 900.779541, test_loss: 186.533615 (Saved)\n",
      "epoch: 14000, train_loss: 697.086182, test_loss: 181.248749 (Saved)\n",
      "epoch: 14100, train_loss: 746.613739, test_loss: 183.983505\n",
      "epoch: 14200, train_loss: 866.165588, test_loss: 185.160461\n",
      "epoch: 14300, train_loss: 685.544983, test_loss: 180.542145 (Saved)\n",
      "epoch: 14400, train_loss: 722.684235, test_loss: 177.474014 (Saved)\n",
      "epoch: 14500, train_loss: 695.726257, test_loss: 173.352127 (Saved)\n",
      "epoch: 14600, train_loss: 809.400726, test_loss: 168.542923 (Saved)\n",
      "epoch: 14700, train_loss: 728.732941, test_loss: 166.943253 (Saved)\n",
      "epoch: 14800, train_loss: 750.501892, test_loss: 169.822678\n",
      "epoch: 14900, train_loss: 689.833008, test_loss: 166.044067 (Saved)\n",
      "epoch: 15000, train_loss: 693.794830, test_loss: 161.050995 (Saved)\n",
      "epoch: 15100, train_loss: 666.067108, test_loss: 159.305130 (Saved)\n",
      "epoch: 15200, train_loss: 638.497375, test_loss: 154.710968 (Saved)\n",
      "epoch: 15300, train_loss: 664.700562, test_loss: 160.799316\n",
      "epoch: 15400, train_loss: 646.693939, test_loss: 156.534546\n",
      "epoch: 15500, train_loss: 705.134766, test_loss: 151.522003 (Saved)\n",
      "epoch: 15600, train_loss: 617.951843, test_loss: 148.892563 (Saved)\n",
      "epoch: 15700, train_loss: 731.159424, test_loss: 146.678253 (Saved)\n",
      "epoch: 15800, train_loss: 632.729523, test_loss: 146.417267 (Saved)\n",
      "epoch: 15900, train_loss: 588.844360, test_loss: 142.901779 (Saved)\n",
      "epoch: 16000, train_loss: 588.097198, test_loss: 142.941925\n",
      "epoch: 16100, train_loss: 621.414246, test_loss: 137.880997 (Saved)\n",
      "epoch: 16200, train_loss: 512.607132, test_loss: 136.702362 (Saved)\n",
      "epoch: 16300, train_loss: 553.135864, test_loss: 135.461655 (Saved)\n",
      "epoch: 16400, train_loss: 540.319214, test_loss: 134.886292 (Saved)\n",
      "epoch: 16500, train_loss: 611.352081, test_loss: 134.738159 (Saved)\n",
      "epoch: 16600, train_loss: 506.712173, test_loss: 128.808548 (Saved)\n",
      "epoch: 16700, train_loss: 523.231506, test_loss: 124.938545 (Saved)\n",
      "epoch: 16800, train_loss: 531.637054, test_loss: 129.867737\n",
      "epoch: 16900, train_loss: 556.048615, test_loss: 123.884727 (Saved)\n",
      "epoch: 17000, train_loss: 506.618607, test_loss: 125.186974\n",
      "epoch: 17100, train_loss: 569.628204, test_loss: 123.287071 (Saved)\n",
      "epoch: 17200, train_loss: 490.131149, test_loss: 120.707939 (Saved)\n",
      "epoch: 17300, train_loss: 466.042587, test_loss: 120.436401 (Saved)\n",
      "epoch: 17400, train_loss: 515.424377, test_loss: 121.318932\n",
      "epoch: 17500, train_loss: 468.204620, test_loss: 116.676933 (Saved)\n",
      "epoch: 17600, train_loss: 466.862823, test_loss: 114.111153 (Saved)\n",
      "epoch: 17700, train_loss: 505.724258, test_loss: 116.429283\n",
      "epoch: 17800, train_loss: 485.432495, test_loss: 110.440239 (Saved)\n",
      "epoch: 17900, train_loss: 425.722321, test_loss: 110.188766 (Saved)\n",
      "epoch: 18000, train_loss: 478.080154, test_loss: 109.065918 (Saved)\n",
      "epoch: 18100, train_loss: 507.636871, test_loss: 109.696373\n",
      "epoch: 18200, train_loss: 464.677780, test_loss: 110.209419\n",
      "epoch: 18300, train_loss: 459.912582, test_loss: 106.481384 (Saved)\n",
      "epoch: 18400, train_loss: 433.825394, test_loss: 104.885376 (Saved)\n",
      "epoch: 18500, train_loss: 443.980743, test_loss: 103.398628 (Saved)\n",
      "epoch: 18600, train_loss: 400.146805, test_loss: 102.923279 (Saved)\n",
      "epoch: 18700, train_loss: 406.139771, test_loss: 102.852692 (Saved)\n",
      "epoch: 18800, train_loss: 397.871964, test_loss: 99.299011 (Saved)\n",
      "epoch: 18900, train_loss: 404.141953, test_loss: 99.668694\n",
      "epoch: 19000, train_loss: 379.536087, test_loss: 97.465942 (Saved)\n",
      "epoch: 19100, train_loss: 399.110641, test_loss: 95.541580 (Saved)\n",
      "epoch: 19200, train_loss: 381.540405, test_loss: 96.905777\n",
      "epoch: 19300, train_loss: 381.690018, test_loss: 95.637039\n",
      "epoch: 19400, train_loss: 363.950302, test_loss: 92.358376 (Saved)\n",
      "epoch: 19500, train_loss: 446.762360, test_loss: 92.003822 (Saved)\n",
      "epoch: 19600, train_loss: 316.145378, test_loss: 90.780235 (Saved)\n",
      "epoch: 19700, train_loss: 358.361816, test_loss: 89.267921 (Saved)\n",
      "epoch: 19800, train_loss: 381.254532, test_loss: 89.370285\n",
      "epoch: 19900, train_loss: 420.442398, test_loss: 86.152390 (Saved)\n",
      "epoch: 20000, train_loss: 359.623154, test_loss: 87.462158\n",
      "epoch: 20100, train_loss: 324.375778, test_loss: 85.662239 (Saved)\n",
      "epoch: 20200, train_loss: 368.420563, test_loss: 85.757576\n",
      "epoch: 20300, train_loss: 404.906815, test_loss: 81.696663 (Saved)\n",
      "epoch: 20400, train_loss: 376.458374, test_loss: 83.495361\n",
      "epoch: 20500, train_loss: 315.173325, test_loss: 80.520340 (Saved)\n",
      "epoch: 20600, train_loss: 361.225891, test_loss: 83.578484\n",
      "epoch: 20700, train_loss: 336.992325, test_loss: 82.121819\n",
      "epoch: 20800, train_loss: 312.512772, test_loss: 82.543182\n",
      "epoch: 20900, train_loss: 332.389740, test_loss: 80.871216\n",
      "epoch: 21000, train_loss: 356.118637, test_loss: 81.465363\n",
      "epoch: 21100, train_loss: 308.007416, test_loss: 79.225998 (Saved)\n",
      "epoch: 21200, train_loss: 347.407974, test_loss: 83.182137\n",
      "epoch: 21300, train_loss: 288.557487, test_loss: 77.263649 (Saved)\n",
      "epoch: 21400, train_loss: 314.272736, test_loss: 76.578850 (Saved)\n",
      "epoch: 21500, train_loss: 327.907974, test_loss: 76.502495 (Saved)\n",
      "epoch: 21600, train_loss: 322.461441, test_loss: 77.948082\n",
      "epoch: 21700, train_loss: 311.065643, test_loss: 76.158852 (Saved)\n",
      "epoch: 21800, train_loss: 330.379608, test_loss: 75.883125 (Saved)\n",
      "epoch: 21900, train_loss: 340.884514, test_loss: 74.790245 (Saved)\n",
      "epoch: 22000, train_loss: 300.543762, test_loss: 74.815811\n",
      "epoch: 22100, train_loss: 289.276550, test_loss: 75.554558\n",
      "epoch: 22200, train_loss: 267.708862, test_loss: 72.733109 (Saved)\n",
      "epoch: 22300, train_loss: 280.795235, test_loss: 74.343582\n",
      "epoch: 22400, train_loss: 317.995193, test_loss: 72.708412 (Saved)\n",
      "epoch: 22500, train_loss: 292.589478, test_loss: 73.274139\n",
      "epoch: 22600, train_loss: 269.141411, test_loss: 69.251678 (Saved)\n",
      "epoch: 22700, train_loss: 286.306335, test_loss: 74.479668\n",
      "epoch: 22800, train_loss: 265.282433, test_loss: 71.397896\n",
      "epoch: 22900, train_loss: 248.878670, test_loss: 71.427971\n",
      "epoch: 23000, train_loss: 254.985283, test_loss: 71.694580\n",
      "epoch: 23100, train_loss: 230.849216, test_loss: 71.528450\n",
      "epoch: 23200, train_loss: 250.225800, test_loss: 72.340813\n",
      "epoch: 23300, train_loss: 269.370712, test_loss: 70.557022\n",
      "epoch: 23400, train_loss: 271.924164, test_loss: 71.779121\n",
      "epoch: 23500, train_loss: 254.372902, test_loss: 71.099724\n",
      "epoch: 23600, train_loss: 258.796333, test_loss: 68.902451 (Saved)\n",
      "epoch: 23700, train_loss: 241.048561, test_loss: 70.473946\n",
      "epoch: 23800, train_loss: 264.900620, test_loss: 67.437332 (Saved)\n",
      "epoch: 23900, train_loss: 292.423019, test_loss: 69.501289\n",
      "epoch: 24000, train_loss: 242.273918, test_loss: 68.358391\n",
      "epoch: 24100, train_loss: 273.491646, test_loss: 68.879059\n",
      "epoch: 24200, train_loss: 269.673645, test_loss: 68.022835\n",
      "epoch: 24300, train_loss: 291.625748, test_loss: 69.136932\n",
      "epoch: 24400, train_loss: 229.928635, test_loss: 69.818161\n",
      "epoch: 24500, train_loss: 227.763664, test_loss: 67.498352\n",
      "epoch: 24600, train_loss: 244.668365, test_loss: 67.995361\n",
      "epoch: 24700, train_loss: 244.093582, test_loss: 67.066452 (Saved)\n",
      "epoch: 24800, train_loss: 263.953285, test_loss: 66.441818 (Saved)\n",
      "epoch: 24900, train_loss: 263.863960, test_loss: 68.174629\n",
      "epoch: 25000, train_loss: 246.426659, test_loss: 68.409798\n",
      "epoch: 25100, train_loss: 230.422318, test_loss: 66.316734 (Saved)\n",
      "epoch: 25200, train_loss: 240.476082, test_loss: 67.927132\n",
      "epoch: 25300, train_loss: 222.875252, test_loss: 68.386765\n",
      "epoch: 25400, train_loss: 225.910583, test_loss: 67.215164\n",
      "epoch: 25500, train_loss: 223.606689, test_loss: 66.999672\n",
      "epoch: 25600, train_loss: 221.543297, test_loss: 65.811539 (Saved)\n",
      "epoch: 25700, train_loss: 216.420555, test_loss: 66.040245\n",
      "epoch: 25800, train_loss: 236.850151, test_loss: 66.912834\n",
      "epoch: 25900, train_loss: 264.041702, test_loss: 66.334763\n",
      "epoch: 26000, train_loss: 214.450294, test_loss: 64.839180 (Saved)\n",
      "epoch: 26100, train_loss: 196.946339, test_loss: 67.814812\n",
      "epoch: 26200, train_loss: 214.670403, test_loss: 65.558830\n",
      "epoch: 26300, train_loss: 211.789597, test_loss: 67.522133\n",
      "epoch: 26400, train_loss: 244.235229, test_loss: 65.715050\n",
      "epoch: 26500, train_loss: 215.322884, test_loss: 65.240524\n",
      "epoch: 26600, train_loss: 218.751968, test_loss: 66.450768\n",
      "epoch: 26700, train_loss: 226.067192, test_loss: 65.069633\n",
      "epoch: 26800, train_loss: 214.914047, test_loss: 64.062180 (Saved)\n",
      "epoch: 26900, train_loss: 227.823883, test_loss: 66.312233\n",
      "epoch: 27000, train_loss: 219.329506, test_loss: 65.904465\n",
      "epoch: 27100, train_loss: 213.292130, test_loss: 65.312828\n",
      "epoch: 27200, train_loss: 207.538673, test_loss: 65.784470\n",
      "epoch: 27300, train_loss: 199.480553, test_loss: 66.774330\n",
      "epoch: 27400, train_loss: 197.866623, test_loss: 65.626175\n",
      "epoch: 27500, train_loss: 209.789368, test_loss: 65.172737\n",
      "epoch: 27600, train_loss: 242.694771, test_loss: 64.969391\n",
      "epoch: 27700, train_loss: 225.543060, test_loss: 64.687614\n",
      "epoch: 27800, train_loss: 215.870399, test_loss: 63.384789 (Saved)\n",
      "epoch: 27900, train_loss: 201.858643, test_loss: 63.953892\n",
      "epoch: 28000, train_loss: 213.565575, test_loss: 64.334122\n",
      "epoch: 28100, train_loss: 212.089760, test_loss: 63.267994 (Saved)\n",
      "epoch: 28200, train_loss: 217.811737, test_loss: 64.227898\n",
      "epoch: 28300, train_loss: 218.945221, test_loss: 63.511147\n",
      "epoch: 28400, train_loss: 223.923462, test_loss: 63.894215\n",
      "epoch: 28500, train_loss: 191.763924, test_loss: 64.736115\n",
      "epoch: 28600, train_loss: 199.056496, test_loss: 64.251030\n",
      "epoch: 28700, train_loss: 197.259819, test_loss: 62.622639 (Saved)\n",
      "epoch: 28800, train_loss: 204.138939, test_loss: 63.704292\n",
      "epoch: 28900, train_loss: 222.953865, test_loss: 62.536736 (Saved)\n",
      "epoch: 29000, train_loss: 231.391335, test_loss: 62.068356 (Saved)\n",
      "epoch: 29100, train_loss: 203.478378, test_loss: 63.501980\n",
      "epoch: 29200, train_loss: 190.730774, test_loss: 60.778057 (Saved)\n",
      "epoch: 29300, train_loss: 211.056908, test_loss: 63.188686\n",
      "epoch: 29400, train_loss: 205.285301, test_loss: 61.549465\n",
      "epoch: 29500, train_loss: 195.453880, test_loss: 62.413406\n",
      "epoch: 29600, train_loss: 207.800293, test_loss: 64.275314\n",
      "epoch: 29700, train_loss: 189.684326, test_loss: 63.765610\n",
      "epoch: 29800, train_loss: 206.342667, test_loss: 62.291695\n",
      "epoch: 29900, train_loss: 199.412773, test_loss: 63.219814\n",
      "epoch: 30000, train_loss: 209.629814, test_loss: 61.663967\n",
      "epoch: 30100, train_loss: 193.552528, test_loss: 61.076599\n",
      "epoch: 30200, train_loss: 199.285126, test_loss: 60.793766\n",
      "epoch: 30300, train_loss: 182.131531, test_loss: 62.312824\n",
      "epoch: 30400, train_loss: 178.716690, test_loss: 61.500462\n",
      "epoch: 30500, train_loss: 179.041779, test_loss: 61.972317\n",
      "epoch: 30600, train_loss: 168.747356, test_loss: 61.026825\n",
      "epoch: 30700, train_loss: 184.211052, test_loss: 61.756584\n",
      "epoch: 30800, train_loss: 162.972435, test_loss: 60.739544 (Saved)\n",
      "epoch: 30900, train_loss: 162.076450, test_loss: 61.238247\n",
      "epoch: 31000, train_loss: 172.990616, test_loss: 59.992302 (Saved)\n",
      "epoch: 31100, train_loss: 167.452404, test_loss: 60.680237\n",
      "epoch: 31200, train_loss: 185.230247, test_loss: 61.170029\n",
      "epoch: 31300, train_loss: 166.853851, test_loss: 60.287552\n",
      "epoch: 31400, train_loss: 204.520111, test_loss: 60.092075\n",
      "epoch: 31500, train_loss: 178.730850, test_loss: 59.017139 (Saved)\n",
      "epoch: 31600, train_loss: 172.700447, test_loss: 59.805656\n",
      "epoch: 31700, train_loss: 181.375931, test_loss: 61.365650\n",
      "epoch: 31800, train_loss: 183.844719, test_loss: 60.167915\n",
      "epoch: 31900, train_loss: 176.734360, test_loss: 60.320015\n",
      "epoch: 32000, train_loss: 169.711136, test_loss: 59.334610\n",
      "epoch: 32100, train_loss: 181.109245, test_loss: 59.728596\n",
      "epoch: 32200, train_loss: 166.131638, test_loss: 59.609158\n",
      "epoch: 32300, train_loss: 183.640930, test_loss: 59.832966\n",
      "epoch: 32400, train_loss: 167.139641, test_loss: 61.109917\n",
      "epoch: 32500, train_loss: 182.510559, test_loss: 60.120308\n",
      "epoch: 32600, train_loss: 179.023819, test_loss: 58.197891 (Saved)\n",
      "epoch: 32700, train_loss: 183.454407, test_loss: 57.942574 (Saved)\n",
      "epoch: 32800, train_loss: 195.161247, test_loss: 59.717800\n",
      "epoch: 32900, train_loss: 177.050003, test_loss: 56.564167 (Saved)\n",
      "epoch: 33000, train_loss: 157.997375, test_loss: 57.768894\n",
      "epoch: 33100, train_loss: 170.356514, test_loss: 57.213211\n",
      "epoch: 33200, train_loss: 174.971664, test_loss: 58.732281\n",
      "epoch: 33300, train_loss: 169.419128, test_loss: 58.213066\n",
      "epoch: 33400, train_loss: 174.897202, test_loss: 57.907398\n",
      "epoch: 33500, train_loss: 178.793716, test_loss: 57.177422\n",
      "epoch: 33600, train_loss: 154.933571, test_loss: 58.349934\n",
      "epoch: 33700, train_loss: 162.842041, test_loss: 56.588604\n",
      "epoch: 33800, train_loss: 173.177567, test_loss: 57.452454\n",
      "epoch: 33900, train_loss: 156.190308, test_loss: 57.812523\n",
      "epoch: 34000, train_loss: 168.294891, test_loss: 55.997097 (Saved)\n",
      "epoch: 34100, train_loss: 152.132748, test_loss: 57.516766\n",
      "epoch: 34200, train_loss: 178.942505, test_loss: 57.275169\n",
      "epoch: 34300, train_loss: 160.040512, test_loss: 56.890491\n",
      "epoch: 34400, train_loss: 168.113243, test_loss: 55.858410 (Saved)\n",
      "epoch: 34500, train_loss: 163.022217, test_loss: 55.950535\n",
      "epoch: 34600, train_loss: 155.615715, test_loss: 56.500065\n",
      "epoch: 34700, train_loss: 160.445564, test_loss: 54.874233 (Saved)\n",
      "epoch: 34800, train_loss: 152.176632, test_loss: 54.970917\n",
      "epoch: 34900, train_loss: 168.919655, test_loss: 54.625298 (Saved)\n",
      "epoch: 35000, train_loss: 165.718193, test_loss: 55.200966\n",
      "epoch: 35100, train_loss: 163.226799, test_loss: 53.893501 (Saved)\n",
      "epoch: 35200, train_loss: 160.544197, test_loss: 55.718258\n",
      "epoch: 35300, train_loss: 178.775757, test_loss: 55.367443\n",
      "epoch: 35400, train_loss: 155.725456, test_loss: 54.755344\n",
      "epoch: 35500, train_loss: 166.104042, test_loss: 53.453945 (Saved)\n",
      "epoch: 35600, train_loss: 160.122398, test_loss: 54.731987\n",
      "epoch: 35700, train_loss: 149.036022, test_loss: 55.055054\n",
      "epoch: 35800, train_loss: 151.799797, test_loss: 53.532127\n",
      "epoch: 35900, train_loss: 179.293510, test_loss: 52.685780 (Saved)\n",
      "epoch: 36000, train_loss: 161.742302, test_loss: 52.502918 (Saved)\n",
      "epoch: 36100, train_loss: 155.203529, test_loss: 53.984802\n",
      "epoch: 36200, train_loss: 167.772011, test_loss: 53.932331\n",
      "epoch: 36300, train_loss: 149.359749, test_loss: 53.356159\n",
      "epoch: 36400, train_loss: 138.291180, test_loss: 51.633080 (Saved)\n",
      "epoch: 36500, train_loss: 147.851326, test_loss: 53.130848\n",
      "epoch: 36600, train_loss: 157.446106, test_loss: 51.834698\n",
      "epoch: 36700, train_loss: 142.242455, test_loss: 53.532871\n",
      "epoch: 36800, train_loss: 146.754326, test_loss: 52.848728\n",
      "epoch: 36900, train_loss: 149.655678, test_loss: 52.087830\n",
      "epoch: 37000, train_loss: 157.035141, test_loss: 51.856808\n",
      "epoch: 37100, train_loss: 135.970974, test_loss: 51.740707\n",
      "epoch: 37200, train_loss: 138.961540, test_loss: 51.082436 (Saved)\n",
      "epoch: 37300, train_loss: 149.015717, test_loss: 50.810547 (Saved)\n",
      "epoch: 37400, train_loss: 148.653481, test_loss: 50.868176\n",
      "epoch: 37500, train_loss: 145.101929, test_loss: 50.111023 (Saved)\n",
      "epoch: 37600, train_loss: 145.000832, test_loss: 50.343666\n",
      "epoch: 37700, train_loss: 152.628891, test_loss: 50.584930\n",
      "epoch: 37800, train_loss: 133.738159, test_loss: 51.374947\n",
      "epoch: 37900, train_loss: 144.713081, test_loss: 51.326206\n",
      "epoch: 38000, train_loss: 141.777206, test_loss: 50.811295\n",
      "epoch: 38100, train_loss: 134.778744, test_loss: 51.168858\n",
      "epoch: 38200, train_loss: 163.079674, test_loss: 50.932301\n",
      "epoch: 38300, train_loss: 127.882877, test_loss: 49.327251 (Saved)\n",
      "epoch: 38400, train_loss: 144.759872, test_loss: 50.388809\n",
      "epoch: 38500, train_loss: 144.246513, test_loss: 50.854736\n",
      "epoch: 38600, train_loss: 148.251793, test_loss: 50.120823\n",
      "epoch: 38700, train_loss: 147.481003, test_loss: 49.206482 (Saved)\n",
      "epoch: 38800, train_loss: 142.361328, test_loss: 49.574863\n",
      "epoch: 38900, train_loss: 146.278435, test_loss: 48.940941 (Saved)\n",
      "epoch: 39000, train_loss: 131.179745, test_loss: 49.447605\n",
      "epoch: 39100, train_loss: 129.192268, test_loss: 47.512012 (Saved)\n",
      "epoch: 39200, train_loss: 138.907845, test_loss: 48.779381\n",
      "epoch: 39300, train_loss: 146.097282, test_loss: 47.626976\n",
      "epoch: 39400, train_loss: 138.366325, test_loss: 48.555660\n",
      "epoch: 39500, train_loss: 133.643742, test_loss: 47.927132\n",
      "epoch: 39600, train_loss: 118.912041, test_loss: 47.092640 (Saved)\n",
      "epoch: 39700, train_loss: 131.456558, test_loss: 47.800377\n",
      "epoch: 39800, train_loss: 152.116192, test_loss: 46.022457 (Saved)\n",
      "epoch: 39900, train_loss: 139.237061, test_loss: 47.340469\n",
      "epoch: 40000, train_loss: 145.180859, test_loss: 46.948669\n",
      "epoch: 40100, train_loss: 134.206276, test_loss: 46.979057\n",
      "epoch: 40200, train_loss: 145.850025, test_loss: 46.566525\n",
      "epoch: 40300, train_loss: 125.995361, test_loss: 46.871254\n",
      "epoch: 40400, train_loss: 132.309326, test_loss: 45.989391 (Saved)\n",
      "epoch: 40500, train_loss: 146.610821, test_loss: 45.307266 (Saved)\n",
      "epoch: 40600, train_loss: 121.120815, test_loss: 45.932903\n",
      "epoch: 40700, train_loss: 137.892487, test_loss: 45.725147\n",
      "epoch: 40800, train_loss: 120.542053, test_loss: 45.087307 (Saved)\n",
      "epoch: 40900, train_loss: 125.625397, test_loss: 45.759968\n",
      "epoch: 41000, train_loss: 147.135185, test_loss: 45.012833 (Saved)\n",
      "epoch: 41100, train_loss: 127.041988, test_loss: 44.726196 (Saved)\n",
      "epoch: 41200, train_loss: 121.533642, test_loss: 44.837524\n",
      "epoch: 41300, train_loss: 115.726238, test_loss: 45.362221\n",
      "epoch: 41400, train_loss: 116.781963, test_loss: 44.439468 (Saved)\n",
      "epoch: 41500, train_loss: 142.141609, test_loss: 44.878990\n",
      "epoch: 41600, train_loss: 139.028652, test_loss: 43.908295 (Saved)\n",
      "epoch: 41700, train_loss: 117.645454, test_loss: 43.411823 (Saved)\n",
      "epoch: 41800, train_loss: 131.590630, test_loss: 44.294605\n",
      "epoch: 41900, train_loss: 137.041821, test_loss: 45.031006\n",
      "epoch: 42000, train_loss: 113.940933, test_loss: 43.629772\n",
      "epoch: 42100, train_loss: 121.575199, test_loss: 42.845291 (Saved)\n",
      "epoch: 42200, train_loss: 136.044952, test_loss: 43.663536\n",
      "epoch: 42300, train_loss: 116.401379, test_loss: 44.008560\n",
      "epoch: 42400, train_loss: 122.700542, test_loss: 43.172375\n",
      "epoch: 42500, train_loss: 126.793446, test_loss: 42.514866 (Saved)\n",
      "epoch: 42600, train_loss: 109.726734, test_loss: 43.746212\n",
      "epoch: 42700, train_loss: 118.733864, test_loss: 43.584793\n",
      "epoch: 42800, train_loss: 103.956703, test_loss: 42.809109\n",
      "epoch: 42900, train_loss: 124.095528, test_loss: 42.643188\n",
      "epoch: 43000, train_loss: 130.110531, test_loss: 42.821644\n",
      "epoch: 43100, train_loss: 131.850777, test_loss: 42.566490\n",
      "epoch: 43200, train_loss: 134.209915, test_loss: 41.872997 (Saved)\n",
      "epoch: 43300, train_loss: 118.116562, test_loss: 41.701714 (Saved)\n",
      "epoch: 43400, train_loss: 124.851059, test_loss: 42.644573\n",
      "epoch: 43500, train_loss: 106.685966, test_loss: 40.687317 (Saved)\n",
      "epoch: 43600, train_loss: 115.281845, test_loss: 41.917820\n",
      "epoch: 43700, train_loss: 118.677242, test_loss: 41.456184\n",
      "epoch: 43800, train_loss: 117.690105, test_loss: 40.745552\n",
      "epoch: 43900, train_loss: 118.870289, test_loss: 42.028656\n",
      "epoch: 44000, train_loss: 115.941139, test_loss: 41.480087\n",
      "epoch: 44100, train_loss: 117.281570, test_loss: 41.181683\n",
      "epoch: 44200, train_loss: 114.340504, test_loss: 40.581707 (Saved)\n",
      "epoch: 44300, train_loss: 112.198921, test_loss: 40.530445 (Saved)\n",
      "epoch: 44400, train_loss: 105.571087, test_loss: 40.236935 (Saved)\n",
      "epoch: 44500, train_loss: 109.777290, test_loss: 40.967480\n",
      "epoch: 44600, train_loss: 108.625404, test_loss: 39.516808 (Saved)\n",
      "epoch: 44700, train_loss: 105.603489, test_loss: 39.628006\n",
      "epoch: 44800, train_loss: 105.985420, test_loss: 40.412838\n",
      "epoch: 44900, train_loss: 114.638027, test_loss: 40.015507\n",
      "epoch: 45000, train_loss: 110.241070, test_loss: 39.141716 (Saved)\n",
      "epoch: 45100, train_loss: 114.570286, test_loss: 40.999851\n",
      "epoch: 45200, train_loss: 115.902229, test_loss: 39.104107 (Saved)\n",
      "epoch: 45300, train_loss: 107.329033, test_loss: 39.083603 (Saved)\n",
      "epoch: 45400, train_loss: 117.902447, test_loss: 39.719345\n",
      "epoch: 45500, train_loss: 106.823143, test_loss: 39.648121\n",
      "epoch: 45600, train_loss: 106.597023, test_loss: 38.762909 (Saved)\n",
      "epoch: 45700, train_loss: 113.131687, test_loss: 37.712566 (Saved)\n",
      "epoch: 45800, train_loss: 100.591866, test_loss: 38.531307\n",
      "epoch: 45900, train_loss: 113.828590, test_loss: 37.816917\n",
      "epoch: 46000, train_loss: 99.671783, test_loss: 38.632214\n",
      "epoch: 46100, train_loss: 107.216969, test_loss: 37.186161 (Saved)\n",
      "epoch: 46200, train_loss: 99.707447, test_loss: 37.672390\n",
      "epoch: 46300, train_loss: 95.537807, test_loss: 36.599583 (Saved)\n",
      "epoch: 46400, train_loss: 107.185925, test_loss: 36.675625\n",
      "epoch: 46500, train_loss: 104.571110, test_loss: 37.063732\n",
      "epoch: 46600, train_loss: 109.814659, test_loss: 36.275188 (Saved)\n",
      "epoch: 46700, train_loss: 99.747276, test_loss: 37.202465\n",
      "epoch: 46800, train_loss: 100.668163, test_loss: 36.357498\n",
      "epoch: 46900, train_loss: 101.296959, test_loss: 36.998562\n",
      "epoch: 47000, train_loss: 95.721817, test_loss: 37.561413\n",
      "epoch: 47100, train_loss: 100.522781, test_loss: 35.502197 (Saved)\n",
      "epoch: 47200, train_loss: 97.193497, test_loss: 35.380650 (Saved)\n",
      "epoch: 47300, train_loss: 95.038399, test_loss: 36.842293\n",
      "epoch: 47400, train_loss: 94.414333, test_loss: 36.202610\n",
      "epoch: 47500, train_loss: 83.406981, test_loss: 36.685711\n",
      "epoch: 47600, train_loss: 94.647629, test_loss: 35.251575 (Saved)\n",
      "epoch: 47700, train_loss: 87.518745, test_loss: 34.994232 (Saved)\n",
      "epoch: 47800, train_loss: 100.350788, test_loss: 35.165241\n",
      "epoch: 47900, train_loss: 99.000076, test_loss: 33.968067 (Saved)\n",
      "epoch: 48000, train_loss: 87.722370, test_loss: 35.350704\n",
      "epoch: 48100, train_loss: 106.583664, test_loss: 34.951752\n",
      "epoch: 48200, train_loss: 83.483786, test_loss: 35.301041\n",
      "epoch: 48300, train_loss: 89.208454, test_loss: 35.213402\n",
      "epoch: 48400, train_loss: 94.602669, test_loss: 34.525402\n",
      "epoch: 48500, train_loss: 100.566494, test_loss: 35.559471\n",
      "epoch: 48600, train_loss: 87.274269, test_loss: 34.446529\n",
      "epoch: 48700, train_loss: 97.812481, test_loss: 35.000404\n",
      "epoch: 48800, train_loss: 82.247099, test_loss: 34.364864\n",
      "epoch: 48900, train_loss: 88.915916, test_loss: 33.667332 (Saved)\n",
      "epoch: 49000, train_loss: 87.051113, test_loss: 33.899551\n",
      "epoch: 49100, train_loss: 91.970329, test_loss: 33.749538\n",
      "epoch: 49200, train_loss: 82.139019, test_loss: 33.645733 (Saved)\n",
      "epoch: 49300, train_loss: 89.515686, test_loss: 32.731197 (Saved)\n",
      "epoch: 49400, train_loss: 87.315430, test_loss: 32.850086\n",
      "epoch: 49500, train_loss: 86.634537, test_loss: 32.152733 (Saved)\n",
      "epoch: 49600, train_loss: 93.646259, test_loss: 33.033279\n",
      "epoch: 49700, train_loss: 86.210030, test_loss: 32.991756\n",
      "epoch: 49800, train_loss: 87.949661, test_loss: 32.299248\n",
      "epoch: 49900, train_loss: 83.364910, test_loss: 33.408173\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/protanjung/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Saving ONNX model: /home/protanjung/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Exported graph: graph(%onnx::Sub_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(4, 2, strides=[2, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(16, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.weight : Float(16, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.weight : Float(4, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.weight : Float(2, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/Constant_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value=  30  260 [ CUDAFloatType{2} ], onnx_name=\"/Constant\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:16:0\n",
      "  %/Sub_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Sub[onnx_name=\"/Sub\"](%onnx::Sub_0, %/Constant_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:16:0\n",
      "  %/Constant_1_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1238   432 [ CUDAFloatType{2} ], onnx_name=\"/Constant_1\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:16:0\n",
      "  %/Div_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Div[onnx_name=\"/Div\"](%/Sub_output_0, %/Constant_1_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:16:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Div_output_0, %fc1.weight, %fc1.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc1 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh\"](%/fc1/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Tanh_output_0, %fc2.weight, %fc2.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc2 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_1_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_1\"](%/fc2/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc3/Gemm_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Tanh_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc3 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_2_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_2\"](%/fc3/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc4/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc4/Gemm\"](%/Tanh_2_output_0, %fc4.weight, %fc4.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc4 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc5/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc5/Gemm\"](%/fc4/Gemm_output_0, %fc5.weight, %fc5.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc5 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc6/Gemm_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc6/Gemm\"](%/fc5/Gemm_output_0, %fc6.weight, %fc6.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc6 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_2_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 450  600 [ CUDAFloatType{2} ], onnx_name=\"/Constant_2\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:23:0\n",
      "  %/Mul_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Mul[onnx_name=\"/Mul\"](%/fc6/Gemm_output_0, %/Constant_2_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:23:0\n",
      "  %/Constant_3_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 0 -300 [ CUDAFloatType{2} ], onnx_name=\"/Constant_3\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:23:0\n",
      "  %29 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/Add\"](%/Mul_output_0, %/Constant_3_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42245/541484460.py:23:0\n",
      "  return (%29)\n",
      "\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Saved ONNX model: /home/protanjung/icar-ng-data/model/pixel2cm_model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'pixel2cm_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 570.,  290.],\n",
      "        [1212.,  678.],\n",
      "        [ 716.,  442.],\n",
      "        [ 524.,  268.],\n",
      "        [ 520.,  394.],\n",
      "        [ 640.,  598.],\n",
      "        [ 938.,  524.],\n",
      "        [ 122.,  452.],\n",
      "        [ 838.,  528.],\n",
      "        [ 488.,  446.],\n",
      "        [1000.,  324.],\n",
      "        [ 204.,  336.],\n",
      "        [ 948.,  438.],\n",
      "        [ 264.,  452.],\n",
      "        [ 746.,  352.],\n",
      "        [ 822.,  324.],\n",
      "        [1272.,  342.],\n",
      "        [ 158.,  532.],\n",
      "        [ 874.,  594.],\n",
      "        [ 910.,  324.],\n",
      "        [1024.,  432.],\n",
      "        [ 954.,  348.],\n",
      "        [ 416.,  446.]], device='cuda:0')\n",
      "y\n",
      "tensor([[ 350.,  -50.],\n",
      "        [   0.,  100.],\n",
      "        [ 100.,   25.],\n",
      "        [ 450., -100.],\n",
      "        [ 150.,  -50.],\n",
      "        [  25.,    0.],\n",
      "        [  50.,   75.],\n",
      "        [ 100., -175.],\n",
      "        [  50.,   50.],\n",
      "        [ 100.,  -50.],\n",
      "        [ 250.,  200.],\n",
      "        [ 250., -250.],\n",
      "        [ 100.,  100.],\n",
      "        [ 100., -125.],\n",
      "        [ 200.,   50.],\n",
      "        [ 250.,  100.],\n",
      "        [ 200.,  300.],\n",
      "        [  50., -125.],\n",
      "        [  25.,   50.],\n",
      "        [ 250.,  150.],\n",
      "        [ 100.,  125.],\n",
      "        [ 200.,  150.],\n",
      "        [ 100.,  -75.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[ 353.0802,  -49.4712],\n",
      "        [   9.1311,   78.4854],\n",
      "        [  98.8054,   27.6697],\n",
      "        [ 443.7282,  -93.5148],\n",
      "        [ 147.2610,  -45.5945],\n",
      "        [  25.0939,    1.4658],\n",
      "        [  51.0296,   67.2586],\n",
      "        [  98.9134, -176.0499],\n",
      "        [  49.8166,   45.1153],\n",
      "        [  99.2321,  -47.1873],\n",
      "        [ 237.1439,  202.5382],\n",
      "        [ 251.3557, -253.6164],\n",
      "        [  97.8511,  100.6624],\n",
      "        [  97.4300, -123.4459],\n",
      "        [ 196.6277,   56.4923],\n",
      "        [ 245.4456,  106.2901],\n",
      "        [ 193.1126,  310.3322],\n",
      "        [  49.1380, -121.2605],\n",
      "        [  26.8947,   42.7973],\n",
      "        [ 241.2464,  154.4962],\n",
      "        [ 100.8750,  126.7543],\n",
      "        [ 194.8987,  158.0564],\n",
      "        [ 100.2240,  -71.9474]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[ -3.0802,  -0.5288],\n",
      "        [ -9.1311,  21.5146],\n",
      "        [  1.1946,  -2.6697],\n",
      "        [  6.2718,  -6.4852],\n",
      "        [  2.7390,  -4.4055],\n",
      "        [ -0.0939,  -1.4658],\n",
      "        [ -1.0296,   7.7414],\n",
      "        [  1.0866,   1.0499],\n",
      "        [  0.1834,   4.8847],\n",
      "        [  0.7679,  -2.8127],\n",
      "        [ 12.8561,  -2.5382],\n",
      "        [ -1.3557,   3.6164],\n",
      "        [  2.1489,  -0.6624],\n",
      "        [  2.5700,  -1.5541],\n",
      "        [  3.3723,  -6.4923],\n",
      "        [  4.5544,  -6.2901],\n",
      "        [  6.8874, -10.3322],\n",
      "        [  0.8620,  -3.7395],\n",
      "        [ -1.8947,   7.2027],\n",
      "        [  8.7536,  -4.4962],\n",
      "        [ -0.8750,  -1.7543],\n",
      "        [  5.1013,  -8.0564],\n",
      "        [ -0.2240,  -3.0526]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
