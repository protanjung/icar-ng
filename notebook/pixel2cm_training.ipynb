{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 19/09/2023 at 23:39:40.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/protanjung/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/protanjung/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/protanjung/icar-ng-data/model\n",
      "Model file: pixel2cm_model.pt\n",
      "Model path: /home/protanjung/icar-ng-data/model/pixel2cm_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'pixel2cm_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([ 60., 274.]), max_x: tensor([1252.,  484.])\n",
      "min_y: tensor([   0., -400.]), max_y: tensor([300., 450.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 0:2].values\n",
    "        y = self.dataframe.iloc[:, 2:4].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 73\n",
      "dataset_test: 18\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.fc5 = nn.Linear(16, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/protanjung/icar-ng-data/model/pixel2cm_model.pt\n",
      "Loaded model: /home/protanjung/icar-ng-data/model/pixel2cm_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 2693.065063, test_loss: 1238.156860 (Saved)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, train_loss: 491.058853, test_loss: 285.734589 (Saved)\n",
      "epoch: 200, train_loss: 434.337616, test_loss: 288.057251\n",
      "epoch: 300, train_loss: 408.622818, test_loss: 287.495819\n",
      "epoch: 400, train_loss: 553.894287, test_loss: 288.807526\n",
      "epoch: 500, train_loss: 418.582748, test_loss: 280.687927 (Saved)\n",
      "epoch: 600, train_loss: 467.052856, test_loss: 288.844177\n",
      "epoch: 700, train_loss: 406.820267, test_loss: 286.634888\n",
      "epoch: 800, train_loss: 438.137726, test_loss: 283.995117\n",
      "epoch: 900, train_loss: 495.497620, test_loss: 280.373962 (Saved)\n",
      "epoch: 1000, train_loss: 594.222260, test_loss: 279.343597 (Saved)\n",
      "epoch: 1100, train_loss: 376.086609, test_loss: 285.670685\n",
      "epoch: 1200, train_loss: 469.573044, test_loss: 284.306152\n",
      "epoch: 1300, train_loss: 361.621674, test_loss: 276.376190 (Saved)\n",
      "epoch: 1400, train_loss: 477.254379, test_loss: 277.072754\n",
      "epoch: 1500, train_loss: 415.396133, test_loss: 281.960938\n",
      "epoch: 1600, train_loss: 413.455093, test_loss: 272.828400 (Saved)\n",
      "epoch: 1700, train_loss: 466.665054, test_loss: 273.690338\n",
      "epoch: 1800, train_loss: 524.290985, test_loss: 278.449646\n",
      "epoch: 1900, train_loss: 389.881165, test_loss: 270.007629 (Saved)\n",
      "epoch: 2000, train_loss: 428.977051, test_loss: 278.389496\n",
      "epoch: 2100, train_loss: 585.526978, test_loss: 273.472961\n",
      "epoch: 2200, train_loss: 408.928375, test_loss: 275.270569\n",
      "epoch: 2300, train_loss: 487.335175, test_loss: 276.571533\n",
      "epoch: 2400, train_loss: 427.306595, test_loss: 277.471252\n",
      "epoch: 2500, train_loss: 367.698212, test_loss: 270.517212\n",
      "epoch: 2600, train_loss: 405.589371, test_loss: 270.294586\n",
      "epoch: 2700, train_loss: 506.082855, test_loss: 273.444244\n",
      "epoch: 2800, train_loss: 500.244354, test_loss: 269.188324 (Saved)\n",
      "epoch: 2900, train_loss: 429.040512, test_loss: 266.750488 (Saved)\n",
      "epoch: 3000, train_loss: 543.299988, test_loss: 272.391052\n",
      "epoch: 3100, train_loss: 444.701965, test_loss: 268.913300\n",
      "epoch: 3200, train_loss: 404.247665, test_loss: 269.722687\n",
      "epoch: 3300, train_loss: 339.417412, test_loss: 269.618011\n",
      "epoch: 3400, train_loss: 451.301865, test_loss: 270.962952\n",
      "epoch: 3500, train_loss: 483.162735, test_loss: 276.623779\n",
      "epoch: 3600, train_loss: 560.298065, test_loss: 274.127136\n",
      "epoch: 3700, train_loss: 474.003830, test_loss: 263.602600 (Saved)\n",
      "epoch: 3800, train_loss: 373.625381, test_loss: 266.422119\n",
      "epoch: 3900, train_loss: 465.837555, test_loss: 262.554596 (Saved)\n",
      "epoch: 4000, train_loss: 360.547577, test_loss: 267.556885\n",
      "epoch: 4100, train_loss: 409.368073, test_loss: 264.420197\n",
      "epoch: 4200, train_loss: 460.829468, test_loss: 264.648590\n",
      "epoch: 4300, train_loss: 387.176575, test_loss: 262.111725 (Saved)\n",
      "epoch: 4400, train_loss: 386.673126, test_loss: 269.646973\n",
      "epoch: 4500, train_loss: 437.823334, test_loss: 268.253479\n",
      "epoch: 4600, train_loss: 403.136871, test_loss: 266.600647\n",
      "epoch: 4700, train_loss: 385.017456, test_loss: 262.460693\n",
      "epoch: 4800, train_loss: 494.384033, test_loss: 263.399200\n",
      "epoch: 4900, train_loss: 457.525589, test_loss: 260.738525 (Saved)\n",
      "epoch: 5000, train_loss: 480.572159, test_loss: 265.387878\n",
      "epoch: 5100, train_loss: 475.854553, test_loss: 264.015930\n",
      "epoch: 5200, train_loss: 410.444092, test_loss: 260.565369 (Saved)\n",
      "epoch: 5300, train_loss: 382.177811, test_loss: 257.752502 (Saved)\n",
      "epoch: 5400, train_loss: 376.847549, test_loss: 260.372955\n",
      "epoch: 5500, train_loss: 361.477570, test_loss: 259.193420\n",
      "epoch: 5600, train_loss: 390.855499, test_loss: 260.702484\n",
      "epoch: 5700, train_loss: 560.853806, test_loss: 261.200714\n",
      "epoch: 5800, train_loss: 473.696823, test_loss: 265.256409\n",
      "epoch: 5900, train_loss: 418.413055, test_loss: 257.917877\n",
      "epoch: 6000, train_loss: 341.927032, test_loss: 260.088928\n",
      "epoch: 6100, train_loss: 391.310944, test_loss: 255.087357 (Saved)\n",
      "epoch: 6200, train_loss: 395.727081, test_loss: 263.840149\n",
      "epoch: 6300, train_loss: 472.718872, test_loss: 256.282349\n",
      "epoch: 6400, train_loss: 385.518021, test_loss: 257.426392\n",
      "epoch: 6500, train_loss: 394.312820, test_loss: 253.652786 (Saved)\n",
      "epoch: 6600, train_loss: 456.218307, test_loss: 248.831818 (Saved)\n",
      "epoch: 6700, train_loss: 364.863922, test_loss: 251.225449\n",
      "epoch: 6800, train_loss: 349.729538, test_loss: 250.820663\n",
      "epoch: 6900, train_loss: 424.313004, test_loss: 257.564911\n",
      "epoch: 7000, train_loss: 443.551788, test_loss: 252.049026\n",
      "epoch: 7100, train_loss: 354.881226, test_loss: 244.018936 (Saved)\n",
      "epoch: 7200, train_loss: 455.401352, test_loss: 254.685471\n",
      "epoch: 7300, train_loss: 356.062836, test_loss: 250.692947\n",
      "epoch: 7400, train_loss: 382.904373, test_loss: 247.123001\n",
      "epoch: 7500, train_loss: 427.033157, test_loss: 252.931854\n",
      "epoch: 7600, train_loss: 379.459534, test_loss: 250.958115\n",
      "epoch: 7700, train_loss: 347.867126, test_loss: 247.520706\n",
      "epoch: 7800, train_loss: 480.701233, test_loss: 249.690247\n",
      "epoch: 7900, train_loss: 379.063049, test_loss: 250.994843\n",
      "epoch: 8000, train_loss: 329.187538, test_loss: 244.408157\n",
      "epoch: 8100, train_loss: 380.484634, test_loss: 246.255051\n",
      "epoch: 8200, train_loss: 366.509460, test_loss: 249.963898\n",
      "epoch: 8300, train_loss: 346.220093, test_loss: 248.045792\n",
      "epoch: 8400, train_loss: 296.961334, test_loss: 238.823685 (Saved)\n",
      "epoch: 8500, train_loss: 354.478607, test_loss: 240.514191\n",
      "epoch: 8600, train_loss: 408.353622, test_loss: 239.578857\n",
      "epoch: 8700, train_loss: 349.293991, test_loss: 245.005829\n",
      "epoch: 8800, train_loss: 342.963425, test_loss: 239.069550\n",
      "epoch: 8900, train_loss: 408.367538, test_loss: 237.671341 (Saved)\n",
      "epoch: 9000, train_loss: 372.446487, test_loss: 238.083527\n",
      "epoch: 9100, train_loss: 398.805206, test_loss: 241.815292\n",
      "epoch: 9200, train_loss: 387.194733, test_loss: 246.820633\n",
      "epoch: 9300, train_loss: 333.403519, test_loss: 237.242401 (Saved)\n",
      "epoch: 9400, train_loss: 362.357483, test_loss: 235.092941 (Saved)\n",
      "epoch: 9500, train_loss: 431.778702, test_loss: 237.739090\n",
      "epoch: 9600, train_loss: 380.079178, test_loss: 238.386673\n",
      "epoch: 9700, train_loss: 363.243698, test_loss: 241.474472\n",
      "epoch: 9800, train_loss: 335.290070, test_loss: 235.140411\n",
      "epoch: 9900, train_loss: 398.424942, test_loss: 236.116104\n",
      "epoch: 10000, train_loss: 424.284149, test_loss: 235.824600\n",
      "epoch: 10100, train_loss: 345.044800, test_loss: 236.401688\n",
      "epoch: 10200, train_loss: 355.103622, test_loss: 232.171707 (Saved)\n",
      "epoch: 10300, train_loss: 300.117393, test_loss: 235.838699\n",
      "epoch: 10400, train_loss: 360.714355, test_loss: 234.303223\n",
      "epoch: 10500, train_loss: 415.476746, test_loss: 232.946121\n",
      "epoch: 10600, train_loss: 346.489120, test_loss: 238.405273\n",
      "epoch: 10700, train_loss: 401.481537, test_loss: 231.561691 (Saved)\n",
      "epoch: 10800, train_loss: 456.952972, test_loss: 224.967346 (Saved)\n",
      "epoch: 10900, train_loss: 351.967026, test_loss: 232.409485\n",
      "epoch: 11000, train_loss: 370.157028, test_loss: 238.734436\n",
      "epoch: 11100, train_loss: 365.790558, test_loss: 222.985001 (Saved)\n",
      "epoch: 11200, train_loss: 278.641670, test_loss: 227.557449\n",
      "epoch: 11300, train_loss: 333.966614, test_loss: 229.690155\n",
      "epoch: 11400, train_loss: 427.416077, test_loss: 228.972473\n",
      "epoch: 11500, train_loss: 346.015915, test_loss: 234.080948\n",
      "epoch: 11600, train_loss: 265.642570, test_loss: 227.784882\n",
      "epoch: 11700, train_loss: 273.065292, test_loss: 228.567062\n",
      "epoch: 11800, train_loss: 370.726913, test_loss: 224.818802\n",
      "epoch: 11900, train_loss: 317.483932, test_loss: 225.032883\n",
      "epoch: 12000, train_loss: 324.134735, test_loss: 223.222366\n",
      "epoch: 12100, train_loss: 437.278107, test_loss: 225.223694\n",
      "epoch: 12200, train_loss: 367.481308, test_loss: 231.866165\n",
      "epoch: 12300, train_loss: 397.254776, test_loss: 230.282776\n",
      "epoch: 12400, train_loss: 400.390610, test_loss: 221.187286 (Saved)\n",
      "epoch: 12500, train_loss: 362.629868, test_loss: 231.942932\n",
      "epoch: 12600, train_loss: 404.193268, test_loss: 225.864258\n",
      "epoch: 12700, train_loss: 313.604324, test_loss: 224.774857\n",
      "epoch: 12800, train_loss: 259.884216, test_loss: 227.478973\n",
      "epoch: 12900, train_loss: 412.666885, test_loss: 218.973969 (Saved)\n",
      "epoch: 13000, train_loss: 280.628807, test_loss: 221.211563\n",
      "epoch: 13100, train_loss: 364.836166, test_loss: 225.103897\n",
      "epoch: 13200, train_loss: 292.951691, test_loss: 219.876404\n",
      "epoch: 13300, train_loss: 276.097786, test_loss: 218.367249 (Saved)\n",
      "epoch: 13400, train_loss: 297.076591, test_loss: 220.402542\n",
      "epoch: 13500, train_loss: 305.075211, test_loss: 217.800461 (Saved)\n",
      "epoch: 13600, train_loss: 295.610191, test_loss: 221.466446\n",
      "epoch: 13700, train_loss: 376.071686, test_loss: 222.194305\n",
      "epoch: 13800, train_loss: 249.563362, test_loss: 219.972443\n",
      "epoch: 13900, train_loss: 295.109161, test_loss: 217.498245 (Saved)\n",
      "epoch: 14000, train_loss: 332.221619, test_loss: 220.003647\n",
      "epoch: 14100, train_loss: 278.981140, test_loss: 220.375870\n",
      "epoch: 14200, train_loss: 399.575439, test_loss: 215.573837 (Saved)\n",
      "epoch: 14300, train_loss: 278.922501, test_loss: 215.435425 (Saved)\n",
      "epoch: 14400, train_loss: 266.870735, test_loss: 213.979599 (Saved)\n",
      "epoch: 14500, train_loss: 306.286987, test_loss: 213.530106 (Saved)\n",
      "epoch: 14600, train_loss: 369.874771, test_loss: 214.070908\n",
      "epoch: 14700, train_loss: 308.972702, test_loss: 211.831955 (Saved)\n",
      "epoch: 14800, train_loss: 272.375755, test_loss: 214.218094\n",
      "epoch: 14900, train_loss: 474.797668, test_loss: 213.242188\n",
      "epoch: 15000, train_loss: 304.809540, test_loss: 206.343567 (Saved)\n",
      "epoch: 15100, train_loss: 312.878632, test_loss: 207.482407\n",
      "epoch: 15200, train_loss: 365.441971, test_loss: 213.651794\n",
      "epoch: 15300, train_loss: 335.644806, test_loss: 205.228714 (Saved)\n",
      "epoch: 15400, train_loss: 283.466194, test_loss: 206.850891\n",
      "epoch: 15500, train_loss: 318.935379, test_loss: 210.278854\n",
      "epoch: 15600, train_loss: 280.761795, test_loss: 203.932007 (Saved)\n",
      "epoch: 15700, train_loss: 269.856972, test_loss: 212.742142\n",
      "epoch: 15800, train_loss: 390.727798, test_loss: 206.101364\n",
      "epoch: 15900, train_loss: 289.437866, test_loss: 208.359833\n",
      "epoch: 16000, train_loss: 375.527222, test_loss: 207.090851\n",
      "epoch: 16100, train_loss: 292.361969, test_loss: 206.159866\n",
      "epoch: 16200, train_loss: 325.020035, test_loss: 209.454025\n",
      "epoch: 16300, train_loss: 330.783966, test_loss: 203.942551\n",
      "epoch: 16400, train_loss: 332.389130, test_loss: 203.815872 (Saved)\n",
      "epoch: 16500, train_loss: 312.727554, test_loss: 204.371155\n",
      "epoch: 16600, train_loss: 306.906326, test_loss: 201.188293 (Saved)\n",
      "epoch: 16700, train_loss: 305.405182, test_loss: 200.839188 (Saved)\n",
      "epoch: 16800, train_loss: 281.577866, test_loss: 197.342316 (Saved)\n",
      "epoch: 16900, train_loss: 328.310669, test_loss: 199.975693\n",
      "epoch: 17000, train_loss: 297.134583, test_loss: 200.902145\n",
      "epoch: 17100, train_loss: 292.806244, test_loss: 200.468918\n",
      "epoch: 17200, train_loss: 245.656830, test_loss: 197.483276\n",
      "epoch: 17300, train_loss: 335.671524, test_loss: 196.370117 (Saved)\n",
      "epoch: 17400, train_loss: 262.956894, test_loss: 201.875443\n",
      "epoch: 17500, train_loss: 302.824783, test_loss: 195.763519 (Saved)\n",
      "epoch: 17600, train_loss: 271.584389, test_loss: 199.041367\n",
      "epoch: 17700, train_loss: 311.308670, test_loss: 195.430237 (Saved)\n",
      "epoch: 17800, train_loss: 245.443054, test_loss: 193.679947 (Saved)\n",
      "epoch: 17900, train_loss: 309.708313, test_loss: 195.119247\n",
      "epoch: 18000, train_loss: 234.369209, test_loss: 192.787354 (Saved)\n",
      "epoch: 18100, train_loss: 273.178497, test_loss: 190.660278 (Saved)\n",
      "epoch: 18200, train_loss: 333.120102, test_loss: 195.010101\n",
      "epoch: 18300, train_loss: 262.087227, test_loss: 189.957870 (Saved)\n",
      "epoch: 18400, train_loss: 328.475067, test_loss: 197.554337\n",
      "epoch: 18500, train_loss: 309.954315, test_loss: 194.974854\n",
      "epoch: 18600, train_loss: 282.978897, test_loss: 189.651566 (Saved)\n",
      "epoch: 18700, train_loss: 264.012665, test_loss: 190.071732\n",
      "epoch: 18800, train_loss: 297.064743, test_loss: 191.795578\n",
      "epoch: 18900, train_loss: 234.535774, test_loss: 194.296555\n",
      "epoch: 19000, train_loss: 345.879295, test_loss: 191.983536\n",
      "epoch: 19100, train_loss: 241.951149, test_loss: 187.030304 (Saved)\n",
      "epoch: 19200, train_loss: 317.691925, test_loss: 189.676270\n",
      "epoch: 19300, train_loss: 284.776184, test_loss: 189.773590\n",
      "epoch: 19400, train_loss: 346.170547, test_loss: 187.934296\n",
      "epoch: 19500, train_loss: 315.549774, test_loss: 188.382401\n",
      "epoch: 19600, train_loss: 310.450012, test_loss: 190.207565\n",
      "epoch: 19700, train_loss: 303.487839, test_loss: 184.561127 (Saved)\n",
      "epoch: 19800, train_loss: 324.210419, test_loss: 188.196182\n",
      "epoch: 19900, train_loss: 265.535797, test_loss: 185.754532\n",
      "epoch: 20000, train_loss: 223.611458, test_loss: 186.946793\n",
      "epoch: 20100, train_loss: 295.102097, test_loss: 179.885559 (Saved)\n",
      "epoch: 20200, train_loss: 260.474380, test_loss: 187.595505\n",
      "epoch: 20300, train_loss: 310.553009, test_loss: 188.538773\n",
      "epoch: 20400, train_loss: 294.632912, test_loss: 185.251770\n",
      "epoch: 20500, train_loss: 238.384392, test_loss: 180.737320\n",
      "epoch: 20600, train_loss: 231.287735, test_loss: 183.296646\n",
      "epoch: 20700, train_loss: 221.664688, test_loss: 180.650772\n",
      "epoch: 20800, train_loss: 342.084961, test_loss: 178.371796 (Saved)\n",
      "epoch: 20900, train_loss: 198.621056, test_loss: 183.148743\n",
      "epoch: 21000, train_loss: 239.646950, test_loss: 182.526672\n",
      "epoch: 21100, train_loss: 212.483521, test_loss: 180.785156\n",
      "epoch: 21200, train_loss: 257.925186, test_loss: 180.854553\n",
      "epoch: 21300, train_loss: 285.436920, test_loss: 180.892990\n",
      "epoch: 21400, train_loss: 238.027122, test_loss: 180.334610\n",
      "epoch: 21500, train_loss: 251.116943, test_loss: 177.463730 (Saved)\n",
      "epoch: 21600, train_loss: 211.653999, test_loss: 175.695801 (Saved)\n",
      "epoch: 21700, train_loss: 256.169464, test_loss: 176.733719\n",
      "epoch: 21800, train_loss: 234.230896, test_loss: 176.649826\n",
      "epoch: 21900, train_loss: 315.133400, test_loss: 175.343307 (Saved)\n",
      "epoch: 22000, train_loss: 216.329483, test_loss: 170.480316 (Saved)\n",
      "epoch: 22100, train_loss: 295.757645, test_loss: 178.282303\n",
      "epoch: 22200, train_loss: 257.950150, test_loss: 176.881897\n",
      "epoch: 22300, train_loss: 258.488632, test_loss: 176.799484\n",
      "epoch: 22400, train_loss: 264.814987, test_loss: 166.893753 (Saved)\n",
      "epoch: 22500, train_loss: 236.277664, test_loss: 178.308167\n",
      "epoch: 22600, train_loss: 271.703094, test_loss: 169.651077\n",
      "epoch: 22700, train_loss: 220.199478, test_loss: 168.308380\n",
      "epoch: 22800, train_loss: 220.924347, test_loss: 171.369522\n",
      "epoch: 22900, train_loss: 196.101738, test_loss: 166.526962 (Saved)\n",
      "epoch: 23000, train_loss: 339.306870, test_loss: 178.628937\n",
      "epoch: 23100, train_loss: 281.840752, test_loss: 172.748047\n",
      "epoch: 23200, train_loss: 288.813606, test_loss: 170.944290\n",
      "epoch: 23300, train_loss: 199.371674, test_loss: 168.781235\n",
      "epoch: 23400, train_loss: 240.950203, test_loss: 170.846207\n",
      "epoch: 23500, train_loss: 315.717216, test_loss: 165.684982 (Saved)\n",
      "epoch: 23600, train_loss: 168.876686, test_loss: 169.069855\n",
      "epoch: 23700, train_loss: 266.622864, test_loss: 167.595383\n",
      "epoch: 23800, train_loss: 214.892044, test_loss: 166.029114\n",
      "epoch: 23900, train_loss: 171.118767, test_loss: 162.996292 (Saved)\n",
      "epoch: 24000, train_loss: 253.523277, test_loss: 169.485733\n",
      "epoch: 24100, train_loss: 200.061310, test_loss: 163.504166\n",
      "epoch: 24200, train_loss: 266.451340, test_loss: 163.754501\n",
      "epoch: 24300, train_loss: 227.546371, test_loss: 167.316025\n",
      "epoch: 24400, train_loss: 182.633465, test_loss: 165.491913\n",
      "epoch: 24500, train_loss: 224.992699, test_loss: 169.369568\n",
      "epoch: 24600, train_loss: 265.832657, test_loss: 161.657761 (Saved)\n",
      "epoch: 24700, train_loss: 268.286789, test_loss: 167.894669\n",
      "epoch: 24800, train_loss: 270.521927, test_loss: 161.947357\n",
      "epoch: 24900, train_loss: 203.396362, test_loss: 163.945526\n",
      "epoch: 25000, train_loss: 303.554184, test_loss: 156.828445 (Saved)\n",
      "epoch: 25100, train_loss: 242.267059, test_loss: 163.829758\n",
      "epoch: 25200, train_loss: 250.954643, test_loss: 161.151611\n",
      "epoch: 25300, train_loss: 240.638863, test_loss: 159.956207\n",
      "epoch: 25400, train_loss: 215.003998, test_loss: 155.917191 (Saved)\n",
      "epoch: 25500, train_loss: 181.387535, test_loss: 161.568604\n",
      "epoch: 25600, train_loss: 199.353867, test_loss: 160.034943\n",
      "epoch: 25700, train_loss: 238.845505, test_loss: 159.115112\n",
      "epoch: 25800, train_loss: 221.759422, test_loss: 157.476807\n",
      "epoch: 25900, train_loss: 192.585419, test_loss: 154.896484 (Saved)\n",
      "epoch: 26000, train_loss: 215.324188, test_loss: 155.699951\n",
      "epoch: 26100, train_loss: 234.249542, test_loss: 158.075058\n",
      "epoch: 26200, train_loss: 212.766747, test_loss: 157.732437\n",
      "epoch: 26300, train_loss: 271.754303, test_loss: 163.929794\n",
      "epoch: 26400, train_loss: 236.246704, test_loss: 154.467667 (Saved)\n",
      "epoch: 26500, train_loss: 204.096169, test_loss: 157.249649\n",
      "epoch: 26600, train_loss: 182.138596, test_loss: 157.443771\n",
      "epoch: 26700, train_loss: 205.575073, test_loss: 159.066330\n",
      "epoch: 26800, train_loss: 242.477547, test_loss: 156.804367\n",
      "epoch: 26900, train_loss: 252.876694, test_loss: 147.977600 (Saved)\n",
      "epoch: 27000, train_loss: 198.000229, test_loss: 155.918625\n",
      "epoch: 27100, train_loss: 232.809143, test_loss: 154.296585\n",
      "epoch: 27200, train_loss: 207.844345, test_loss: 150.204834\n",
      "epoch: 27300, train_loss: 261.799820, test_loss: 154.803558\n",
      "epoch: 27400, train_loss: 194.821007, test_loss: 154.856689\n",
      "epoch: 27500, train_loss: 167.621265, test_loss: 149.618851\n",
      "epoch: 27600, train_loss: 228.731636, test_loss: 156.626251\n",
      "epoch: 27700, train_loss: 201.897987, test_loss: 149.908264\n",
      "epoch: 27800, train_loss: 225.995110, test_loss: 150.305847\n",
      "epoch: 27900, train_loss: 202.628922, test_loss: 151.969788\n",
      "epoch: 28000, train_loss: 188.678841, test_loss: 150.377808\n",
      "epoch: 28100, train_loss: 197.162666, test_loss: 146.638336 (Saved)\n",
      "epoch: 28200, train_loss: 225.753883, test_loss: 149.689148\n",
      "epoch: 28300, train_loss: 168.071716, test_loss: 149.944336\n",
      "epoch: 28400, train_loss: 147.293602, test_loss: 150.300919\n",
      "epoch: 28500, train_loss: 217.394180, test_loss: 149.985809\n",
      "epoch: 28600, train_loss: 229.200768, test_loss: 147.273300\n",
      "epoch: 28700, train_loss: 170.781181, test_loss: 149.692062\n",
      "epoch: 28800, train_loss: 171.981033, test_loss: 148.271057\n",
      "epoch: 28900, train_loss: 154.480175, test_loss: 149.277542\n",
      "epoch: 29000, train_loss: 182.216293, test_loss: 144.132080 (Saved)\n",
      "epoch: 29100, train_loss: 201.289818, test_loss: 139.862747 (Saved)\n",
      "epoch: 29200, train_loss: 208.848412, test_loss: 143.479355\n",
      "epoch: 29300, train_loss: 171.721428, test_loss: 142.191132\n",
      "epoch: 29400, train_loss: 206.331985, test_loss: 144.079926\n",
      "epoch: 29500, train_loss: 166.263542, test_loss: 143.498169\n",
      "epoch: 29600, train_loss: 158.529667, test_loss: 142.766220\n",
      "epoch: 29700, train_loss: 174.744827, test_loss: 138.675613 (Saved)\n",
      "epoch: 29800, train_loss: 216.138252, test_loss: 146.829559\n",
      "epoch: 29900, train_loss: 187.330406, test_loss: 143.274551\n",
      "epoch: 30000, train_loss: 198.589142, test_loss: 144.695282\n",
      "epoch: 30100, train_loss: 181.520149, test_loss: 144.652420\n",
      "epoch: 30200, train_loss: 170.508682, test_loss: 145.036972\n",
      "epoch: 30300, train_loss: 203.283234, test_loss: 142.396454\n",
      "epoch: 30400, train_loss: 184.852516, test_loss: 144.071594\n",
      "epoch: 30500, train_loss: 187.711967, test_loss: 145.791504\n",
      "epoch: 30600, train_loss: 164.850029, test_loss: 138.993851\n",
      "epoch: 30700, train_loss: 167.148956, test_loss: 143.579224\n",
      "epoch: 30800, train_loss: 229.403015, test_loss: 136.444427 (Saved)\n",
      "epoch: 30900, train_loss: 220.857086, test_loss: 140.988968\n",
      "epoch: 31000, train_loss: 169.053680, test_loss: 135.647949 (Saved)\n",
      "epoch: 31100, train_loss: 142.839684, test_loss: 136.582031\n",
      "epoch: 31200, train_loss: 203.509636, test_loss: 138.693130\n",
      "epoch: 31300, train_loss: 175.137779, test_loss: 142.191284\n",
      "epoch: 31400, train_loss: 224.771706, test_loss: 140.240616\n",
      "epoch: 31500, train_loss: 231.273087, test_loss: 141.367508\n",
      "epoch: 31600, train_loss: 169.068703, test_loss: 135.474808 (Saved)\n",
      "epoch: 31700, train_loss: 145.683479, test_loss: 140.522171\n",
      "epoch: 31800, train_loss: 148.834721, test_loss: 134.514114 (Saved)\n",
      "epoch: 31900, train_loss: 174.755775, test_loss: 134.744339\n",
      "epoch: 32000, train_loss: 183.234505, test_loss: 134.745499\n",
      "epoch: 32100, train_loss: 183.062866, test_loss: 134.591415\n",
      "epoch: 32200, train_loss: 186.754288, test_loss: 133.269043 (Saved)\n",
      "epoch: 32300, train_loss: 159.618164, test_loss: 133.369888\n",
      "epoch: 32400, train_loss: 204.817429, test_loss: 138.458633\n",
      "epoch: 32500, train_loss: 177.515007, test_loss: 137.498810\n",
      "epoch: 32600, train_loss: 145.669174, test_loss: 133.376297\n",
      "epoch: 32700, train_loss: 201.882423, test_loss: 142.167343\n",
      "epoch: 32800, train_loss: 157.400955, test_loss: 131.110001 (Saved)\n",
      "epoch: 32900, train_loss: 191.293518, test_loss: 135.740799\n",
      "epoch: 33000, train_loss: 179.117325, test_loss: 131.055313 (Saved)\n",
      "epoch: 33100, train_loss: 180.631042, test_loss: 131.774551\n",
      "epoch: 33200, train_loss: 202.427017, test_loss: 129.448593 (Saved)\n",
      "epoch: 33300, train_loss: 163.575333, test_loss: 131.704330\n",
      "epoch: 33400, train_loss: 173.023529, test_loss: 134.352982\n",
      "epoch: 33500, train_loss: 248.860878, test_loss: 134.424805\n",
      "epoch: 33600, train_loss: 123.888741, test_loss: 133.654694\n",
      "epoch: 33700, train_loss: 157.884773, test_loss: 128.833084 (Saved)\n",
      "epoch: 33800, train_loss: 159.581268, test_loss: 134.087479\n",
      "epoch: 33900, train_loss: 156.291336, test_loss: 132.848114\n",
      "epoch: 34000, train_loss: 159.390450, test_loss: 127.241295 (Saved)\n",
      "epoch: 34100, train_loss: 125.088665, test_loss: 133.090149\n",
      "epoch: 34200, train_loss: 137.770939, test_loss: 127.138947 (Saved)\n",
      "epoch: 34300, train_loss: 138.049332, test_loss: 131.483627\n",
      "epoch: 34400, train_loss: 138.507618, test_loss: 130.068192\n",
      "epoch: 34500, train_loss: 189.462524, test_loss: 129.857101\n",
      "epoch: 34600, train_loss: 163.037636, test_loss: 128.588272\n",
      "epoch: 34700, train_loss: 155.696594, test_loss: 125.807755 (Saved)\n",
      "epoch: 34800, train_loss: 148.178307, test_loss: 128.302139\n",
      "epoch: 34900, train_loss: 166.642212, test_loss: 132.405930\n",
      "epoch: 35000, train_loss: 167.764366, test_loss: 127.929794\n",
      "epoch: 35100, train_loss: 144.670479, test_loss: 133.124069\n",
      "epoch: 35200, train_loss: 153.733299, test_loss: 125.922905\n",
      "epoch: 35300, train_loss: 184.545593, test_loss: 130.898163\n",
      "epoch: 35400, train_loss: 147.497429, test_loss: 128.093857\n",
      "epoch: 35500, train_loss: 132.597748, test_loss: 129.471954\n",
      "epoch: 35600, train_loss: 126.348606, test_loss: 130.282166\n",
      "epoch: 35700, train_loss: 149.017258, test_loss: 125.396339 (Saved)\n",
      "epoch: 35800, train_loss: 172.016251, test_loss: 126.749001\n",
      "epoch: 35900, train_loss: 151.426598, test_loss: 125.945297\n",
      "epoch: 36000, train_loss: 143.478401, test_loss: 123.236153 (Saved)\n",
      "epoch: 36100, train_loss: 165.561790, test_loss: 126.731186\n",
      "epoch: 36200, train_loss: 167.971283, test_loss: 122.092705 (Saved)\n",
      "epoch: 36300, train_loss: 115.729271, test_loss: 126.614868\n",
      "epoch: 36400, train_loss: 117.002438, test_loss: 125.509415\n",
      "epoch: 36500, train_loss: 125.908852, test_loss: 128.847809\n",
      "epoch: 36600, train_loss: 170.540550, test_loss: 125.639267\n",
      "epoch: 36700, train_loss: 161.978569, test_loss: 123.516197\n",
      "epoch: 36800, train_loss: 160.009331, test_loss: 122.917580\n",
      "epoch: 36900, train_loss: 143.410851, test_loss: 123.106499\n",
      "epoch: 37000, train_loss: 129.498329, test_loss: 120.896675 (Saved)\n",
      "epoch: 37100, train_loss: 126.928577, test_loss: 125.678795\n",
      "epoch: 37200, train_loss: 189.620026, test_loss: 123.372665\n",
      "epoch: 37300, train_loss: 127.814846, test_loss: 124.854736\n",
      "epoch: 37400, train_loss: 123.264671, test_loss: 123.086617\n",
      "epoch: 37500, train_loss: 159.420982, test_loss: 122.718575\n",
      "epoch: 37600, train_loss: 115.467503, test_loss: 120.207603 (Saved)\n",
      "epoch: 37700, train_loss: 127.337875, test_loss: 121.174538\n",
      "epoch: 37800, train_loss: 165.288681, test_loss: 123.620224\n",
      "epoch: 37900, train_loss: 150.445625, test_loss: 121.697365\n",
      "epoch: 38000, train_loss: 119.737743, test_loss: 119.227188 (Saved)\n",
      "epoch: 38100, train_loss: 153.510437, test_loss: 122.691948\n",
      "epoch: 38200, train_loss: 183.221542, test_loss: 122.825386\n",
      "epoch: 38300, train_loss: 181.812103, test_loss: 120.934189\n",
      "epoch: 38400, train_loss: 110.355698, test_loss: 120.482559\n",
      "epoch: 38500, train_loss: 121.685577, test_loss: 119.373085\n",
      "epoch: 38600, train_loss: 158.509064, test_loss: 121.252228\n",
      "epoch: 38700, train_loss: 144.304382, test_loss: 119.051186 (Saved)\n",
      "epoch: 38800, train_loss: 137.852364, test_loss: 120.909645\n",
      "epoch: 38900, train_loss: 124.110641, test_loss: 117.917030 (Saved)\n",
      "epoch: 39000, train_loss: 160.429321, test_loss: 117.941895\n",
      "epoch: 39100, train_loss: 136.979706, test_loss: 119.475693\n",
      "epoch: 39200, train_loss: 108.773125, test_loss: 117.137672 (Saved)\n",
      "epoch: 39300, train_loss: 142.699905, test_loss: 115.769028 (Saved)\n",
      "epoch: 39400, train_loss: 126.003311, test_loss: 119.760063\n",
      "epoch: 39500, train_loss: 162.913109, test_loss: 117.441383\n",
      "epoch: 39600, train_loss: 135.812210, test_loss: 121.727310\n",
      "epoch: 39700, train_loss: 140.628380, test_loss: 119.171227\n",
      "epoch: 39800, train_loss: 161.429474, test_loss: 116.311729\n",
      "epoch: 39900, train_loss: 167.338783, test_loss: 115.133507 (Saved)\n",
      "epoch: 40000, train_loss: 123.730896, test_loss: 115.454941\n",
      "epoch: 40100, train_loss: 163.269768, test_loss: 119.909477\n",
      "epoch: 40200, train_loss: 157.694466, test_loss: 117.676056\n",
      "epoch: 40300, train_loss: 168.142715, test_loss: 116.971176\n",
      "epoch: 40400, train_loss: 142.435555, test_loss: 115.371460\n",
      "epoch: 40500, train_loss: 156.708694, test_loss: 118.033607\n",
      "epoch: 40600, train_loss: 115.249203, test_loss: 115.813210\n",
      "epoch: 40700, train_loss: 112.814049, test_loss: 114.164444 (Saved)\n",
      "epoch: 40800, train_loss: 119.710011, test_loss: 116.642090\n",
      "epoch: 40900, train_loss: 139.292145, test_loss: 115.489555\n",
      "epoch: 41000, train_loss: 117.231678, test_loss: 117.542389\n",
      "epoch: 41100, train_loss: 154.694206, test_loss: 115.791534\n",
      "epoch: 41200, train_loss: 127.014038, test_loss: 115.493874\n",
      "epoch: 41300, train_loss: 102.691961, test_loss: 114.687706\n",
      "epoch: 41400, train_loss: 163.460800, test_loss: 112.887985 (Saved)\n",
      "epoch: 41500, train_loss: 99.115427, test_loss: 114.149956\n",
      "epoch: 41600, train_loss: 139.572182, test_loss: 114.699615\n",
      "epoch: 41700, train_loss: 120.211651, test_loss: 112.898415\n",
      "epoch: 41800, train_loss: 156.813923, test_loss: 114.213394\n",
      "epoch: 41900, train_loss: 127.273952, test_loss: 118.469757\n",
      "epoch: 42000, train_loss: 120.984169, test_loss: 113.815567\n",
      "epoch: 42100, train_loss: 129.824268, test_loss: 111.096313 (Saved)\n",
      "epoch: 42200, train_loss: 122.180531, test_loss: 116.800186\n",
      "epoch: 42300, train_loss: 120.976883, test_loss: 113.605888\n",
      "epoch: 42400, train_loss: 107.905602, test_loss: 111.830826\n",
      "epoch: 42500, train_loss: 146.422142, test_loss: 111.500587\n",
      "epoch: 42600, train_loss: 140.265167, test_loss: 115.102211\n",
      "epoch: 42700, train_loss: 171.184673, test_loss: 113.097298\n",
      "epoch: 42800, train_loss: 160.888100, test_loss: 109.571671 (Saved)\n",
      "epoch: 42900, train_loss: 154.003441, test_loss: 112.768417\n",
      "epoch: 43000, train_loss: 124.111782, test_loss: 115.211967\n",
      "epoch: 43100, train_loss: 136.127670, test_loss: 114.607887\n",
      "epoch: 43200, train_loss: 113.682617, test_loss: 112.354530\n",
      "epoch: 43300, train_loss: 136.509811, test_loss: 115.959503\n",
      "epoch: 43400, train_loss: 144.429924, test_loss: 113.963600\n",
      "epoch: 43500, train_loss: 168.280029, test_loss: 113.351845\n",
      "epoch: 43600, train_loss: 107.432343, test_loss: 113.589653\n",
      "epoch: 43700, train_loss: 118.853489, test_loss: 111.080299\n",
      "epoch: 43800, train_loss: 110.476192, test_loss: 112.744072\n",
      "epoch: 43900, train_loss: 113.403111, test_loss: 112.220703\n",
      "epoch: 44000, train_loss: 116.645657, test_loss: 110.069862\n",
      "epoch: 44100, train_loss: 112.437492, test_loss: 112.311272\n",
      "epoch: 44200, train_loss: 102.252316, test_loss: 109.290977 (Saved)\n",
      "epoch: 44300, train_loss: 120.123821, test_loss: 111.216934\n",
      "epoch: 44400, train_loss: 125.240955, test_loss: 111.632477\n",
      "epoch: 44500, train_loss: 96.922880, test_loss: 113.996635\n",
      "epoch: 44600, train_loss: 140.447044, test_loss: 110.804710\n",
      "epoch: 44700, train_loss: 125.915314, test_loss: 111.032951\n",
      "epoch: 44800, train_loss: 125.231754, test_loss: 111.013145\n",
      "epoch: 44900, train_loss: 101.079563, test_loss: 113.487465\n",
      "epoch: 45000, train_loss: 154.291912, test_loss: 109.434608\n",
      "epoch: 45100, train_loss: 111.057789, test_loss: 110.828659\n",
      "epoch: 45200, train_loss: 145.097912, test_loss: 107.652382 (Saved)\n",
      "epoch: 45300, train_loss: 98.704638, test_loss: 111.135529\n",
      "epoch: 45400, train_loss: 124.324833, test_loss: 112.622749\n",
      "epoch: 45500, train_loss: 137.977139, test_loss: 108.959724\n",
      "epoch: 45600, train_loss: 108.522846, test_loss: 109.596657\n",
      "epoch: 45700, train_loss: 146.225182, test_loss: 108.953751\n",
      "epoch: 45800, train_loss: 161.276779, test_loss: 107.544243 (Saved)\n",
      "epoch: 45900, train_loss: 114.218822, test_loss: 110.624107\n",
      "epoch: 46000, train_loss: 117.886990, test_loss: 111.773354\n",
      "epoch: 46100, train_loss: 106.514591, test_loss: 110.473579\n",
      "epoch: 46200, train_loss: 99.995480, test_loss: 110.489479\n",
      "epoch: 46300, train_loss: 138.017063, test_loss: 110.809517\n",
      "epoch: 46400, train_loss: 131.060608, test_loss: 107.846786\n",
      "epoch: 46500, train_loss: 150.801552, test_loss: 109.385963\n",
      "epoch: 46600, train_loss: 131.588692, test_loss: 107.268799 (Saved)\n",
      "epoch: 46700, train_loss: 134.304535, test_loss: 107.505493\n",
      "epoch: 46800, train_loss: 141.415215, test_loss: 108.726723\n",
      "epoch: 46900, train_loss: 131.690414, test_loss: 110.111366\n",
      "epoch: 47000, train_loss: 103.714367, test_loss: 109.143959\n",
      "epoch: 47100, train_loss: 116.530540, test_loss: 109.871994\n",
      "epoch: 47200, train_loss: 140.880653, test_loss: 111.456589\n",
      "epoch: 47300, train_loss: 129.994011, test_loss: 110.443268\n",
      "epoch: 47400, train_loss: 135.488586, test_loss: 108.614357\n",
      "epoch: 47500, train_loss: 130.121082, test_loss: 106.404434 (Saved)\n",
      "epoch: 47600, train_loss: 116.773197, test_loss: 106.662270\n",
      "epoch: 47700, train_loss: 125.311245, test_loss: 108.410492\n",
      "epoch: 47800, train_loss: 147.664173, test_loss: 108.874977\n",
      "epoch: 47900, train_loss: 126.221409, test_loss: 109.320190\n",
      "epoch: 48000, train_loss: 121.531902, test_loss: 112.796417\n",
      "epoch: 48100, train_loss: 95.550409, test_loss: 108.202202\n",
      "epoch: 48200, train_loss: 112.346588, test_loss: 108.819618\n",
      "epoch: 48300, train_loss: 126.851349, test_loss: 106.169533 (Saved)\n",
      "epoch: 48400, train_loss: 83.880014, test_loss: 106.958717\n",
      "epoch: 48500, train_loss: 165.790695, test_loss: 108.455566\n",
      "epoch: 48600, train_loss: 148.375282, test_loss: 108.527809\n",
      "epoch: 48700, train_loss: 96.063856, test_loss: 107.608185\n",
      "epoch: 48800, train_loss: 108.813046, test_loss: 106.960442\n",
      "epoch: 48900, train_loss: 126.072227, test_loss: 105.888184 (Saved)\n",
      "epoch: 49000, train_loss: 128.102451, test_loss: 108.924530\n",
      "epoch: 49100, train_loss: 161.558502, test_loss: 108.099030\n",
      "epoch: 49200, train_loss: 107.756069, test_loss: 104.101860 (Saved)\n",
      "epoch: 49300, train_loss: 132.983311, test_loss: 107.923813\n",
      "epoch: 49400, train_loss: 137.274685, test_loss: 104.566704\n",
      "epoch: 49500, train_loss: 151.127182, test_loss: 105.990822\n",
      "epoch: 49600, train_loss: 142.148296, test_loss: 108.100006\n",
      "epoch: 49700, train_loss: 106.666924, test_loss: 105.020142\n",
      "epoch: 49800, train_loss: 107.380119, test_loss: 105.965309\n",
      "epoch: 49900, train_loss: 140.894402, test_loss: 106.892311\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/protanjung/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Saving ONNX model: /home/protanjung/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Exported graph: graph(%onnx::Sub_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(4, 2, strides=[2, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(16, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.weight : Float(16, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.weight : Float(4, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.weight : Float(2, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/Constant_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value=  60  274 [ CUDAFloatType{2} ], onnx_name=\"/Constant\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:16:0\n",
      "  %/Sub_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Sub[onnx_name=\"/Sub\"](%onnx::Sub_0, %/Constant_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:16:0\n",
      "  %/Constant_1_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1192   210 [ CUDAFloatType{2} ], onnx_name=\"/Constant_1\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:16:0\n",
      "  %/Div_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Div[onnx_name=\"/Div\"](%/Sub_output_0, %/Constant_1_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:16:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Div_output_0, %fc1.weight, %fc1.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc1 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh\"](%/fc1/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Tanh_output_0, %fc2.weight, %fc2.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc2 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_1_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_1\"](%/fc2/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc3/Gemm_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Tanh_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc3 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_2_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_2\"](%/fc3/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc4/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc4/Gemm\"](%/Tanh_2_output_0, %fc4.weight, %fc4.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc4 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc5/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc5/Gemm\"](%/fc4/Gemm_output_0, %fc5.weight, %fc5.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc5 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc6/Gemm_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc6/Gemm\"](%/fc5/Gemm_output_0, %fc6.weight, %fc6.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc6 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_2_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 300  850 [ CUDAFloatType{2} ], onnx_name=\"/Constant_2\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:23:0\n",
      "  %/Mul_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Mul[onnx_name=\"/Mul\"](%/fc6/Gemm_output_0, %/Constant_2_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:23:0\n",
      "  %/Constant_3_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 0 -400 [ CUDAFloatType{2} ], onnx_name=\"/Constant_3\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:23:0\n",
      "  %29 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/Add\"](%/Mul_output_0, %/Constant_3_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123053/541484460.py:23:0\n",
      "  return (%29)\n",
      "\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Saved ONNX model: /home/protanjung/icar-ng-data/model/pixel2cm_model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'pixel2cm_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 374.,  422.],\n",
      "        [1192.,  408.],\n",
      "        [1170.,  288.],\n",
      "        [1216.,  364.],\n",
      "        [ 116.,  430.],\n",
      "        [1094.,  290.],\n",
      "        [1152.,  308.],\n",
      "        [ 186.,  298.],\n",
      "        [ 172.,  354.],\n",
      "        [ 246.,  298.],\n",
      "        [ 376.,  294.],\n",
      "        [ 568.,  304.],\n",
      "        [ 306.,  484.],\n",
      "        [ 452.,  344.],\n",
      "        [ 642.,  274.],\n",
      "        [1262.,  276.],\n",
      "        [ 642.,  332.],\n",
      "        [ 494.,  304.]], device='cuda:0')\n",
      "y\n",
      "tensor([[  50.,  100.],\n",
      "        [  50., -200.],\n",
      "        [ 250., -350.],\n",
      "        [ 100., -250.],\n",
      "        [  50.,  200.],\n",
      "        [ 250., -300.],\n",
      "        [ 200., -300.],\n",
      "        [ 300.,  350.],\n",
      "        [ 150.,  250.],\n",
      "        [ 300.,  300.],\n",
      "        [ 300.,  200.],\n",
      "        [ 250.,   50.],\n",
      "        [   0.,  100.],\n",
      "        [ 150.,  100.],\n",
      "        [ 300.,    0.],\n",
      "        [ 300., -450.],\n",
      "        [ 150.,    0.],\n",
      "        [ 250.,  100.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[  45.3106,   96.0662],\n",
      "        [  43.1142, -180.4718],\n",
      "        [ 252.5845, -349.2013],\n",
      "        [  93.4807, -243.5412],\n",
      "        [  47.3886,  191.2451],\n",
      "        [ 249.1367, -299.6594],\n",
      "        [ 200.2229, -304.4657],\n",
      "        [ 291.6025,  337.7297],\n",
      "        [ 149.0193,  257.1096],\n",
      "        [ 284.7269,  292.2072],\n",
      "        [ 283.7198,  199.3419],\n",
      "        [ 237.1167,   53.0305],\n",
      "        [   3.5990,   78.7831],\n",
      "        [ 149.7437,  105.9028],\n",
      "        [ 325.6958,    7.4353],\n",
      "        [ 285.6842, -427.9204],\n",
      "        [ 163.9593,    1.0199],\n",
      "        [ 242.7998,  104.3897]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[  4.6894,   3.9338],\n",
      "        [  6.8858, -19.5282],\n",
      "        [ -2.5845,  -0.7987],\n",
      "        [  6.5193,  -6.4588],\n",
      "        [  2.6114,   8.7549],\n",
      "        [  0.8633,  -0.3406],\n",
      "        [ -0.2229,   4.4657],\n",
      "        [  8.3975,  12.2703],\n",
      "        [  0.9807,  -7.1096],\n",
      "        [ 15.2731,   7.7928],\n",
      "        [ 16.2802,   0.6581],\n",
      "        [ 12.8833,  -3.0305],\n",
      "        [ -3.5990,  21.2169],\n",
      "        [  0.2563,  -5.9028],\n",
      "        [-25.6958,  -7.4353],\n",
      "        [ 14.3158, -22.0796],\n",
      "        [-13.9593,  -1.0199],\n",
      "        [  7.2002,  -4.3897]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
