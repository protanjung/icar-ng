{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 06/11/2023 at 21:44:12.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/icar/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/icar/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/icar/icar-ng-data/model\n",
      "Model file: pixel2cm_model.pt\n",
      "Model path: /home/icar/icar-ng-data/model/pixel2cm_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'pixel2cm_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([ 30., 204.]), max_x: tensor([1272.,  692.])\n",
      "min_y: tensor([   0.0000, -848.5281]), max_y: tensor([1200.0000,  665.6403])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 0:2].values\n",
    "        y = self.dataframe.iloc[:, 2:4].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 118\n",
      "dataset_test: 29\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 20)\n",
    "        self.fc3 = nn.Linear(20, 80)\n",
    "        self.fc4 = nn.Linear(80, 20)\n",
    "        self.fc5 = nn.Linear(20, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/icar/icar-ng-data/model/pixel2cm_model.pt\n",
      "Loaded model: /home/icar/icar-ng-data/model/pixel2cm_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 99745.433594, test_loss: 49039.175781 (Saved)\n",
      "epoch: 100, train_loss: 59294.900391, test_loss: 21118.013672 (Saved)\n",
      "epoch: 200, train_loss: 50584.414062, test_loss: 17335.777344 (Saved)\n",
      "epoch: 300, train_loss: 45350.251953, test_loss: 15263.666016 (Saved)\n",
      "epoch: 400, train_loss: 38816.373047, test_loss: 13604.242188 (Saved)\n",
      "epoch: 500, train_loss: 33330.827148, test_loss: 12273.928711 (Saved)\n",
      "epoch: 600, train_loss: 28808.886719, test_loss: 11175.048828 (Saved)\n",
      "epoch: 700, train_loss: 24233.949219, test_loss: 10210.197266 (Saved)\n",
      "epoch: 800, train_loss: 20276.033203, test_loss: 9277.875000 (Saved)\n",
      "epoch: 900, train_loss: 17023.334473, test_loss: 8285.533203 (Saved)\n",
      "epoch: 1000, train_loss: 14429.555664, test_loss: 7242.764160 (Saved)\n",
      "epoch: 1100, train_loss: 12476.355957, test_loss: 6385.020508 (Saved)\n",
      "epoch: 1200, train_loss: 10768.278809, test_loss: 5676.866699 (Saved)\n",
      "epoch: 1300, train_loss: 9192.306641, test_loss: 4916.367676 (Saved)\n",
      "epoch: 1400, train_loss: 7885.403320, test_loss: 4249.076172 (Saved)\n",
      "epoch: 1500, train_loss: 6749.453613, test_loss: 3655.256836 (Saved)\n",
      "epoch: 1600, train_loss: 5790.280762, test_loss: 3236.073486 (Saved)\n",
      "epoch: 1700, train_loss: 5344.454346, test_loss: 2899.376709 (Saved)\n",
      "epoch: 1800, train_loss: 4822.996826, test_loss: 2608.765137 (Saved)\n",
      "epoch: 1900, train_loss: 4469.959473, test_loss: 2346.745605 (Saved)\n",
      "epoch: 2000, train_loss: 4023.825439, test_loss: 2152.864502 (Saved)\n",
      "epoch: 2100, train_loss: 3795.251221, test_loss: 1988.012085 (Saved)\n",
      "epoch: 2200, train_loss: 3499.484985, test_loss: 1837.069824 (Saved)\n",
      "epoch: 2300, train_loss: 3265.090576, test_loss: 1730.609009 (Saved)\n",
      "epoch: 2400, train_loss: 3238.908447, test_loss: 1637.178589 (Saved)\n",
      "epoch: 2500, train_loss: 2955.916626, test_loss: 1562.525269 (Saved)\n",
      "epoch: 2600, train_loss: 2934.883301, test_loss: 1500.734009 (Saved)\n",
      "epoch: 2700, train_loss: 2687.702148, test_loss: 1456.597412 (Saved)\n",
      "epoch: 2800, train_loss: 2659.405029, test_loss: 1407.692871 (Saved)\n",
      "epoch: 2900, train_loss: 2577.663208, test_loss: 1365.746216 (Saved)\n",
      "epoch: 3000, train_loss: 2470.831299, test_loss: 1323.046387 (Saved)\n",
      "epoch: 3100, train_loss: 2272.872742, test_loss: 1286.062866 (Saved)\n",
      "epoch: 3200, train_loss: 2259.499695, test_loss: 1251.004028 (Saved)\n",
      "epoch: 3300, train_loss: 2211.048584, test_loss: 1215.982178 (Saved)\n",
      "epoch: 3400, train_loss: 2025.855896, test_loss: 1188.459351 (Saved)\n",
      "epoch: 3500, train_loss: 1995.299988, test_loss: 1156.177856 (Saved)\n",
      "epoch: 3600, train_loss: 1880.694519, test_loss: 1124.206299 (Saved)\n",
      "epoch: 3700, train_loss: 1842.017639, test_loss: 1095.979370 (Saved)\n",
      "epoch: 3800, train_loss: 1757.048218, test_loss: 1068.210571 (Saved)\n",
      "epoch: 3900, train_loss: 1702.846802, test_loss: 1036.897095 (Saved)\n",
      "epoch: 4000, train_loss: 1606.599243, test_loss: 1006.304321 (Saved)\n",
      "epoch: 4100, train_loss: 1602.787598, test_loss: 979.528442 (Saved)\n",
      "epoch: 4200, train_loss: 1549.696533, test_loss: 946.199768 (Saved)\n",
      "epoch: 4300, train_loss: 1481.824219, test_loss: 916.780823 (Saved)\n",
      "epoch: 4400, train_loss: 1424.598938, test_loss: 890.488403 (Saved)\n",
      "epoch: 4500, train_loss: 1364.237305, test_loss: 863.739014 (Saved)\n",
      "epoch: 4600, train_loss: 1286.777527, test_loss: 836.275330 (Saved)\n",
      "epoch: 4700, train_loss: 1243.012634, test_loss: 810.355591 (Saved)\n",
      "epoch: 4800, train_loss: 1210.389587, test_loss: 777.951477 (Saved)\n",
      "epoch: 4900, train_loss: 1125.897766, test_loss: 750.246155 (Saved)\n",
      "epoch: 5000, train_loss: 1096.413727, test_loss: 726.028259 (Saved)\n",
      "epoch: 5100, train_loss: 1090.658936, test_loss: 700.525696 (Saved)\n",
      "epoch: 5200, train_loss: 1021.548004, test_loss: 671.989197 (Saved)\n",
      "epoch: 5300, train_loss: 988.580780, test_loss: 653.043762 (Saved)\n",
      "epoch: 5400, train_loss: 927.782501, test_loss: 635.031677 (Saved)\n",
      "epoch: 5500, train_loss: 932.547485, test_loss: 614.096985 (Saved)\n",
      "epoch: 5600, train_loss: 908.113647, test_loss: 588.568665 (Saved)\n",
      "epoch: 5700, train_loss: 843.369995, test_loss: 568.297424 (Saved)\n",
      "epoch: 5800, train_loss: 799.627350, test_loss: 546.178833 (Saved)\n",
      "epoch: 5900, train_loss: 778.016846, test_loss: 523.781799 (Saved)\n",
      "epoch: 6000, train_loss: 750.256042, test_loss: 505.467712 (Saved)\n",
      "epoch: 6100, train_loss: 698.676544, test_loss: 486.073181 (Saved)\n",
      "epoch: 6200, train_loss: 683.360107, test_loss: 468.030487 (Saved)\n",
      "epoch: 6300, train_loss: 651.909668, test_loss: 454.144592 (Saved)\n",
      "epoch: 6400, train_loss: 621.955841, test_loss: 441.285522 (Saved)\n",
      "epoch: 6500, train_loss: 603.861298, test_loss: 427.592804 (Saved)\n",
      "epoch: 6600, train_loss: 577.627609, test_loss: 415.351440 (Saved)\n",
      "epoch: 6700, train_loss: 542.601700, test_loss: 402.739319 (Saved)\n",
      "epoch: 6800, train_loss: 526.848373, test_loss: 394.807678 (Saved)\n",
      "epoch: 6900, train_loss: 512.748474, test_loss: 385.709381 (Saved)\n",
      "epoch: 7000, train_loss: 483.432404, test_loss: 375.770477 (Saved)\n",
      "epoch: 7100, train_loss: 464.229248, test_loss: 371.558716 (Saved)\n",
      "epoch: 7200, train_loss: 449.252075, test_loss: 365.833160 (Saved)\n",
      "epoch: 7300, train_loss: 424.643982, test_loss: 360.392365 (Saved)\n",
      "epoch: 7400, train_loss: 407.600647, test_loss: 353.272980 (Saved)\n",
      "epoch: 7500, train_loss: 396.660614, test_loss: 346.105225 (Saved)\n",
      "epoch: 7600, train_loss: 374.791733, test_loss: 339.060608 (Saved)\n",
      "epoch: 7700, train_loss: 362.567047, test_loss: 334.805817 (Saved)\n",
      "epoch: 7800, train_loss: 354.664719, test_loss: 332.968475 (Saved)\n",
      "epoch: 7900, train_loss: 350.913818, test_loss: 327.858551 (Saved)\n",
      "epoch: 8000, train_loss: 334.783539, test_loss: 325.139038 (Saved)\n",
      "epoch: 8100, train_loss: 319.191452, test_loss: 322.277039 (Saved)\n",
      "epoch: 8200, train_loss: 313.024139, test_loss: 319.792419 (Saved)\n",
      "epoch: 8300, train_loss: 307.641052, test_loss: 314.432648 (Saved)\n",
      "epoch: 8400, train_loss: 291.373734, test_loss: 313.421112 (Saved)\n",
      "epoch: 8500, train_loss: 283.816269, test_loss: 310.598267 (Saved)\n",
      "epoch: 8600, train_loss: 276.631218, test_loss: 307.031555 (Saved)\n",
      "epoch: 8700, train_loss: 270.800293, test_loss: 305.797974 (Saved)\n",
      "epoch: 8800, train_loss: 260.152855, test_loss: 304.643890 (Saved)\n",
      "epoch: 8900, train_loss: 253.586624, test_loss: 304.606781 (Saved)\n",
      "epoch: 9000, train_loss: 247.855247, test_loss: 302.828125 (Saved)\n",
      "epoch: 9100, train_loss: 240.566757, test_loss: 300.954163 (Saved)\n",
      "epoch: 9200, train_loss: 240.125206, test_loss: 300.251404 (Saved)\n",
      "epoch: 9300, train_loss: 232.480537, test_loss: 297.681610 (Saved)\n",
      "epoch: 9400, train_loss: 227.638115, test_loss: 296.757935 (Saved)\n",
      "epoch: 9500, train_loss: 224.312080, test_loss: 294.488708 (Saved)\n",
      "epoch: 9600, train_loss: 219.830757, test_loss: 292.961182 (Saved)\n",
      "epoch: 9700, train_loss: 219.006737, test_loss: 292.982361\n",
      "epoch: 9800, train_loss: 213.490166, test_loss: 290.864288 (Saved)\n",
      "epoch: 9900, train_loss: 209.559250, test_loss: 289.651733 (Saved)\n",
      "epoch: 10000, train_loss: 205.088951, test_loss: 289.673004\n",
      "epoch: 10100, train_loss: 200.129555, test_loss: 288.846802 (Saved)\n",
      "epoch: 10200, train_loss: 196.227592, test_loss: 286.621674 (Saved)\n",
      "epoch: 10300, train_loss: 190.313515, test_loss: 285.710388 (Saved)\n",
      "epoch: 10400, train_loss: 190.533310, test_loss: 285.014923 (Saved)\n",
      "epoch: 10500, train_loss: 183.724289, test_loss: 283.126648 (Saved)\n",
      "epoch: 10600, train_loss: 181.982300, test_loss: 281.559601 (Saved)\n",
      "epoch: 10700, train_loss: 180.563255, test_loss: 282.713531\n",
      "epoch: 10800, train_loss: 174.422508, test_loss: 281.827972\n",
      "epoch: 10900, train_loss: 178.481522, test_loss: 279.346344 (Saved)\n",
      "epoch: 11000, train_loss: 170.594353, test_loss: 279.008820 (Saved)\n",
      "epoch: 11100, train_loss: 169.113426, test_loss: 278.996704 (Saved)\n",
      "epoch: 11200, train_loss: 165.971588, test_loss: 278.482788 (Saved)\n",
      "epoch: 11300, train_loss: 165.576286, test_loss: 276.608856 (Saved)\n",
      "epoch: 11400, train_loss: 166.366409, test_loss: 276.091827 (Saved)\n",
      "epoch: 11500, train_loss: 161.336273, test_loss: 275.323029 (Saved)\n",
      "epoch: 11600, train_loss: 158.774414, test_loss: 274.656342 (Saved)\n",
      "epoch: 11700, train_loss: 159.753815, test_loss: 274.270416 (Saved)\n",
      "epoch: 11800, train_loss: 160.639832, test_loss: 273.996857 (Saved)\n",
      "epoch: 11900, train_loss: 155.985428, test_loss: 275.091339\n",
      "epoch: 12000, train_loss: 154.737999, test_loss: 273.677368 (Saved)\n",
      "epoch: 12100, train_loss: 154.835022, test_loss: 273.562408 (Saved)\n",
      "epoch: 12200, train_loss: 154.614765, test_loss: 273.545135 (Saved)\n",
      "epoch: 12300, train_loss: 152.207634, test_loss: 274.551636\n",
      "epoch: 12400, train_loss: 151.007790, test_loss: 273.224152 (Saved)\n",
      "epoch: 12500, train_loss: 147.418892, test_loss: 272.470673 (Saved)\n",
      "epoch: 12600, train_loss: 144.125294, test_loss: 272.825500\n",
      "epoch: 12700, train_loss: 146.082161, test_loss: 272.691071\n",
      "epoch: 12800, train_loss: 148.644669, test_loss: 271.698456 (Saved)\n",
      "epoch: 12900, train_loss: 145.467148, test_loss: 271.597137 (Saved)\n",
      "epoch: 13000, train_loss: 144.613396, test_loss: 271.639038\n",
      "epoch: 13100, train_loss: 144.165169, test_loss: 271.006165 (Saved)\n",
      "epoch: 13200, train_loss: 141.161430, test_loss: 271.054810\n",
      "epoch: 13300, train_loss: 141.018768, test_loss: 270.652252 (Saved)\n",
      "epoch: 13400, train_loss: 143.533691, test_loss: 270.325226 (Saved)\n",
      "epoch: 13500, train_loss: 139.531178, test_loss: 270.106354 (Saved)\n",
      "epoch: 13600, train_loss: 142.819370, test_loss: 270.797668\n",
      "epoch: 13700, train_loss: 138.234806, test_loss: 270.118042\n",
      "epoch: 13800, train_loss: 142.465336, test_loss: 269.904388 (Saved)\n",
      "epoch: 13900, train_loss: 137.238934, test_loss: 270.508942\n",
      "epoch: 14000, train_loss: 134.075851, test_loss: 270.204559\n",
      "epoch: 14100, train_loss: 136.625137, test_loss: 269.627625 (Saved)\n",
      "epoch: 14200, train_loss: 136.073303, test_loss: 270.384338\n",
      "epoch: 14300, train_loss: 135.599228, test_loss: 269.463409 (Saved)\n",
      "epoch: 14400, train_loss: 132.950310, test_loss: 269.052948 (Saved)\n",
      "epoch: 14500, train_loss: 134.248985, test_loss: 269.003662 (Saved)\n",
      "epoch: 14600, train_loss: 135.398628, test_loss: 269.272705\n",
      "epoch: 14700, train_loss: 132.042389, test_loss: 269.330658\n",
      "epoch: 14800, train_loss: 135.068748, test_loss: 269.637054\n",
      "epoch: 14900, train_loss: 134.683746, test_loss: 268.485748 (Saved)\n",
      "epoch: 15000, train_loss: 132.539284, test_loss: 268.502258\n",
      "epoch: 15100, train_loss: 136.531101, test_loss: 269.376617\n",
      "epoch: 15200, train_loss: 132.101791, test_loss: 268.967499\n",
      "epoch: 15300, train_loss: 131.097637, test_loss: 267.919708 (Saved)\n",
      "epoch: 15400, train_loss: 130.417576, test_loss: 267.996613\n",
      "epoch: 15500, train_loss: 128.986954, test_loss: 267.543915 (Saved)\n",
      "epoch: 15600, train_loss: 130.109009, test_loss: 268.491516\n",
      "epoch: 15700, train_loss: 133.274250, test_loss: 267.933228\n",
      "epoch: 15800, train_loss: 132.357159, test_loss: 268.069641\n",
      "epoch: 15900, train_loss: 131.036503, test_loss: 268.658447\n",
      "epoch: 16000, train_loss: 133.064865, test_loss: 268.100128\n",
      "epoch: 16100, train_loss: 129.881699, test_loss: 267.930786\n",
      "epoch: 16200, train_loss: 126.733032, test_loss: 267.876953\n",
      "epoch: 16300, train_loss: 126.986835, test_loss: 267.664490\n",
      "epoch: 16400, train_loss: 127.308861, test_loss: 266.878296 (Saved)\n",
      "epoch: 16500, train_loss: 130.417351, test_loss: 268.164856\n",
      "epoch: 16600, train_loss: 129.306114, test_loss: 267.933411\n",
      "epoch: 16700, train_loss: 123.997410, test_loss: 267.467041\n",
      "epoch: 16800, train_loss: 130.174660, test_loss: 267.681641\n",
      "epoch: 16900, train_loss: 128.268776, test_loss: 266.580536 (Saved)\n",
      "epoch: 17000, train_loss: 125.284954, test_loss: 267.106262\n",
      "epoch: 17100, train_loss: 127.186386, test_loss: 266.760010\n",
      "epoch: 17200, train_loss: 128.252583, test_loss: 267.009369\n",
      "epoch: 17300, train_loss: 125.194115, test_loss: 267.371155\n",
      "epoch: 17400, train_loss: 124.072243, test_loss: 267.500275\n",
      "epoch: 17500, train_loss: 124.755085, test_loss: 267.214294\n",
      "epoch: 17600, train_loss: 125.576622, test_loss: 267.506622\n",
      "epoch: 17700, train_loss: 123.188702, test_loss: 266.234894 (Saved)\n",
      "epoch: 17800, train_loss: 122.096184, test_loss: 267.297974\n",
      "epoch: 17900, train_loss: 121.140923, test_loss: 266.651367\n",
      "epoch: 18000, train_loss: 123.387939, test_loss: 267.299530\n",
      "epoch: 18100, train_loss: 125.703758, test_loss: 266.405151\n",
      "epoch: 18200, train_loss: 120.895260, test_loss: 266.804779\n",
      "epoch: 18300, train_loss: 120.952358, test_loss: 266.864960\n",
      "epoch: 18400, train_loss: 121.289665, test_loss: 266.151337 (Saved)\n",
      "epoch: 18500, train_loss: 120.808765, test_loss: 267.409058\n",
      "epoch: 18600, train_loss: 118.519577, test_loss: 266.519684\n",
      "epoch: 18700, train_loss: 121.793392, test_loss: 265.782074 (Saved)\n",
      "epoch: 18800, train_loss: 121.885738, test_loss: 265.930695\n",
      "epoch: 18900, train_loss: 122.750542, test_loss: 266.761719\n",
      "epoch: 19000, train_loss: 120.677147, test_loss: 267.546906\n",
      "epoch: 19100, train_loss: 122.601280, test_loss: 266.325714\n",
      "epoch: 19200, train_loss: 120.731804, test_loss: 266.742798\n",
      "epoch: 19300, train_loss: 120.877987, test_loss: 266.777649\n",
      "epoch: 19400, train_loss: 123.986053, test_loss: 267.185791\n",
      "epoch: 19500, train_loss: 123.089455, test_loss: 266.533203\n",
      "epoch: 19600, train_loss: 120.831284, test_loss: 266.316681\n",
      "epoch: 19700, train_loss: 122.101997, test_loss: 265.854401\n",
      "epoch: 19800, train_loss: 114.916733, test_loss: 266.649658\n",
      "epoch: 19900, train_loss: 118.383270, test_loss: 265.990356\n",
      "epoch: 20000, train_loss: 120.123398, test_loss: 266.534088\n",
      "epoch: 20100, train_loss: 118.634762, test_loss: 266.101349\n",
      "epoch: 20200, train_loss: 118.141476, test_loss: 266.669189\n",
      "epoch: 20300, train_loss: 119.558167, test_loss: 266.580658\n",
      "epoch: 20400, train_loss: 116.894646, test_loss: 266.713684\n",
      "epoch: 20500, train_loss: 118.799519, test_loss: 267.071899\n",
      "epoch: 20600, train_loss: 117.166862, test_loss: 266.311127\n",
      "epoch: 20700, train_loss: 114.975880, test_loss: 266.566315\n",
      "epoch: 20800, train_loss: 114.136944, test_loss: 267.251923\n",
      "epoch: 20900, train_loss: 117.563385, test_loss: 268.260590\n",
      "epoch: 21000, train_loss: 118.462132, test_loss: 266.737823\n",
      "epoch: 21100, train_loss: 118.148037, test_loss: 267.708160\n",
      "epoch: 21200, train_loss: 118.472500, test_loss: 266.167908\n",
      "epoch: 21300, train_loss: 116.381653, test_loss: 266.845154\n",
      "epoch: 21400, train_loss: 118.335606, test_loss: 266.959045\n",
      "epoch: 21500, train_loss: 116.513332, test_loss: 267.149536\n",
      "epoch: 21600, train_loss: 118.217926, test_loss: 267.973541\n",
      "epoch: 21700, train_loss: 116.515594, test_loss: 267.877014\n",
      "epoch: 21800, train_loss: 112.316780, test_loss: 267.287750\n",
      "epoch: 21900, train_loss: 117.499138, test_loss: 267.771515\n",
      "epoch: 22000, train_loss: 115.871029, test_loss: 267.712646\n",
      "epoch: 22100, train_loss: 118.684196, test_loss: 267.908142\n",
      "epoch: 22200, train_loss: 115.415653, test_loss: 268.664520\n",
      "epoch: 22300, train_loss: 114.098961, test_loss: 268.190948\n",
      "epoch: 22400, train_loss: 116.084766, test_loss: 268.146118\n",
      "epoch: 22500, train_loss: 113.676506, test_loss: 267.454132\n",
      "epoch: 22600, train_loss: 116.158401, test_loss: 268.197052\n",
      "epoch: 22700, train_loss: 115.057434, test_loss: 266.448212\n",
      "epoch: 22800, train_loss: 111.232048, test_loss: 266.755157\n",
      "epoch: 22900, train_loss: 116.517704, test_loss: 266.994507\n",
      "epoch: 23000, train_loss: 113.769402, test_loss: 267.311432\n",
      "epoch: 23100, train_loss: 113.784752, test_loss: 266.761841\n",
      "epoch: 23200, train_loss: 113.038380, test_loss: 267.139679\n",
      "epoch: 23300, train_loss: 116.221947, test_loss: 265.979889\n",
      "epoch: 23400, train_loss: 116.702034, test_loss: 266.876404\n",
      "epoch: 23500, train_loss: 112.955559, test_loss: 267.377594\n",
      "epoch: 23600, train_loss: 112.445110, test_loss: 267.721588\n",
      "epoch: 23700, train_loss: 114.617310, test_loss: 266.988861\n",
      "epoch: 23800, train_loss: 114.897411, test_loss: 267.793762\n",
      "epoch: 23900, train_loss: 116.029770, test_loss: 266.773254\n",
      "epoch: 24000, train_loss: 113.902859, test_loss: 267.195526\n",
      "epoch: 24100, train_loss: 117.017666, test_loss: 267.309723\n",
      "epoch: 24200, train_loss: 114.730602, test_loss: 266.130432\n",
      "epoch: 24300, train_loss: 117.184719, test_loss: 267.895844\n",
      "epoch: 24400, train_loss: 112.435162, test_loss: 266.756989\n",
      "epoch: 24500, train_loss: 111.955803, test_loss: 266.472321\n",
      "epoch: 24600, train_loss: 110.557404, test_loss: 267.340179\n",
      "epoch: 24700, train_loss: 113.584801, test_loss: 268.426941\n",
      "epoch: 24800, train_loss: 114.149261, test_loss: 267.232697\n",
      "epoch: 24900, train_loss: 112.198795, test_loss: 266.364532\n",
      "epoch: 25000, train_loss: 112.179688, test_loss: 266.130463\n",
      "epoch: 25100, train_loss: 109.647842, test_loss: 267.186920\n",
      "epoch: 25200, train_loss: 112.480640, test_loss: 266.191071\n",
      "epoch: 25300, train_loss: 111.351780, test_loss: 266.780640\n",
      "epoch: 25400, train_loss: 113.620934, test_loss: 265.947388\n",
      "epoch: 25500, train_loss: 114.570194, test_loss: 266.696381\n",
      "epoch: 25600, train_loss: 109.483562, test_loss: 266.596893\n",
      "epoch: 25700, train_loss: 111.077835, test_loss: 267.291718\n",
      "epoch: 25800, train_loss: 113.980114, test_loss: 267.181549\n",
      "epoch: 25900, train_loss: 111.648773, test_loss: 267.287231\n",
      "epoch: 26000, train_loss: 111.214661, test_loss: 269.211182\n",
      "epoch: 26100, train_loss: 109.524010, test_loss: 269.441925\n",
      "epoch: 26200, train_loss: 113.728180, test_loss: 269.574310\n",
      "epoch: 26300, train_loss: 112.408859, test_loss: 269.985413\n",
      "epoch: 26400, train_loss: 113.050991, test_loss: 268.943115\n",
      "epoch: 26500, train_loss: 114.252419, test_loss: 269.359680\n",
      "epoch: 26600, train_loss: 109.332485, test_loss: 268.805603\n",
      "epoch: 26700, train_loss: 111.394279, test_loss: 268.472870\n",
      "epoch: 26800, train_loss: 109.768555, test_loss: 268.364288\n",
      "epoch: 26900, train_loss: 109.455967, test_loss: 267.798828\n",
      "epoch: 27000, train_loss: 110.662094, test_loss: 268.549377\n",
      "epoch: 27100, train_loss: 112.662430, test_loss: 268.255585\n",
      "epoch: 27200, train_loss: 111.830769, test_loss: 268.575348\n",
      "epoch: 27300, train_loss: 111.663258, test_loss: 268.595520\n",
      "epoch: 27400, train_loss: 111.420036, test_loss: 267.371216\n",
      "epoch: 27500, train_loss: 111.190796, test_loss: 268.145874\n",
      "epoch: 27600, train_loss: 109.570114, test_loss: 268.471832\n",
      "epoch: 27700, train_loss: 112.046116, test_loss: 267.355072\n",
      "epoch: 27800, train_loss: 110.397095, test_loss: 267.008606\n",
      "epoch: 27900, train_loss: 112.500763, test_loss: 268.304657\n",
      "epoch: 28000, train_loss: 108.261696, test_loss: 267.225647\n",
      "epoch: 28100, train_loss: 109.379021, test_loss: 268.076050\n",
      "epoch: 28200, train_loss: 111.359711, test_loss: 267.209625\n",
      "epoch: 28300, train_loss: 111.406502, test_loss: 267.761383\n",
      "epoch: 28400, train_loss: 110.943672, test_loss: 266.834717\n",
      "epoch: 28500, train_loss: 107.660007, test_loss: 267.105988\n",
      "epoch: 28600, train_loss: 111.908363, test_loss: 266.954010\n",
      "epoch: 28700, train_loss: 109.754150, test_loss: 267.026672\n",
      "epoch: 28800, train_loss: 111.452408, test_loss: 268.093872\n",
      "epoch: 28900, train_loss: 109.842186, test_loss: 266.653839\n",
      "epoch: 29000, train_loss: 112.450333, test_loss: 266.618561\n",
      "epoch: 29100, train_loss: 107.257442, test_loss: 267.406219\n",
      "epoch: 29200, train_loss: 108.735249, test_loss: 267.005432\n",
      "epoch: 29300, train_loss: 109.341095, test_loss: 266.160461\n",
      "epoch: 29400, train_loss: 108.176010, test_loss: 266.958893\n",
      "epoch: 29500, train_loss: 109.325195, test_loss: 267.890076\n",
      "epoch: 29600, train_loss: 109.382053, test_loss: 267.344604\n",
      "epoch: 29700, train_loss: 112.620033, test_loss: 266.309631\n",
      "epoch: 29800, train_loss: 109.981548, test_loss: 267.131134\n",
      "epoch: 29900, train_loss: 105.798412, test_loss: 266.981384\n",
      "epoch: 30000, train_loss: 110.176563, test_loss: 266.793243\n",
      "epoch: 30100, train_loss: 108.063805, test_loss: 265.799194\n",
      "epoch: 30200, train_loss: 106.947407, test_loss: 267.324890\n",
      "epoch: 30300, train_loss: 108.216805, test_loss: 266.786499\n",
      "epoch: 30400, train_loss: 110.527840, test_loss: 266.165070\n",
      "epoch: 30500, train_loss: 105.791828, test_loss: 266.988708\n",
      "epoch: 30600, train_loss: 107.007153, test_loss: 266.388733\n",
      "epoch: 30700, train_loss: 108.532726, test_loss: 267.642944\n",
      "epoch: 30800, train_loss: 106.617657, test_loss: 266.735229\n",
      "epoch: 30900, train_loss: 107.904087, test_loss: 265.580597 (Saved)\n",
      "epoch: 31000, train_loss: 107.036083, test_loss: 266.922302\n",
      "epoch: 31100, train_loss: 106.461887, test_loss: 266.212860\n",
      "epoch: 31200, train_loss: 106.718307, test_loss: 265.850311\n",
      "epoch: 31300, train_loss: 111.530243, test_loss: 266.544952\n",
      "epoch: 31400, train_loss: 110.291321, test_loss: 266.423523\n",
      "epoch: 31500, train_loss: 106.848988, test_loss: 265.983643\n",
      "epoch: 31600, train_loss: 106.744675, test_loss: 266.009796\n",
      "epoch: 31700, train_loss: 104.305559, test_loss: 265.964813\n",
      "epoch: 31800, train_loss: 107.047245, test_loss: 265.769104\n",
      "epoch: 31900, train_loss: 108.457905, test_loss: 266.159424\n",
      "epoch: 32000, train_loss: 108.358921, test_loss: 264.866760 (Saved)\n",
      "epoch: 32100, train_loss: 106.705112, test_loss: 266.004486\n",
      "epoch: 32200, train_loss: 106.662277, test_loss: 265.438690\n",
      "epoch: 32300, train_loss: 104.802162, test_loss: 265.347717\n",
      "epoch: 32400, train_loss: 107.264599, test_loss: 264.602295 (Saved)\n",
      "epoch: 32500, train_loss: 105.896980, test_loss: 265.921387\n",
      "epoch: 32600, train_loss: 107.444538, test_loss: 265.314789\n",
      "epoch: 32700, train_loss: 107.810631, test_loss: 265.441040\n",
      "epoch: 32800, train_loss: 106.666355, test_loss: 264.916565\n",
      "epoch: 32900, train_loss: 108.708187, test_loss: 265.125031\n",
      "epoch: 33000, train_loss: 107.833885, test_loss: 266.000885\n",
      "epoch: 33100, train_loss: 105.281231, test_loss: 266.195190\n",
      "epoch: 33200, train_loss: 108.332024, test_loss: 265.748932\n",
      "epoch: 33300, train_loss: 107.932194, test_loss: 265.277588\n",
      "epoch: 33400, train_loss: 107.667385, test_loss: 264.363617 (Saved)\n",
      "epoch: 33500, train_loss: 105.864826, test_loss: 265.024841\n",
      "epoch: 33600, train_loss: 109.345467, test_loss: 265.144470\n",
      "epoch: 33700, train_loss: 107.009060, test_loss: 265.059692\n",
      "epoch: 33800, train_loss: 106.075619, test_loss: 264.633545\n",
      "epoch: 33900, train_loss: 109.248714, test_loss: 264.187439 (Saved)\n",
      "epoch: 34000, train_loss: 105.737476, test_loss: 264.868408\n",
      "epoch: 34100, train_loss: 104.995655, test_loss: 264.975189\n",
      "epoch: 34200, train_loss: 107.904144, test_loss: 265.228638\n",
      "epoch: 34300, train_loss: 109.466408, test_loss: 263.737762 (Saved)\n",
      "epoch: 34400, train_loss: 108.176773, test_loss: 263.697174 (Saved)\n",
      "epoch: 34500, train_loss: 107.430855, test_loss: 263.410614 (Saved)\n",
      "epoch: 34600, train_loss: 110.151020, test_loss: 264.335144\n",
      "epoch: 34700, train_loss: 107.321800, test_loss: 264.781616\n",
      "epoch: 34800, train_loss: 106.384506, test_loss: 264.579407\n",
      "epoch: 34900, train_loss: 105.076183, test_loss: 262.875092 (Saved)\n",
      "epoch: 35000, train_loss: 108.107628, test_loss: 263.164459\n",
      "epoch: 35100, train_loss: 108.831505, test_loss: 263.623322\n",
      "epoch: 35200, train_loss: 106.346867, test_loss: 263.178528\n",
      "epoch: 35300, train_loss: 107.258007, test_loss: 264.265106\n",
      "epoch: 35400, train_loss: 105.626530, test_loss: 263.932495\n",
      "epoch: 35500, train_loss: 105.734787, test_loss: 263.684601\n",
      "epoch: 35600, train_loss: 103.104542, test_loss: 263.283203\n",
      "epoch: 35700, train_loss: 106.758438, test_loss: 262.814178 (Saved)\n",
      "epoch: 35800, train_loss: 108.774796, test_loss: 263.680756\n",
      "epoch: 35900, train_loss: 105.491798, test_loss: 263.091522\n",
      "epoch: 36000, train_loss: 109.610088, test_loss: 262.771759 (Saved)\n",
      "epoch: 36100, train_loss: 105.663910, test_loss: 263.481262\n",
      "epoch: 36200, train_loss: 107.684868, test_loss: 261.876068 (Saved)\n",
      "epoch: 36300, train_loss: 109.330490, test_loss: 262.457886\n",
      "epoch: 36400, train_loss: 102.899265, test_loss: 262.317474\n",
      "epoch: 36500, train_loss: 104.419540, test_loss: 262.936737\n",
      "epoch: 36600, train_loss: 107.198254, test_loss: 262.313751\n",
      "epoch: 36700, train_loss: 103.354923, test_loss: 263.004303\n",
      "epoch: 36800, train_loss: 105.492439, test_loss: 262.457916\n",
      "epoch: 36900, train_loss: 106.188457, test_loss: 262.915924\n",
      "epoch: 37000, train_loss: 104.746964, test_loss: 262.745728\n",
      "epoch: 37100, train_loss: 104.651447, test_loss: 262.310913\n",
      "epoch: 37200, train_loss: 106.616043, test_loss: 262.155304\n",
      "epoch: 37300, train_loss: 106.533108, test_loss: 262.678040\n",
      "epoch: 37400, train_loss: 106.581833, test_loss: 262.801636\n",
      "epoch: 37500, train_loss: 104.635262, test_loss: 263.385620\n",
      "epoch: 37600, train_loss: 102.796356, test_loss: 262.560364\n",
      "epoch: 37700, train_loss: 107.901447, test_loss: 261.868378 (Saved)\n",
      "epoch: 37800, train_loss: 106.734344, test_loss: 261.761871 (Saved)\n",
      "epoch: 37900, train_loss: 106.996048, test_loss: 262.949036\n",
      "epoch: 38000, train_loss: 105.098541, test_loss: 263.375763\n",
      "epoch: 38100, train_loss: 103.008831, test_loss: 262.278717\n",
      "epoch: 38200, train_loss: 107.042355, test_loss: 262.064911\n",
      "epoch: 38300, train_loss: 107.420704, test_loss: 261.294556 (Saved)\n",
      "epoch: 38400, train_loss: 105.944572, test_loss: 261.917908\n",
      "epoch: 38500, train_loss: 104.887135, test_loss: 262.518158\n",
      "epoch: 38600, train_loss: 105.949253, test_loss: 261.276489 (Saved)\n",
      "epoch: 38700, train_loss: 103.120872, test_loss: 262.469482\n",
      "epoch: 38800, train_loss: 102.400288, test_loss: 262.447296\n",
      "epoch: 38900, train_loss: 103.913826, test_loss: 262.087738\n",
      "epoch: 39000, train_loss: 104.852745, test_loss: 261.704163\n",
      "epoch: 39100, train_loss: 104.076965, test_loss: 262.085907\n",
      "epoch: 39200, train_loss: 104.574444, test_loss: 262.130646\n",
      "epoch: 39300, train_loss: 105.897388, test_loss: 260.982239 (Saved)\n",
      "epoch: 39400, train_loss: 103.398224, test_loss: 262.242218\n",
      "epoch: 39500, train_loss: 104.826233, test_loss: 260.822571 (Saved)\n",
      "epoch: 39600, train_loss: 104.590485, test_loss: 261.350830\n",
      "epoch: 39700, train_loss: 105.731525, test_loss: 261.117340\n",
      "epoch: 39800, train_loss: 108.230549, test_loss: 260.631409 (Saved)\n",
      "epoch: 39900, train_loss: 106.404995, test_loss: 261.348846\n",
      "epoch: 40000, train_loss: 104.479294, test_loss: 261.234619\n",
      "epoch: 40100, train_loss: 106.452152, test_loss: 261.869781\n",
      "epoch: 40200, train_loss: 103.273781, test_loss: 261.452698\n",
      "epoch: 40300, train_loss: 106.031357, test_loss: 260.504639 (Saved)\n",
      "epoch: 40400, train_loss: 104.173473, test_loss: 261.061096\n",
      "epoch: 40500, train_loss: 105.282982, test_loss: 260.734558\n",
      "epoch: 40600, train_loss: 105.031971, test_loss: 260.388275 (Saved)\n",
      "epoch: 40700, train_loss: 103.469456, test_loss: 260.672821\n",
      "epoch: 40800, train_loss: 103.207954, test_loss: 260.274902 (Saved)\n",
      "epoch: 40900, train_loss: 104.734863, test_loss: 260.568512\n",
      "epoch: 41000, train_loss: 103.508213, test_loss: 259.795288 (Saved)\n",
      "epoch: 41100, train_loss: 102.822388, test_loss: 260.112366\n",
      "epoch: 41200, train_loss: 105.327396, test_loss: 260.731964\n",
      "epoch: 41300, train_loss: 105.579544, test_loss: 259.658112 (Saved)\n",
      "epoch: 41400, train_loss: 101.943623, test_loss: 261.302948\n",
      "epoch: 41500, train_loss: 104.401634, test_loss: 259.400055 (Saved)\n",
      "epoch: 41600, train_loss: 106.176815, test_loss: 259.900421\n",
      "epoch: 41700, train_loss: 105.457024, test_loss: 259.750488\n",
      "epoch: 41800, train_loss: 104.908157, test_loss: 260.091766\n",
      "epoch: 41900, train_loss: 103.891804, test_loss: 260.964020\n",
      "epoch: 42000, train_loss: 106.359211, test_loss: 260.774475\n",
      "epoch: 42100, train_loss: 102.689991, test_loss: 259.761871\n",
      "epoch: 42200, train_loss: 104.244942, test_loss: 259.516907\n",
      "epoch: 42300, train_loss: 103.301304, test_loss: 260.158203\n",
      "epoch: 42400, train_loss: 106.651859, test_loss: 259.749329\n",
      "epoch: 42500, train_loss: 107.722858, test_loss: 260.327240\n",
      "epoch: 42600, train_loss: 100.482077, test_loss: 259.756744\n",
      "epoch: 42700, train_loss: 103.111629, test_loss: 259.497650\n",
      "epoch: 42800, train_loss: 105.222794, test_loss: 259.886322\n",
      "epoch: 42900, train_loss: 104.480370, test_loss: 259.535522\n",
      "epoch: 43000, train_loss: 106.355331, test_loss: 259.676575\n",
      "epoch: 43100, train_loss: 103.628162, test_loss: 259.384491 (Saved)\n",
      "epoch: 43200, train_loss: 106.236740, test_loss: 259.001404 (Saved)\n",
      "epoch: 43300, train_loss: 103.567909, test_loss: 258.455139 (Saved)\n",
      "epoch: 43400, train_loss: 103.270935, test_loss: 259.721832\n",
      "epoch: 43500, train_loss: 106.237503, test_loss: 258.729156\n",
      "epoch: 43600, train_loss: 102.712688, test_loss: 258.524017\n",
      "epoch: 43700, train_loss: 102.436104, test_loss: 259.080444\n",
      "epoch: 43800, train_loss: 101.854652, test_loss: 260.368927\n",
      "epoch: 43900, train_loss: 104.004314, test_loss: 259.044525\n",
      "epoch: 44000, train_loss: 105.483570, test_loss: 258.531036\n",
      "epoch: 44100, train_loss: 103.381691, test_loss: 259.220459\n",
      "epoch: 44200, train_loss: 106.289597, test_loss: 258.377289 (Saved)\n",
      "epoch: 44300, train_loss: 103.898228, test_loss: 260.826294\n",
      "epoch: 44400, train_loss: 102.269001, test_loss: 259.068298\n",
      "epoch: 44500, train_loss: 104.175968, test_loss: 257.961517 (Saved)\n",
      "epoch: 44600, train_loss: 102.579170, test_loss: 258.197723\n",
      "epoch: 44700, train_loss: 105.490864, test_loss: 258.606964\n",
      "epoch: 44800, train_loss: 105.961250, test_loss: 258.296570\n",
      "epoch: 44900, train_loss: 102.600887, test_loss: 257.895752 (Saved)\n",
      "epoch: 45000, train_loss: 101.699791, test_loss: 258.861298\n",
      "epoch: 45100, train_loss: 101.792339, test_loss: 258.210022\n",
      "epoch: 45200, train_loss: 102.824223, test_loss: 258.537079\n",
      "epoch: 45300, train_loss: 104.224873, test_loss: 258.512848\n",
      "epoch: 45400, train_loss: 101.320065, test_loss: 258.853821\n",
      "epoch: 45500, train_loss: 105.438705, test_loss: 258.245056\n",
      "epoch: 45600, train_loss: 104.902561, test_loss: 258.468079\n",
      "epoch: 45700, train_loss: 105.089748, test_loss: 257.484863 (Saved)\n",
      "epoch: 45800, train_loss: 101.731010, test_loss: 259.008362\n",
      "epoch: 45900, train_loss: 103.251774, test_loss: 258.539093\n",
      "epoch: 46000, train_loss: 102.066074, test_loss: 258.318054\n",
      "epoch: 46100, train_loss: 105.212803, test_loss: 257.405487 (Saved)\n",
      "epoch: 46200, train_loss: 104.800907, test_loss: 257.602051\n",
      "epoch: 46300, train_loss: 102.684193, test_loss: 258.419403\n",
      "epoch: 46400, train_loss: 104.674572, test_loss: 257.953339\n",
      "epoch: 46500, train_loss: 103.549129, test_loss: 257.288544 (Saved)\n",
      "epoch: 46600, train_loss: 101.165936, test_loss: 257.718353\n",
      "epoch: 46700, train_loss: 103.712070, test_loss: 258.398315\n",
      "epoch: 46800, train_loss: 102.442348, test_loss: 258.055359\n",
      "epoch: 46900, train_loss: 102.386589, test_loss: 257.160400 (Saved)\n",
      "epoch: 47000, train_loss: 99.488173, test_loss: 257.337280\n",
      "epoch: 47100, train_loss: 103.562035, test_loss: 257.033722 (Saved)\n",
      "epoch: 47200, train_loss: 102.123199, test_loss: 257.456268\n",
      "epoch: 47300, train_loss: 103.053558, test_loss: 256.910126 (Saved)\n",
      "epoch: 47400, train_loss: 101.316540, test_loss: 255.416718 (Saved)\n",
      "epoch: 47500, train_loss: 103.330387, test_loss: 256.394592\n",
      "epoch: 47600, train_loss: 101.821918, test_loss: 257.300171\n",
      "epoch: 47700, train_loss: 103.845188, test_loss: 256.565643\n",
      "epoch: 47800, train_loss: 104.557037, test_loss: 255.542679\n",
      "epoch: 47900, train_loss: 102.478745, test_loss: 255.029678 (Saved)\n",
      "epoch: 48000, train_loss: 100.847553, test_loss: 255.233063\n",
      "epoch: 48100, train_loss: 103.685596, test_loss: 255.304214\n",
      "epoch: 48200, train_loss: 100.565365, test_loss: 255.334457\n",
      "epoch: 48300, train_loss: 101.428555, test_loss: 254.151505 (Saved)\n",
      "epoch: 48400, train_loss: 104.845398, test_loss: 254.583481\n",
      "epoch: 48500, train_loss: 103.236809, test_loss: 256.626831\n",
      "epoch: 48600, train_loss: 102.483753, test_loss: 254.743118\n",
      "epoch: 48700, train_loss: 102.634838, test_loss: 254.796112\n",
      "epoch: 48800, train_loss: 99.775536, test_loss: 253.743301 (Saved)\n",
      "epoch: 48900, train_loss: 99.425232, test_loss: 255.156250\n",
      "epoch: 49000, train_loss: 102.428490, test_loss: 255.177887\n",
      "epoch: 49100, train_loss: 102.674774, test_loss: 253.406921 (Saved)\n",
      "epoch: 49200, train_loss: 101.825962, test_loss: 253.793655\n",
      "epoch: 49300, train_loss: 100.805801, test_loss: 253.982452\n",
      "epoch: 49400, train_loss: 101.011044, test_loss: 254.222977\n",
      "epoch: 49500, train_loss: 101.648933, test_loss: 254.090652\n",
      "epoch: 49600, train_loss: 99.816559, test_loss: 254.614594\n",
      "epoch: 49700, train_loss: 103.435177, test_loss: 254.018509\n",
      "epoch: 49800, train_loss: 100.870495, test_loss: 254.628525\n",
      "epoch: 49900, train_loss: 101.163158, test_loss: 252.719925 (Saved)\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/icar/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Saving ONNX model: /home/icar/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Failed to save ONNX model: /home/icar/icar-ng-data/model/pixel2cm_model.onnx\n",
      "Module onnx is not installed!\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'pixel2cm_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 158.,  398.],\n",
      "        [ 298.,  284.],\n",
      "        [ 642.,  290.],\n",
      "        [ 640.,  598.],\n",
      "        [1184.,  432.],\n",
      "        [ 488.,  446.],\n",
      "        [1032.,  208.],\n",
      "        [ 642.,  326.],\n",
      "        [ 150.,  300.],\n",
      "        [ 838.,  528.],\n",
      "        [  90.,  690.],\n",
      "        [ 730.,  324.],\n",
      "        [ 122.,  452.],\n",
      "        [ 354.,  236.],\n",
      "        [ 552.,  264.],\n",
      "        [ 138.,  488.],\n",
      "        [ 922.,  688.],\n",
      "        [ 934.,  260.],\n",
      "        [1024.,  432.],\n",
      "        [ 642.,  390.],\n",
      "        [ 524.,  268.],\n",
      "        [ 782.,  286.],\n",
      "        [  64.,  230.],\n",
      "        [1014.,  232.],\n",
      "        [ 362.,  296.],\n",
      "        [ 538.,  356.],\n",
      "        [ 542.,  228.],\n",
      "        [ 562.,  446.],\n",
      "        [ 832.,  226.]], device='cuda:0')\n",
      "y\n",
      "tensor([[ 150.0000,  200.0000],\n",
      "        [ 416.0251,  277.3501],\n",
      "        [ 350.0000,    0.0000],\n",
      "        [  25.0000,    0.0000],\n",
      "        [ 100.0000, -175.0000],\n",
      "        [ 100.0000,   50.0000],\n",
      "        [ 998.4603, -665.6403],\n",
      "        [ 250.0000,    0.0000],\n",
      "        [ 353.5534,  353.5534],\n",
      "        [  50.0000,  -50.0000],\n",
      "        [   0.0000,  100.0000],\n",
      "        [ 250.0000,  -50.0000],\n",
      "        [ 100.0000,  175.0000],\n",
      "        [ 715.5417,  357.7709],\n",
      "        [ 493.1970,   82.1995],\n",
      "        [  75.0000,  150.0000],\n",
      "        [   0.0000,  -50.0000],\n",
      "        [ 450.0000, -250.0000],\n",
      "        [ 100.0000, -125.0000],\n",
      "        [ 150.0000,    0.0000],\n",
      "        [ 450.0000,  100.0000],\n",
      "        [ 350.0000, -100.0000],\n",
      "        [ 848.5281,  848.5281],\n",
      "        [ 665.6403, -443.7602],\n",
      "        [ 350.0000,  200.0000],\n",
      "        [ 200.0000,   50.0000],\n",
      "        [ 789.1151,  131.5192],\n",
      "        [ 100.0000,   25.0000],\n",
      "        [ 758.9467, -252.9822]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[ 1.4977e+02,  2.0335e+02],\n",
      "        [ 3.9855e+02,  2.7776e+02],\n",
      "        [ 3.4166e+02, -4.3748e+00],\n",
      "        [ 2.3275e+01, -4.0010e+00],\n",
      "        [ 9.7317e+01, -1.7250e+02],\n",
      "        [ 1.0148e+02,  4.8269e+01],\n",
      "        [ 9.9519e+02, -6.5114e+02],\n",
      "        [ 2.6115e+02,  5.7513e-01],\n",
      "        [ 3.6275e+02,  3.6443e+02],\n",
      "        [ 4.3769e+01, -5.5180e+01],\n",
      "        [ 7.9962e+00,  8.8731e+01],\n",
      "        [ 2.6242e+02, -4.7588e+01],\n",
      "        [ 9.4742e+01,  1.7511e+02],\n",
      "        [ 7.3396e+02,  3.6941e+02],\n",
      "        [ 4.7501e+02,  7.6858e+01],\n",
      "        [ 6.9202e+01,  1.4862e+02],\n",
      "        [ 9.5045e+00, -5.1376e+01],\n",
      "        [ 4.5268e+02, -2.5897e+02],\n",
      "        [ 9.8843e+01, -1.2728e+02],\n",
      "        [ 1.5854e+02, -1.9593e+00],\n",
      "        [ 4.5297e+02,  1.0095e+02],\n",
      "        [ 3.4630e+02, -9.5929e+01],\n",
      "        [ 9.5034e+02,  8.5786e+02],\n",
      "        [ 6.7878e+02, -4.6388e+02],\n",
      "        [ 3.4587e+02,  1.9774e+02],\n",
      "        [ 2.0694e+02,  4.8766e+01],\n",
      "        [ 7.6677e+02,  1.4770e+02],\n",
      "        [ 1.0102e+02,  2.3249e+01],\n",
      "        [ 7.4695e+02, -2.6282e+02]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[   0.2252,   -3.3467],\n",
      "        [  17.4727,   -0.4086],\n",
      "        [   8.3402,    4.3748],\n",
      "        [   1.7248,    4.0010],\n",
      "        [   2.6828,   -2.4995],\n",
      "        [  -1.4844,    1.7308],\n",
      "        [   3.2753,  -14.5002],\n",
      "        [ -11.1503,   -0.5751],\n",
      "        [  -9.1984,  -10.8768],\n",
      "        [   6.2306,    5.1802],\n",
      "        [  -7.9962,   11.2689],\n",
      "        [ -12.4250,   -2.4122],\n",
      "        [   5.2584,   -0.1097],\n",
      "        [ -18.4230,  -11.6407],\n",
      "        [  18.1891,    5.3418],\n",
      "        [   5.7981,    1.3751],\n",
      "        [  -9.5045,    1.3762],\n",
      "        [  -2.6847,    8.9690],\n",
      "        [   1.1571,    2.2811],\n",
      "        [  -8.5413,    1.9593],\n",
      "        [  -2.9712,   -0.9453],\n",
      "        [   3.6978,   -4.0709],\n",
      "        [-101.8149,   -9.3308],\n",
      "        [ -13.1358,   20.1235],\n",
      "        [   4.1255,    2.2647],\n",
      "        [  -6.9445,    1.2336],\n",
      "        [  22.3448,  -16.1814],\n",
      "        [  -1.0188,    1.7508],\n",
      "        [  11.9926,    9.8365]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
