{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 26/09/2023 at 12:12:00.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/protanjung/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/protanjung/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/protanjung/icar-ng-data/model\n",
      "Model file: cm2pixel_model.pt\n",
      "Model path: /home/protanjung/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'cm2pixel_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([   0., -300.]), max_x: tensor([450., 300.])\n",
      "min_y: tensor([ 30., 260.]), max_y: tensor([1272.,  692.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 2:4].values\n",
    "        y = self.dataframe.iloc[:, 0:2].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 91\n",
      "dataset_test: 23\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.fc5 = nn.Linear(16, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/protanjung/icar-ng-data/model/cm2pixel_model.pt\n",
      "Loaded model: /home/protanjung/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 603806.593750, test_loss: 345026.250000 (Saved)\n",
      "epoch: 100, train_loss: 552160.218750, test_loss: 306016.406250 (Saved)\n",
      "epoch: 200, train_loss: 408851.546875, test_loss: 267971.531250 (Saved)\n",
      "epoch: 300, train_loss: 401239.937500, test_loss: 232533.046875 (Saved)\n",
      "epoch: 400, train_loss: 334654.390625, test_loss: 204883.062500 (Saved)\n",
      "epoch: 500, train_loss: 295870.296875, test_loss: 180752.656250 (Saved)\n",
      "epoch: 600, train_loss: 285114.265625, test_loss: 161032.437500 (Saved)\n",
      "epoch: 700, train_loss: 243091.406250, test_loss: 144518.203125 (Saved)\n",
      "epoch: 800, train_loss: 212041.062500, test_loss: 130545.046875 (Saved)\n",
      "epoch: 900, train_loss: 198844.078125, test_loss: 117760.218750 (Saved)\n",
      "epoch: 1000, train_loss: 184585.851562, test_loss: 106671.937500 (Saved)\n",
      "epoch: 1100, train_loss: 166292.796875, test_loss: 97129.960938 (Saved)\n",
      "epoch: 1200, train_loss: 140291.453125, test_loss: 88549.304688 (Saved)\n",
      "epoch: 1300, train_loss: 129812.582031, test_loss: 81009.156250 (Saved)\n",
      "epoch: 1400, train_loss: 114018.199219, test_loss: 74294.468750 (Saved)\n",
      "epoch: 1500, train_loss: 108860.980469, test_loss: 68106.296875 (Saved)\n",
      "epoch: 1600, train_loss: 96362.265625, test_loss: 62651.066406 (Saved)\n",
      "epoch: 1700, train_loss: 95459.300781, test_loss: 57511.996094 (Saved)\n",
      "epoch: 1800, train_loss: 76004.542969, test_loss: 52987.914062 (Saved)\n",
      "epoch: 1900, train_loss: 73529.835938, test_loss: 48902.121094 (Saved)\n",
      "epoch: 2000, train_loss: 78269.351562, test_loss: 45023.191406 (Saved)\n",
      "epoch: 2100, train_loss: 61851.572266, test_loss: 41518.230469 (Saved)\n",
      "epoch: 2200, train_loss: 57440.687500, test_loss: 38370.750000 (Saved)\n",
      "epoch: 2300, train_loss: 48784.671875, test_loss: 35417.085938 (Saved)\n",
      "epoch: 2400, train_loss: 47074.525391, test_loss: 32776.085938 (Saved)\n",
      "epoch: 2500, train_loss: 43289.914062, test_loss: 30299.998047 (Saved)\n",
      "epoch: 2600, train_loss: 40063.351562, test_loss: 28096.212891 (Saved)\n",
      "epoch: 2700, train_loss: 37469.941406, test_loss: 25970.312500 (Saved)\n",
      "epoch: 2800, train_loss: 37969.083984, test_loss: 24090.326172 (Saved)\n",
      "epoch: 2900, train_loss: 32959.563477, test_loss: 22387.857422 (Saved)\n",
      "epoch: 3000, train_loss: 30173.097656, test_loss: 20887.369141 (Saved)\n",
      "epoch: 3100, train_loss: 30130.101562, test_loss: 19402.150391 (Saved)\n",
      "epoch: 3200, train_loss: 23458.955078, test_loss: 18131.816406 (Saved)\n",
      "epoch: 3300, train_loss: 27098.679688, test_loss: 17057.673828 (Saved)\n",
      "epoch: 3400, train_loss: 24924.312500, test_loss: 16012.955078 (Saved)\n",
      "epoch: 3500, train_loss: 22278.616211, test_loss: 15096.241211 (Saved)\n",
      "epoch: 3600, train_loss: 21249.458008, test_loss: 14301.168945 (Saved)\n",
      "epoch: 3700, train_loss: 20156.873047, test_loss: 13648.568359 (Saved)\n",
      "epoch: 3800, train_loss: 17658.817383, test_loss: 13004.318359 (Saved)\n",
      "epoch: 3900, train_loss: 16920.802246, test_loss: 12409.423828 (Saved)\n",
      "epoch: 4000, train_loss: 16790.041016, test_loss: 11887.603516 (Saved)\n",
      "epoch: 4100, train_loss: 17273.661133, test_loss: 11463.652344 (Saved)\n",
      "epoch: 4200, train_loss: 16250.201660, test_loss: 11032.510742 (Saved)\n",
      "epoch: 4300, train_loss: 15782.308594, test_loss: 10570.120117 (Saved)\n",
      "epoch: 4400, train_loss: 16322.462402, test_loss: 10237.494141 (Saved)\n",
      "epoch: 4500, train_loss: 15832.182617, test_loss: 9953.742188 (Saved)\n",
      "epoch: 4600, train_loss: 16009.663086, test_loss: 9681.830078 (Saved)\n",
      "epoch: 4700, train_loss: 16533.342773, test_loss: 9334.628906 (Saved)\n",
      "epoch: 4800, train_loss: 13128.819336, test_loss: 9046.252930 (Saved)\n",
      "epoch: 4900, train_loss: 13597.163574, test_loss: 8753.518555 (Saved)\n",
      "epoch: 5000, train_loss: 11040.263428, test_loss: 8564.479492 (Saved)\n",
      "epoch: 5100, train_loss: 15199.915527, test_loss: 8303.784180 (Saved)\n",
      "epoch: 5200, train_loss: 14763.993164, test_loss: 8079.705078 (Saved)\n",
      "epoch: 5300, train_loss: 11692.482422, test_loss: 7869.911133 (Saved)\n",
      "epoch: 5400, train_loss: 10425.483154, test_loss: 7682.339355 (Saved)\n",
      "epoch: 5500, train_loss: 11519.182617, test_loss: 7464.495605 (Saved)\n",
      "epoch: 5600, train_loss: 12039.822266, test_loss: 7346.876953 (Saved)\n",
      "epoch: 5700, train_loss: 12902.583496, test_loss: 7129.459473 (Saved)\n",
      "epoch: 5800, train_loss: 10558.678223, test_loss: 6902.266602 (Saved)\n",
      "epoch: 5900, train_loss: 9911.423340, test_loss: 6692.088379 (Saved)\n",
      "epoch: 6000, train_loss: 10924.561523, test_loss: 6584.668457 (Saved)\n",
      "epoch: 6100, train_loss: 10760.690918, test_loss: 6426.633301 (Saved)\n",
      "epoch: 6200, train_loss: 9744.541992, test_loss: 6225.039062 (Saved)\n",
      "epoch: 6300, train_loss: 10329.652344, test_loss: 6079.983887 (Saved)\n",
      "epoch: 6400, train_loss: 9253.620117, test_loss: 5888.664551 (Saved)\n",
      "epoch: 6500, train_loss: 8666.970947, test_loss: 5761.632812 (Saved)\n",
      "epoch: 6600, train_loss: 8946.680664, test_loss: 5609.634766 (Saved)\n",
      "epoch: 6700, train_loss: 9056.657227, test_loss: 5458.796875 (Saved)\n",
      "epoch: 6800, train_loss: 7653.144287, test_loss: 5295.270508 (Saved)\n",
      "epoch: 6900, train_loss: 9639.424805, test_loss: 5140.191895 (Saved)\n",
      "epoch: 7000, train_loss: 9025.610840, test_loss: 4991.320801 (Saved)\n",
      "epoch: 7100, train_loss: 7767.300537, test_loss: 4866.540039 (Saved)\n",
      "epoch: 7200, train_loss: 7382.275391, test_loss: 4712.157715 (Saved)\n",
      "epoch: 7300, train_loss: 7646.276123, test_loss: 4610.098145 (Saved)\n",
      "epoch: 7400, train_loss: 7182.155762, test_loss: 4487.621582 (Saved)\n",
      "epoch: 7500, train_loss: 7440.233398, test_loss: 4400.899414 (Saved)\n",
      "epoch: 7600, train_loss: 6276.916504, test_loss: 4264.520020 (Saved)\n",
      "epoch: 7700, train_loss: 6316.622314, test_loss: 4114.310547 (Saved)\n",
      "epoch: 7800, train_loss: 6486.594971, test_loss: 3977.618164 (Saved)\n",
      "epoch: 7900, train_loss: 6231.258301, test_loss: 3882.504150 (Saved)\n",
      "epoch: 8000, train_loss: 6283.595703, test_loss: 3782.882568 (Saved)\n",
      "epoch: 8100, train_loss: 7264.075439, test_loss: 3688.371338 (Saved)\n",
      "epoch: 8200, train_loss: 6050.413574, test_loss: 3545.496094 (Saved)\n",
      "epoch: 8300, train_loss: 6545.723145, test_loss: 3459.074707 (Saved)\n",
      "epoch: 8400, train_loss: 5524.748535, test_loss: 3396.470215 (Saved)\n",
      "epoch: 8500, train_loss: 5453.523926, test_loss: 3284.277344 (Saved)\n",
      "epoch: 8600, train_loss: 5876.125977, test_loss: 3208.656982 (Saved)\n",
      "epoch: 8700, train_loss: 4796.896484, test_loss: 3115.109375 (Saved)\n",
      "epoch: 8800, train_loss: 5976.227539, test_loss: 3048.301025 (Saved)\n",
      "epoch: 8900, train_loss: 5270.438965, test_loss: 2924.292480 (Saved)\n",
      "epoch: 9000, train_loss: 4735.703247, test_loss: 2859.738525 (Saved)\n",
      "epoch: 9100, train_loss: 4678.043335, test_loss: 2789.604248 (Saved)\n",
      "epoch: 9200, train_loss: 5132.380127, test_loss: 2719.274414 (Saved)\n",
      "epoch: 9300, train_loss: 5307.286865, test_loss: 2651.183105 (Saved)\n",
      "epoch: 9400, train_loss: 4413.351807, test_loss: 2587.050293 (Saved)\n",
      "epoch: 9500, train_loss: 4285.491821, test_loss: 2513.773682 (Saved)\n",
      "epoch: 9600, train_loss: 4276.111328, test_loss: 2487.002441 (Saved)\n",
      "epoch: 9700, train_loss: 4172.042480, test_loss: 2412.391357 (Saved)\n",
      "epoch: 9800, train_loss: 4105.639038, test_loss: 2360.695801 (Saved)\n",
      "epoch: 9900, train_loss: 4189.104492, test_loss: 2319.629395 (Saved)\n",
      "epoch: 10000, train_loss: 3882.373169, test_loss: 2267.406006 (Saved)\n",
      "epoch: 10100, train_loss: 3749.621582, test_loss: 2225.007568 (Saved)\n",
      "epoch: 10200, train_loss: 4287.383789, test_loss: 2166.348389 (Saved)\n",
      "epoch: 10300, train_loss: 4101.661499, test_loss: 2130.220215 (Saved)\n",
      "epoch: 10400, train_loss: 4367.030273, test_loss: 2078.715820 (Saved)\n",
      "epoch: 10500, train_loss: 4037.356201, test_loss: 2034.857544 (Saved)\n",
      "epoch: 10600, train_loss: 4363.377197, test_loss: 1987.206787 (Saved)\n",
      "epoch: 10700, train_loss: 3454.193848, test_loss: 1955.196045 (Saved)\n",
      "epoch: 10800, train_loss: 3445.883179, test_loss: 1925.587158 (Saved)\n",
      "epoch: 10900, train_loss: 3096.923889, test_loss: 1886.955933 (Saved)\n",
      "epoch: 11000, train_loss: 4095.554443, test_loss: 1856.285400 (Saved)\n",
      "epoch: 11100, train_loss: 4064.375977, test_loss: 1821.775513 (Saved)\n",
      "epoch: 11200, train_loss: 3284.182373, test_loss: 1786.027222 (Saved)\n",
      "epoch: 11300, train_loss: 3935.609131, test_loss: 1758.682251 (Saved)\n",
      "epoch: 11400, train_loss: 3170.884766, test_loss: 1726.190430 (Saved)\n",
      "epoch: 11500, train_loss: 3222.775757, test_loss: 1696.148438 (Saved)\n",
      "epoch: 11600, train_loss: 3480.260986, test_loss: 1668.732422 (Saved)\n",
      "epoch: 11700, train_loss: 3167.501343, test_loss: 1651.526855 (Saved)\n",
      "epoch: 11800, train_loss: 3207.066895, test_loss: 1617.419556 (Saved)\n",
      "epoch: 11900, train_loss: 3234.060669, test_loss: 1591.210083 (Saved)\n",
      "epoch: 12000, train_loss: 3154.843750, test_loss: 1571.106689 (Saved)\n",
      "epoch: 12100, train_loss: 3041.859497, test_loss: 1544.541504 (Saved)\n",
      "epoch: 12200, train_loss: 2524.495056, test_loss: 1516.663086 (Saved)\n",
      "epoch: 12300, train_loss: 3059.759644, test_loss: 1492.906738 (Saved)\n",
      "epoch: 12400, train_loss: 2762.766479, test_loss: 1462.366821 (Saved)\n",
      "epoch: 12500, train_loss: 2538.727844, test_loss: 1439.733398 (Saved)\n",
      "epoch: 12600, train_loss: 2939.441406, test_loss: 1418.725952 (Saved)\n",
      "epoch: 12700, train_loss: 2378.967041, test_loss: 1396.039307 (Saved)\n",
      "epoch: 12800, train_loss: 2628.776978, test_loss: 1377.643066 (Saved)\n",
      "epoch: 12900, train_loss: 2537.150635, test_loss: 1362.019165 (Saved)\n",
      "epoch: 13000, train_loss: 2811.743286, test_loss: 1330.051270 (Saved)\n",
      "epoch: 13100, train_loss: 2234.537720, test_loss: 1313.323975 (Saved)\n",
      "epoch: 13200, train_loss: 2891.414062, test_loss: 1290.250366 (Saved)\n",
      "epoch: 13300, train_loss: 2285.355347, test_loss: 1273.956299 (Saved)\n",
      "epoch: 13400, train_loss: 2538.568726, test_loss: 1252.120239 (Saved)\n",
      "epoch: 13500, train_loss: 2627.680481, test_loss: 1232.215820 (Saved)\n",
      "epoch: 13600, train_loss: 2573.131836, test_loss: 1210.816895 (Saved)\n",
      "epoch: 13700, train_loss: 2702.377930, test_loss: 1208.612305 (Saved)\n",
      "epoch: 13800, train_loss: 2688.152527, test_loss: 1176.298218 (Saved)\n",
      "epoch: 13900, train_loss: 2300.997437, test_loss: 1150.716431 (Saved)\n",
      "epoch: 14000, train_loss: 2176.059570, test_loss: 1137.133911 (Saved)\n",
      "epoch: 14100, train_loss: 2179.715942, test_loss: 1123.157349 (Saved)\n",
      "epoch: 14200, train_loss: 2038.203674, test_loss: 1101.479126 (Saved)\n",
      "epoch: 14300, train_loss: 1947.671631, test_loss: 1084.207275 (Saved)\n",
      "epoch: 14400, train_loss: 2371.826782, test_loss: 1062.491821 (Saved)\n",
      "epoch: 14500, train_loss: 1784.390442, test_loss: 1044.281982 (Saved)\n",
      "epoch: 14600, train_loss: 1957.653381, test_loss: 1029.080933 (Saved)\n",
      "epoch: 14700, train_loss: 2268.135986, test_loss: 1015.197388 (Saved)\n",
      "epoch: 14800, train_loss: 1825.984497, test_loss: 988.057922 (Saved)\n",
      "epoch: 14900, train_loss: 1662.672546, test_loss: 972.091248 (Saved)\n",
      "epoch: 15000, train_loss: 1778.898254, test_loss: 956.852356 (Saved)\n",
      "epoch: 15100, train_loss: 1706.698547, test_loss: 932.067200 (Saved)\n",
      "epoch: 15200, train_loss: 1535.536377, test_loss: 915.619324 (Saved)\n",
      "epoch: 15300, train_loss: 2102.384033, test_loss: 905.869690 (Saved)\n",
      "epoch: 15400, train_loss: 1636.315552, test_loss: 890.530579 (Saved)\n",
      "epoch: 15500, train_loss: 1482.451904, test_loss: 867.629761 (Saved)\n",
      "epoch: 15600, train_loss: 1599.101074, test_loss: 858.804138 (Saved)\n",
      "epoch: 15700, train_loss: 1469.833862, test_loss: 844.256958 (Saved)\n",
      "epoch: 15800, train_loss: 1648.164185, test_loss: 823.506531 (Saved)\n",
      "epoch: 15900, train_loss: 1313.818970, test_loss: 805.797485 (Saved)\n",
      "epoch: 16000, train_loss: 1514.175964, test_loss: 792.995667 (Saved)\n",
      "epoch: 16100, train_loss: 1592.825684, test_loss: 772.643860 (Saved)\n",
      "epoch: 16200, train_loss: 1513.247559, test_loss: 762.187256 (Saved)\n",
      "epoch: 16300, train_loss: 1206.190674, test_loss: 746.326843 (Saved)\n",
      "epoch: 16400, train_loss: 1302.418152, test_loss: 728.493835 (Saved)\n",
      "epoch: 16500, train_loss: 1261.012024, test_loss: 714.564270 (Saved)\n",
      "epoch: 16600, train_loss: 1082.890839, test_loss: 701.505188 (Saved)\n",
      "epoch: 16700, train_loss: 1490.731506, test_loss: 691.840393 (Saved)\n",
      "epoch: 16800, train_loss: 1167.457306, test_loss: 672.358276 (Saved)\n",
      "epoch: 16900, train_loss: 1015.650055, test_loss: 659.959167 (Saved)\n",
      "epoch: 17000, train_loss: 1139.267334, test_loss: 643.558960 (Saved)\n",
      "epoch: 17100, train_loss: 1268.422699, test_loss: 630.453552 (Saved)\n",
      "epoch: 17200, train_loss: 1084.213257, test_loss: 618.963074 (Saved)\n",
      "epoch: 17300, train_loss: 935.838989, test_loss: 606.434937 (Saved)\n",
      "epoch: 17400, train_loss: 1204.827271, test_loss: 592.976807 (Saved)\n",
      "epoch: 17500, train_loss: 931.877411, test_loss: 576.363525 (Saved)\n",
      "epoch: 17600, train_loss: 896.561859, test_loss: 566.694275 (Saved)\n",
      "epoch: 17700, train_loss: 920.036285, test_loss: 556.214111 (Saved)\n",
      "epoch: 17800, train_loss: 959.393433, test_loss: 542.397278 (Saved)\n",
      "epoch: 17900, train_loss: 848.694214, test_loss: 532.633606 (Saved)\n",
      "epoch: 18000, train_loss: 1105.434052, test_loss: 524.354370 (Saved)\n",
      "epoch: 18100, train_loss: 972.922699, test_loss: 504.213654 (Saved)\n",
      "epoch: 18200, train_loss: 840.252533, test_loss: 496.995422 (Saved)\n",
      "epoch: 18300, train_loss: 919.989136, test_loss: 485.692932 (Saved)\n",
      "epoch: 18400, train_loss: 876.863739, test_loss: 475.883453 (Saved)\n",
      "epoch: 18500, train_loss: 706.252258, test_loss: 459.432526 (Saved)\n",
      "epoch: 18600, train_loss: 758.178497, test_loss: 447.489227 (Saved)\n",
      "epoch: 18700, train_loss: 956.234039, test_loss: 439.291748 (Saved)\n",
      "epoch: 18800, train_loss: 654.047867, test_loss: 429.194092 (Saved)\n",
      "epoch: 18900, train_loss: 665.079987, test_loss: 424.240753 (Saved)\n",
      "epoch: 19000, train_loss: 692.791901, test_loss: 408.820404 (Saved)\n",
      "epoch: 19100, train_loss: 657.888184, test_loss: 401.046631 (Saved)\n",
      "epoch: 19200, train_loss: 651.142120, test_loss: 388.834808 (Saved)\n",
      "epoch: 19300, train_loss: 732.860199, test_loss: 380.226196 (Saved)\n",
      "epoch: 19400, train_loss: 585.955063, test_loss: 372.313599 (Saved)\n",
      "epoch: 19500, train_loss: 764.826324, test_loss: 362.855743 (Saved)\n",
      "epoch: 19600, train_loss: 549.648483, test_loss: 353.123596 (Saved)\n",
      "epoch: 19700, train_loss: 590.143036, test_loss: 343.666504 (Saved)\n",
      "epoch: 19800, train_loss: 559.074463, test_loss: 337.532440 (Saved)\n",
      "epoch: 19900, train_loss: 653.878067, test_loss: 330.590790 (Saved)\n",
      "epoch: 20000, train_loss: 561.644958, test_loss: 323.482483 (Saved)\n",
      "epoch: 20100, train_loss: 461.264267, test_loss: 315.158600 (Saved)\n",
      "epoch: 20200, train_loss: 552.146240, test_loss: 308.400177 (Saved)\n",
      "epoch: 20300, train_loss: 574.895584, test_loss: 301.557892 (Saved)\n",
      "epoch: 20400, train_loss: 552.253571, test_loss: 295.181061 (Saved)\n",
      "epoch: 20500, train_loss: 435.030334, test_loss: 289.175781 (Saved)\n",
      "epoch: 20600, train_loss: 515.381287, test_loss: 286.196930 (Saved)\n",
      "epoch: 20700, train_loss: 493.217926, test_loss: 276.481201 (Saved)\n",
      "epoch: 20800, train_loss: 423.083282, test_loss: 270.665222 (Saved)\n",
      "epoch: 20900, train_loss: 380.494759, test_loss: 265.424255 (Saved)\n",
      "epoch: 21000, train_loss: 476.576508, test_loss: 260.286957 (Saved)\n",
      "epoch: 21100, train_loss: 466.655838, test_loss: 255.364517 (Saved)\n",
      "epoch: 21200, train_loss: 390.066360, test_loss: 248.768906 (Saved)\n",
      "epoch: 21300, train_loss: 366.140289, test_loss: 244.453400 (Saved)\n",
      "epoch: 21400, train_loss: 443.542160, test_loss: 238.902130 (Saved)\n",
      "epoch: 21500, train_loss: 418.922333, test_loss: 232.963913 (Saved)\n",
      "epoch: 21600, train_loss: 317.197533, test_loss: 229.945831 (Saved)\n",
      "epoch: 21700, train_loss: 324.614708, test_loss: 229.387375 (Saved)\n",
      "epoch: 21800, train_loss: 345.173325, test_loss: 223.314377 (Saved)\n",
      "epoch: 21900, train_loss: 313.823158, test_loss: 220.615158 (Saved)\n",
      "epoch: 22000, train_loss: 350.909637, test_loss: 215.679794 (Saved)\n",
      "epoch: 22100, train_loss: 295.941406, test_loss: 212.374222 (Saved)\n",
      "epoch: 22200, train_loss: 310.837280, test_loss: 209.640762 (Saved)\n",
      "epoch: 22300, train_loss: 288.648567, test_loss: 208.905899 (Saved)\n",
      "epoch: 22400, train_loss: 289.347557, test_loss: 203.676270 (Saved)\n",
      "epoch: 22500, train_loss: 286.654434, test_loss: 200.426315 (Saved)\n",
      "epoch: 22600, train_loss: 264.853844, test_loss: 198.523590 (Saved)\n",
      "epoch: 22700, train_loss: 273.898605, test_loss: 196.771408 (Saved)\n",
      "epoch: 22800, train_loss: 264.271805, test_loss: 192.187927 (Saved)\n",
      "epoch: 22900, train_loss: 284.961548, test_loss: 192.440903\n",
      "epoch: 23000, train_loss: 275.139099, test_loss: 188.583969 (Saved)\n",
      "epoch: 23100, train_loss: 248.274567, test_loss: 188.086197 (Saved)\n",
      "epoch: 23200, train_loss: 263.578018, test_loss: 185.951096 (Saved)\n",
      "epoch: 23300, train_loss: 307.070862, test_loss: 182.748245 (Saved)\n",
      "epoch: 23400, train_loss: 297.175003, test_loss: 181.570526 (Saved)\n",
      "epoch: 23500, train_loss: 266.433182, test_loss: 180.406281 (Saved)\n",
      "epoch: 23600, train_loss: 268.685219, test_loss: 180.520721\n",
      "epoch: 23700, train_loss: 275.682144, test_loss: 178.319397 (Saved)\n",
      "epoch: 23800, train_loss: 244.450188, test_loss: 174.882919 (Saved)\n",
      "epoch: 23900, train_loss: 274.734390, test_loss: 176.946396\n",
      "epoch: 24000, train_loss: 282.292542, test_loss: 173.690475 (Saved)\n",
      "epoch: 24100, train_loss: 223.839035, test_loss: 173.296570 (Saved)\n",
      "epoch: 24200, train_loss: 227.551369, test_loss: 172.147202 (Saved)\n",
      "epoch: 24300, train_loss: 236.572350, test_loss: 170.573898 (Saved)\n",
      "epoch: 24400, train_loss: 228.517891, test_loss: 169.641663 (Saved)\n",
      "epoch: 24500, train_loss: 263.983208, test_loss: 166.483902 (Saved)\n",
      "epoch: 24600, train_loss: 258.691574, test_loss: 166.362152 (Saved)\n",
      "epoch: 24700, train_loss: 247.739059, test_loss: 165.348969 (Saved)\n",
      "epoch: 24800, train_loss: 269.633080, test_loss: 163.155518 (Saved)\n",
      "epoch: 24900, train_loss: 256.206154, test_loss: 163.185699\n",
      "epoch: 25000, train_loss: 222.726410, test_loss: 161.771759 (Saved)\n",
      "epoch: 25100, train_loss: 246.832855, test_loss: 162.253830\n",
      "epoch: 25200, train_loss: 216.078621, test_loss: 160.135178 (Saved)\n",
      "epoch: 25300, train_loss: 215.816452, test_loss: 158.881302 (Saved)\n",
      "epoch: 25400, train_loss: 222.946877, test_loss: 159.433334\n",
      "epoch: 25500, train_loss: 205.663048, test_loss: 156.365891 (Saved)\n",
      "epoch: 25600, train_loss: 232.618729, test_loss: 157.584778\n",
      "epoch: 25700, train_loss: 210.131264, test_loss: 155.234497 (Saved)\n",
      "epoch: 25800, train_loss: 216.755203, test_loss: 154.969147 (Saved)\n",
      "epoch: 25900, train_loss: 208.149597, test_loss: 156.070541\n",
      "epoch: 26000, train_loss: 209.889099, test_loss: 155.203018\n",
      "epoch: 26100, train_loss: 242.101837, test_loss: 154.360001 (Saved)\n",
      "epoch: 26200, train_loss: 212.482323, test_loss: 153.428757 (Saved)\n",
      "epoch: 26300, train_loss: 206.615623, test_loss: 152.374329 (Saved)\n",
      "epoch: 26400, train_loss: 223.600868, test_loss: 151.805359 (Saved)\n",
      "epoch: 26500, train_loss: 197.058716, test_loss: 149.294861 (Saved)\n",
      "epoch: 26600, train_loss: 200.536697, test_loss: 150.590698\n",
      "epoch: 26700, train_loss: 199.310936, test_loss: 148.900620 (Saved)\n",
      "epoch: 26800, train_loss: 200.580444, test_loss: 147.572922 (Saved)\n",
      "epoch: 26900, train_loss: 225.707138, test_loss: 148.783432\n",
      "epoch: 27000, train_loss: 205.550613, test_loss: 148.768417\n",
      "epoch: 27100, train_loss: 199.573280, test_loss: 146.615051 (Saved)\n",
      "epoch: 27200, train_loss: 226.345734, test_loss: 145.075150 (Saved)\n",
      "epoch: 27300, train_loss: 189.580902, test_loss: 142.566177 (Saved)\n",
      "epoch: 27400, train_loss: 203.331474, test_loss: 144.221176\n",
      "epoch: 27500, train_loss: 209.464714, test_loss: 141.117905 (Saved)\n",
      "epoch: 27600, train_loss: 193.391731, test_loss: 143.186935\n",
      "epoch: 27700, train_loss: 206.584068, test_loss: 140.971024 (Saved)\n",
      "epoch: 27800, train_loss: 213.983871, test_loss: 139.681625 (Saved)\n",
      "epoch: 27900, train_loss: 218.065865, test_loss: 140.248627\n",
      "epoch: 28000, train_loss: 203.155457, test_loss: 139.697708\n",
      "epoch: 28100, train_loss: 203.231140, test_loss: 139.264954 (Saved)\n",
      "epoch: 28200, train_loss: 211.647690, test_loss: 137.942093 (Saved)\n",
      "epoch: 28300, train_loss: 187.134079, test_loss: 136.808090 (Saved)\n",
      "epoch: 28400, train_loss: 191.835686, test_loss: 134.070801 (Saved)\n",
      "epoch: 28500, train_loss: 181.829300, test_loss: 134.274780\n",
      "epoch: 28600, train_loss: 200.916809, test_loss: 136.600464\n",
      "epoch: 28700, train_loss: 206.350571, test_loss: 133.577286 (Saved)\n",
      "epoch: 28800, train_loss: 177.866051, test_loss: 131.712982 (Saved)\n",
      "epoch: 28900, train_loss: 198.326080, test_loss: 133.234482\n",
      "epoch: 29000, train_loss: 181.822632, test_loss: 130.682266 (Saved)\n",
      "epoch: 29100, train_loss: 188.090897, test_loss: 131.388962\n",
      "epoch: 29200, train_loss: 181.817535, test_loss: 129.744446 (Saved)\n",
      "epoch: 29300, train_loss: 186.661201, test_loss: 129.915680\n",
      "epoch: 29400, train_loss: 172.153893, test_loss: 129.821762\n",
      "epoch: 29500, train_loss: 194.155479, test_loss: 127.833199 (Saved)\n",
      "epoch: 29600, train_loss: 177.254524, test_loss: 128.392746\n",
      "epoch: 29700, train_loss: 176.105370, test_loss: 124.718040 (Saved)\n",
      "epoch: 29800, train_loss: 187.332443, test_loss: 124.845917\n",
      "epoch: 29900, train_loss: 186.665260, test_loss: 126.200684\n",
      "epoch: 30000, train_loss: 176.272263, test_loss: 125.066376\n",
      "epoch: 30100, train_loss: 176.668732, test_loss: 123.363197 (Saved)\n",
      "epoch: 30200, train_loss: 171.416412, test_loss: 123.405617\n",
      "epoch: 30300, train_loss: 192.337296, test_loss: 122.150307 (Saved)\n",
      "epoch: 30400, train_loss: 174.147888, test_loss: 120.872360 (Saved)\n",
      "epoch: 30500, train_loss: 175.002968, test_loss: 120.787346 (Saved)\n",
      "epoch: 30600, train_loss: 178.624260, test_loss: 118.879547 (Saved)\n",
      "epoch: 30700, train_loss: 173.205727, test_loss: 120.108124\n",
      "epoch: 30800, train_loss: 162.495949, test_loss: 118.388512 (Saved)\n",
      "epoch: 30900, train_loss: 168.975403, test_loss: 117.233902 (Saved)\n",
      "epoch: 31000, train_loss: 162.632851, test_loss: 116.264488 (Saved)\n",
      "epoch: 31100, train_loss: 174.979492, test_loss: 116.197693 (Saved)\n",
      "epoch: 31200, train_loss: 168.904877, test_loss: 115.175484 (Saved)\n",
      "epoch: 31300, train_loss: 158.472435, test_loss: 113.986778 (Saved)\n",
      "epoch: 31400, train_loss: 186.737160, test_loss: 114.945549\n",
      "epoch: 31500, train_loss: 176.404121, test_loss: 114.924171\n",
      "epoch: 31600, train_loss: 168.737923, test_loss: 113.144501 (Saved)\n",
      "epoch: 31700, train_loss: 177.986084, test_loss: 112.580055 (Saved)\n",
      "epoch: 31800, train_loss: 170.075638, test_loss: 111.888443 (Saved)\n",
      "epoch: 31900, train_loss: 153.497597, test_loss: 111.836258 (Saved)\n",
      "epoch: 32000, train_loss: 181.007835, test_loss: 109.106445 (Saved)\n",
      "epoch: 32100, train_loss: 156.241325, test_loss: 108.056427 (Saved)\n",
      "epoch: 32200, train_loss: 149.553162, test_loss: 108.992149\n",
      "epoch: 32300, train_loss: 163.439133, test_loss: 108.685997\n",
      "epoch: 32400, train_loss: 160.221909, test_loss: 106.332436 (Saved)\n",
      "epoch: 32500, train_loss: 164.979271, test_loss: 106.519478\n",
      "epoch: 32600, train_loss: 134.997421, test_loss: 105.913551 (Saved)\n",
      "epoch: 32700, train_loss: 172.709824, test_loss: 104.457985 (Saved)\n",
      "epoch: 32800, train_loss: 139.861141, test_loss: 104.164551 (Saved)\n",
      "epoch: 32900, train_loss: 141.408039, test_loss: 103.757561 (Saved)\n",
      "epoch: 33000, train_loss: 137.478645, test_loss: 100.987167 (Saved)\n",
      "epoch: 33100, train_loss: 158.162674, test_loss: 101.644577\n",
      "epoch: 33200, train_loss: 144.051559, test_loss: 101.340569\n",
      "epoch: 33300, train_loss: 155.056625, test_loss: 101.100494\n",
      "epoch: 33400, train_loss: 143.896721, test_loss: 99.886223 (Saved)\n",
      "epoch: 33500, train_loss: 137.927738, test_loss: 99.651222 (Saved)\n",
      "epoch: 33600, train_loss: 149.302299, test_loss: 98.800102 (Saved)\n",
      "epoch: 33700, train_loss: 135.315987, test_loss: 98.301422 (Saved)\n",
      "epoch: 33800, train_loss: 138.860344, test_loss: 96.206970 (Saved)\n",
      "epoch: 33900, train_loss: 129.898113, test_loss: 97.390862\n",
      "epoch: 34000, train_loss: 152.094166, test_loss: 97.207085\n",
      "epoch: 34100, train_loss: 129.002758, test_loss: 95.853241 (Saved)\n",
      "epoch: 34200, train_loss: 134.587219, test_loss: 94.114418 (Saved)\n",
      "epoch: 34300, train_loss: 145.263542, test_loss: 94.153351\n",
      "epoch: 34400, train_loss: 151.074524, test_loss: 93.938118 (Saved)\n",
      "epoch: 34500, train_loss: 122.425865, test_loss: 94.385742\n",
      "epoch: 34600, train_loss: 122.378445, test_loss: 92.260330 (Saved)\n",
      "epoch: 34700, train_loss: 141.006889, test_loss: 91.493233 (Saved)\n",
      "epoch: 34800, train_loss: 121.230549, test_loss: 90.742188 (Saved)\n",
      "epoch: 34900, train_loss: 130.270901, test_loss: 89.662964 (Saved)\n",
      "epoch: 35000, train_loss: 121.317577, test_loss: 90.940430\n",
      "epoch: 35100, train_loss: 148.323265, test_loss: 89.085793 (Saved)\n",
      "epoch: 35200, train_loss: 118.527298, test_loss: 88.050751 (Saved)\n",
      "epoch: 35300, train_loss: 151.514008, test_loss: 87.181229 (Saved)\n",
      "epoch: 35400, train_loss: 125.931816, test_loss: 87.860741\n",
      "epoch: 35500, train_loss: 129.781425, test_loss: 86.537384 (Saved)\n",
      "epoch: 35600, train_loss: 125.979183, test_loss: 87.470078\n",
      "epoch: 35700, train_loss: 124.287392, test_loss: 87.356964\n",
      "epoch: 35800, train_loss: 130.919136, test_loss: 84.019279 (Saved)\n",
      "epoch: 35900, train_loss: 117.091354, test_loss: 85.764977\n",
      "epoch: 36000, train_loss: 116.424374, test_loss: 83.854904 (Saved)\n",
      "epoch: 36100, train_loss: 124.177364, test_loss: 82.310059 (Saved)\n",
      "epoch: 36200, train_loss: 123.402588, test_loss: 82.683868\n",
      "epoch: 36300, train_loss: 111.298939, test_loss: 81.992783 (Saved)\n",
      "epoch: 36400, train_loss: 111.695538, test_loss: 81.058434 (Saved)\n",
      "epoch: 36500, train_loss: 124.537292, test_loss: 80.699356 (Saved)\n",
      "epoch: 36600, train_loss: 124.610420, test_loss: 80.060791 (Saved)\n",
      "epoch: 36700, train_loss: 108.259212, test_loss: 79.322334 (Saved)\n",
      "epoch: 36800, train_loss: 123.246426, test_loss: 80.429337\n",
      "epoch: 36900, train_loss: 113.387691, test_loss: 79.798996\n",
      "epoch: 37000, train_loss: 116.131996, test_loss: 78.529457 (Saved)\n",
      "epoch: 37100, train_loss: 116.732677, test_loss: 78.217506 (Saved)\n",
      "epoch: 37200, train_loss: 112.881084, test_loss: 76.391861 (Saved)\n",
      "epoch: 37300, train_loss: 108.278992, test_loss: 76.226486 (Saved)\n",
      "epoch: 37400, train_loss: 117.694489, test_loss: 75.763351 (Saved)\n",
      "epoch: 37500, train_loss: 108.773232, test_loss: 74.529991 (Saved)\n",
      "epoch: 37600, train_loss: 107.153267, test_loss: 74.747803\n",
      "epoch: 37700, train_loss: 120.843197, test_loss: 73.723312 (Saved)\n",
      "epoch: 37800, train_loss: 111.490776, test_loss: 72.726746 (Saved)\n",
      "epoch: 37900, train_loss: 117.849144, test_loss: 73.210197\n",
      "epoch: 38000, train_loss: 102.957745, test_loss: 72.653435 (Saved)\n",
      "epoch: 38100, train_loss: 98.327618, test_loss: 71.211861 (Saved)\n",
      "epoch: 38200, train_loss: 105.178501, test_loss: 71.274582\n",
      "epoch: 38300, train_loss: 117.888088, test_loss: 71.601509\n",
      "epoch: 38400, train_loss: 113.969681, test_loss: 69.250488 (Saved)\n",
      "epoch: 38500, train_loss: 115.309105, test_loss: 69.702766\n",
      "epoch: 38600, train_loss: 100.961842, test_loss: 68.521835 (Saved)\n",
      "epoch: 38700, train_loss: 100.475597, test_loss: 68.744102\n",
      "epoch: 38800, train_loss: 89.943977, test_loss: 66.675194 (Saved)\n",
      "epoch: 38900, train_loss: 105.797237, test_loss: 67.329269\n",
      "epoch: 39000, train_loss: 94.350681, test_loss: 66.720894\n",
      "epoch: 39100, train_loss: 103.873405, test_loss: 65.884125 (Saved)\n",
      "epoch: 39200, train_loss: 96.729259, test_loss: 66.408592\n",
      "epoch: 39300, train_loss: 95.277668, test_loss: 64.805954 (Saved)\n",
      "epoch: 39400, train_loss: 107.656372, test_loss: 65.769402\n",
      "epoch: 39500, train_loss: 107.535366, test_loss: 64.051750 (Saved)\n",
      "epoch: 39600, train_loss: 101.235176, test_loss: 63.091274 (Saved)\n",
      "epoch: 39700, train_loss: 92.836399, test_loss: 62.949368 (Saved)\n",
      "epoch: 39800, train_loss: 92.308834, test_loss: 62.458530 (Saved)\n",
      "epoch: 39900, train_loss: 96.826382, test_loss: 62.652473\n",
      "epoch: 40000, train_loss: 93.214836, test_loss: 60.901241 (Saved)\n",
      "epoch: 40100, train_loss: 87.932899, test_loss: 61.657852\n",
      "epoch: 40200, train_loss: 93.404881, test_loss: 60.306892 (Saved)\n",
      "epoch: 40300, train_loss: 94.604748, test_loss: 60.069614 (Saved)\n",
      "epoch: 40400, train_loss: 88.352524, test_loss: 59.520275 (Saved)\n",
      "epoch: 40500, train_loss: 92.677471, test_loss: 59.505409 (Saved)\n",
      "epoch: 40600, train_loss: 80.740482, test_loss: 58.856510 (Saved)\n",
      "epoch: 40700, train_loss: 85.720879, test_loss: 59.048862\n",
      "epoch: 40800, train_loss: 86.531170, test_loss: 57.802715 (Saved)\n",
      "epoch: 40900, train_loss: 84.027180, test_loss: 57.896343\n",
      "epoch: 41000, train_loss: 82.673172, test_loss: 56.412735 (Saved)\n",
      "epoch: 41100, train_loss: 80.282688, test_loss: 56.845238\n",
      "epoch: 41200, train_loss: 94.511494, test_loss: 56.117886 (Saved)\n",
      "epoch: 41300, train_loss: 89.556786, test_loss: 55.516174 (Saved)\n",
      "epoch: 41400, train_loss: 92.689659, test_loss: 54.820911 (Saved)\n",
      "epoch: 41500, train_loss: 81.430340, test_loss: 54.523670 (Saved)\n",
      "epoch: 41600, train_loss: 91.199108, test_loss: 54.008553 (Saved)\n",
      "epoch: 41700, train_loss: 89.967274, test_loss: 53.724457 (Saved)\n",
      "epoch: 41800, train_loss: 85.134327, test_loss: 54.010628\n",
      "epoch: 41900, train_loss: 82.504673, test_loss: 53.060345 (Saved)\n",
      "epoch: 42000, train_loss: 76.451221, test_loss: 52.329842 (Saved)\n",
      "epoch: 42100, train_loss: 81.179829, test_loss: 52.193054 (Saved)\n",
      "epoch: 42200, train_loss: 87.303391, test_loss: 52.795067\n",
      "epoch: 42300, train_loss: 79.603188, test_loss: 50.889984 (Saved)\n",
      "epoch: 42400, train_loss: 85.824722, test_loss: 50.011421 (Saved)\n",
      "epoch: 42500, train_loss: 84.316444, test_loss: 49.943466 (Saved)\n",
      "epoch: 42600, train_loss: 84.812263, test_loss: 50.740177\n",
      "epoch: 42700, train_loss: 76.640472, test_loss: 49.022053 (Saved)\n",
      "epoch: 42800, train_loss: 85.958942, test_loss: 48.703552 (Saved)\n",
      "epoch: 42900, train_loss: 80.525932, test_loss: 48.219791 (Saved)\n",
      "epoch: 43000, train_loss: 83.047684, test_loss: 47.937538 (Saved)\n",
      "epoch: 43100, train_loss: 72.967804, test_loss: 46.925083 (Saved)\n",
      "epoch: 43200, train_loss: 89.297329, test_loss: 47.740459\n",
      "epoch: 43300, train_loss: 75.026371, test_loss: 46.881409 (Saved)\n",
      "epoch: 43400, train_loss: 75.084015, test_loss: 46.254082 (Saved)\n",
      "epoch: 43500, train_loss: 75.589233, test_loss: 46.334366\n",
      "epoch: 43600, train_loss: 75.187580, test_loss: 45.213173 (Saved)\n",
      "epoch: 43700, train_loss: 80.863413, test_loss: 45.630066\n",
      "epoch: 43800, train_loss: 69.387558, test_loss: 44.171833 (Saved)\n",
      "epoch: 43900, train_loss: 69.147673, test_loss: 44.391216\n",
      "epoch: 44000, train_loss: 71.767544, test_loss: 44.013195 (Saved)\n",
      "epoch: 44100, train_loss: 61.844124, test_loss: 43.222485 (Saved)\n",
      "epoch: 44200, train_loss: 71.510563, test_loss: 43.967648\n",
      "epoch: 44300, train_loss: 72.987011, test_loss: 43.266655\n",
      "epoch: 44400, train_loss: 67.442165, test_loss: 42.829998 (Saved)\n",
      "epoch: 44500, train_loss: 64.736649, test_loss: 41.785793 (Saved)\n",
      "epoch: 44600, train_loss: 63.865911, test_loss: 41.610760 (Saved)\n",
      "epoch: 44700, train_loss: 66.168581, test_loss: 41.487682 (Saved)\n",
      "epoch: 44800, train_loss: 68.691357, test_loss: 40.739647 (Saved)\n",
      "epoch: 44900, train_loss: 66.804790, test_loss: 40.294533 (Saved)\n",
      "epoch: 45000, train_loss: 77.762730, test_loss: 40.117126 (Saved)\n",
      "epoch: 45100, train_loss: 62.437380, test_loss: 39.828739 (Saved)\n",
      "epoch: 45200, train_loss: 71.115421, test_loss: 39.924534\n",
      "epoch: 45300, train_loss: 62.080006, test_loss: 39.388775 (Saved)\n",
      "epoch: 45400, train_loss: 64.099117, test_loss: 38.783382 (Saved)\n",
      "epoch: 45500, train_loss: 64.012512, test_loss: 38.174480 (Saved)\n",
      "epoch: 45600, train_loss: 61.793692, test_loss: 38.258968\n",
      "epoch: 45700, train_loss: 70.039816, test_loss: 37.491844 (Saved)\n",
      "epoch: 45800, train_loss: 69.294842, test_loss: 37.727596\n",
      "epoch: 45900, train_loss: 68.332653, test_loss: 37.395439 (Saved)\n",
      "epoch: 46000, train_loss: 59.966278, test_loss: 36.860920 (Saved)\n",
      "epoch: 46100, train_loss: 63.747093, test_loss: 36.832737 (Saved)\n",
      "epoch: 46200, train_loss: 59.733807, test_loss: 35.704144 (Saved)\n",
      "epoch: 46300, train_loss: 61.142048, test_loss: 35.508217 (Saved)\n",
      "epoch: 46400, train_loss: 59.755699, test_loss: 35.562973\n",
      "epoch: 46500, train_loss: 62.616943, test_loss: 35.098988 (Saved)\n",
      "epoch: 46600, train_loss: 57.233900, test_loss: 35.134441\n",
      "epoch: 46700, train_loss: 59.544043, test_loss: 34.493870 (Saved)\n",
      "epoch: 46800, train_loss: 60.304382, test_loss: 34.342388 (Saved)\n",
      "epoch: 46900, train_loss: 58.104572, test_loss: 34.564041\n",
      "epoch: 47000, train_loss: 58.525751, test_loss: 33.778118 (Saved)\n",
      "epoch: 47100, train_loss: 64.389767, test_loss: 33.623821 (Saved)\n",
      "epoch: 47200, train_loss: 54.852745, test_loss: 33.135235 (Saved)\n",
      "epoch: 47300, train_loss: 59.030890, test_loss: 33.411240\n",
      "epoch: 47400, train_loss: 61.972420, test_loss: 33.120682 (Saved)\n",
      "epoch: 47500, train_loss: 55.703920, test_loss: 32.446476 (Saved)\n",
      "epoch: 47600, train_loss: 63.555981, test_loss: 32.079414 (Saved)\n",
      "epoch: 47700, train_loss: 61.615910, test_loss: 31.850948 (Saved)\n",
      "epoch: 47800, train_loss: 59.549999, test_loss: 31.783390 (Saved)\n",
      "epoch: 47900, train_loss: 59.253946, test_loss: 32.311844\n",
      "epoch: 48000, train_loss: 62.415787, test_loss: 31.596100 (Saved)\n",
      "epoch: 48100, train_loss: 57.491610, test_loss: 30.435181 (Saved)\n",
      "epoch: 48200, train_loss: 51.783123, test_loss: 30.744951\n",
      "epoch: 48300, train_loss: 59.969973, test_loss: 30.062452 (Saved)\n",
      "epoch: 48400, train_loss: 55.653105, test_loss: 29.930304 (Saved)\n",
      "epoch: 48500, train_loss: 55.434389, test_loss: 30.411022\n",
      "epoch: 48600, train_loss: 54.835981, test_loss: 30.601580\n",
      "epoch: 48700, train_loss: 52.843721, test_loss: 29.337992 (Saved)\n",
      "epoch: 48800, train_loss: 56.920248, test_loss: 29.668137\n",
      "epoch: 48900, train_loss: 50.537785, test_loss: 29.178675 (Saved)\n",
      "epoch: 49000, train_loss: 54.264334, test_loss: 28.655149 (Saved)\n",
      "epoch: 49100, train_loss: 50.832138, test_loss: 28.326305 (Saved)\n",
      "epoch: 49200, train_loss: 48.372421, test_loss: 28.607725\n",
      "epoch: 49300, train_loss: 55.286299, test_loss: 27.800484 (Saved)\n",
      "epoch: 49400, train_loss: 59.132786, test_loss: 27.566145 (Saved)\n",
      "epoch: 49500, train_loss: 52.314814, test_loss: 28.004297\n",
      "epoch: 49600, train_loss: 49.574499, test_loss: 27.443752 (Saved)\n",
      "epoch: 49700, train_loss: 47.681059, test_loss: 27.658957\n",
      "epoch: 49800, train_loss: 53.678904, test_loss: 27.085182 (Saved)\n",
      "epoch: 49900, train_loss: 52.929022, test_loss: 26.786219 (Saved)\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/protanjung/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Saving ONNX model: /home/protanjung/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Exported graph: graph(%onnx::Sub_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(4, 2, strides=[2, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(16, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.weight : Float(16, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.weight : Float(4, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.weight : Float(2, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/Constant_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 0 -300 [ CUDAFloatType{2} ], onnx_name=\"/Constant\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:16:0\n",
      "  %/Sub_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Sub[onnx_name=\"/Sub\"](%onnx::Sub_0, %/Constant_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:16:0\n",
      "  %/Constant_1_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 450  600 [ CUDAFloatType{2} ], onnx_name=\"/Constant_1\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:16:0\n",
      "  %/Div_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Div[onnx_name=\"/Div\"](%/Sub_output_0, %/Constant_1_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:16:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Div_output_0, %fc1.weight, %fc1.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc1 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh\"](%/fc1/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Tanh_output_0, %fc2.weight, %fc2.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc2 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_1_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_1\"](%/fc2/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc3/Gemm_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Tanh_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc3 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_2_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_2\"](%/fc3/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc4/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc4/Gemm\"](%/Tanh_2_output_0, %fc4.weight, %fc4.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc4 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc5/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc5/Gemm\"](%/fc4/Gemm_output_0, %fc5.weight, %fc5.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc5 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc6/Gemm_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc6/Gemm\"](%/fc5/Gemm_output_0, %fc6.weight, %fc6.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc6 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_2_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1242   432 [ CUDAFloatType{2} ], onnx_name=\"/Constant_2\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:23:0\n",
      "  %/Mul_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Mul[onnx_name=\"/Mul\"](%/fc6/Gemm_output_0, %/Constant_2_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:23:0\n",
      "  %/Constant_3_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value=  30  260 [ CUDAFloatType{2} ], onnx_name=\"/Constant_3\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:23:0\n",
      "  %29 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/Add\"](%/Mul_output_0, %/Constant_3_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_42326/541484460.py:23:0\n",
      "  return (%29)\n",
      "\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Saved ONNX model: /home/protanjung/icar-ng-data/model/cm2pixel_model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'cm2pixel_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 100.,  150.],\n",
      "        [  75.,   50.],\n",
      "        [ 350.,  -50.],\n",
      "        [ 350., -300.],\n",
      "        [   0.,  -75.],\n",
      "        [ 100.,  -75.],\n",
      "        [ 350.,  150.],\n",
      "        [ 250., -200.],\n",
      "        [  50., -150.],\n",
      "        [ 250.,  250.],\n",
      "        [ 150., -250.],\n",
      "        [ 450., -200.],\n",
      "        [ 450., -300.],\n",
      "        [  75.,  150.],\n",
      "        [  50.,  100.],\n",
      "        [ 250., -100.],\n",
      "        [ 250.,  150.],\n",
      "        [ 100., -200.],\n",
      "        [  75.,  -50.],\n",
      "        [ 450.,  -50.],\n",
      "        [ 100., -150.],\n",
      "        [ 350.,    0.],\n",
      "        [ 200.,    0.]], device='cuda:0')\n",
      "y\n",
      "tensor([[1098.,  434.],\n",
      "        [ 816.,  478.],\n",
      "        [ 570.,  290.],\n",
      "        [ 222.,  298.],\n",
      "        [ 226.,  690.],\n",
      "        [ 416.,  446.],\n",
      "        [ 852.,  286.],\n",
      "        [ 286.,  332.],\n",
      "        [  60.,  534.],\n",
      "        [1092.,  318.],\n",
      "        [  36.,  400.],\n",
      "        [ 410.,  270.],\n",
      "        [ 298.,  272.],\n",
      "        [1164.,  470.],\n",
      "        [1040.,  522.],\n",
      "        [ 462.,  328.],\n",
      "        [ 910.,  324.],\n",
      "        [  46.,  452.],\n",
      "        [ 472.,  484.],\n",
      "        [ 582.,  268.],\n",
      "        [ 194.,  452.],\n",
      "        [ 642.,  290.],\n",
      "        [ 642.,  354.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[1104.2079,  436.9523],\n",
      "        [ 812.0732,  479.9867],\n",
      "        [ 573.8004,  291.4407],\n",
      "        [ 223.0535,  296.1109],\n",
      "        [ 238.9830,  685.7022],\n",
      "        [ 415.9720,  447.1711],\n",
      "        [ 851.5806,  287.1536],\n",
      "        [ 292.8445,  332.1313],\n",
      "        [  55.6691,  546.3011],\n",
      "        [1088.0441,  313.3039],\n",
      "        [  30.9225,  405.3595],\n",
      "        [ 405.7810,  268.1164],\n",
      "        [ 290.1965,  267.4725],\n",
      "        [1170.9095,  478.2445],\n",
      "        [1044.2443,  530.5803],\n",
      "        [ 470.7433,  327.2422],\n",
      "        [ 907.0458,  316.8361],\n",
      "        [  39.7001,  459.7008],\n",
      "        [ 468.2766,  485.3687],\n",
      "        [ 578.3268,  269.8356],\n",
      "        [ 190.9289,  454.0554],\n",
      "        [ 642.7972,  290.5618],\n",
      "        [ 645.0547,  348.2336]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[ -6.2079,  -2.9523],\n",
      "        [  3.9268,  -1.9867],\n",
      "        [ -3.8004,  -1.4407],\n",
      "        [ -1.0535,   1.8891],\n",
      "        [-12.9830,   4.2978],\n",
      "        [  0.0280,  -1.1711],\n",
      "        [  0.4194,  -1.1536],\n",
      "        [ -6.8445,  -0.1313],\n",
      "        [  4.3309, -12.3011],\n",
      "        [  3.9559,   4.6961],\n",
      "        [  5.0775,  -5.3595],\n",
      "        [  4.2190,   1.8836],\n",
      "        [  7.8035,   4.5275],\n",
      "        [ -6.9095,  -8.2445],\n",
      "        [ -4.2443,  -8.5803],\n",
      "        [ -8.7433,   0.7578],\n",
      "        [  2.9542,   7.1639],\n",
      "        [  6.2999,  -7.7008],\n",
      "        [  3.7234,  -1.3687],\n",
      "        [  3.6732,  -1.8356],\n",
      "        [  3.0711,  -2.0554],\n",
      "        [ -0.7972,  -0.5618],\n",
      "        [ -3.0547,   5.7664]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
