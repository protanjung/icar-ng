{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 02/01/2024 at 10:45:55.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/icar/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/icar/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/icar/icar-ng-data/model\n",
      "Model file: cm2pixel_model.pt\n",
      "Model path: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'cm2pixel_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([   0.0000, -665.6403]), max_x: tensor([1200.0000,  848.5281])\n",
      "min_y: tensor([ 36., 204.]), max_y: tensor([1272.,  692.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 2:4].values\n",
    "        y = self.dataframe.iloc[:, 0:2].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 118\n",
      "dataset_test: 29\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 20)\n",
    "        self.fc3 = nn.Linear(20, 80)\n",
    "        self.fc4 = nn.Linear(80, 20)\n",
    "        self.fc5 = nn.Linear(20, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n",
      "Loaded model: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 73125.289062, test_loss: 42636.726562 (Saved)\n",
      "epoch: 100, train_loss: 8077.095215, test_loss: 4556.588379 (Saved)\n",
      "epoch: 200, train_loss: 1718.268677, test_loss: 913.086914 (Saved)\n",
      "epoch: 300, train_loss: 538.754898, test_loss: 314.804596 (Saved)\n",
      "epoch: 400, train_loss: 219.228249, test_loss: 136.300308 (Saved)\n",
      "epoch: 500, train_loss: 123.270710, test_loss: 81.291687 (Saved)\n",
      "epoch: 600, train_loss: 89.402271, test_loss: 62.329597 (Saved)\n",
      "epoch: 700, train_loss: 71.483967, test_loss: 54.795700 (Saved)\n",
      "epoch: 800, train_loss: 62.338158, test_loss: 50.787575 (Saved)\n",
      "epoch: 900, train_loss: 53.955643, test_loss: 47.993069 (Saved)\n",
      "epoch: 1000, train_loss: 49.292027, test_loss: 45.540276 (Saved)\n",
      "epoch: 1100, train_loss: 47.094807, test_loss: 43.074421 (Saved)\n",
      "epoch: 1200, train_loss: 43.487545, test_loss: 40.875008 (Saved)\n",
      "epoch: 1300, train_loss: 40.384619, test_loss: 39.035568 (Saved)\n",
      "epoch: 1400, train_loss: 38.510887, test_loss: 37.528011 (Saved)\n",
      "epoch: 1500, train_loss: 36.032251, test_loss: 36.053143 (Saved)\n",
      "epoch: 1600, train_loss: 34.334443, test_loss: 34.860546 (Saved)\n",
      "epoch: 1700, train_loss: 31.721554, test_loss: 33.945930 (Saved)\n",
      "epoch: 1800, train_loss: 29.978985, test_loss: 33.286732 (Saved)\n",
      "epoch: 1900, train_loss: 28.969604, test_loss: 32.682163 (Saved)\n",
      "epoch: 2000, train_loss: 26.881810, test_loss: 31.964340 (Saved)\n",
      "epoch: 2100, train_loss: 25.633050, test_loss: 31.317204 (Saved)\n",
      "epoch: 2200, train_loss: 24.138865, test_loss: 30.664879 (Saved)\n",
      "epoch: 2300, train_loss: 23.538306, test_loss: 30.161158 (Saved)\n",
      "epoch: 2400, train_loss: 22.436723, test_loss: 29.777912 (Saved)\n",
      "epoch: 2500, train_loss: 21.348404, test_loss: 29.500648 (Saved)\n",
      "epoch: 2600, train_loss: 20.255152, test_loss: 29.084730 (Saved)\n",
      "epoch: 2700, train_loss: 19.950530, test_loss: 28.761698 (Saved)\n",
      "epoch: 2800, train_loss: 19.375480, test_loss: 28.572615 (Saved)\n",
      "epoch: 2900, train_loss: 18.419986, test_loss: 28.447884 (Saved)\n",
      "epoch: 3000, train_loss: 17.758552, test_loss: 28.446306 (Saved)\n",
      "epoch: 3100, train_loss: 17.664505, test_loss: 28.280033 (Saved)\n",
      "epoch: 3200, train_loss: 16.624485, test_loss: 28.375080\n",
      "epoch: 3300, train_loss: 16.664099, test_loss: 28.386307\n",
      "epoch: 3400, train_loss: 16.079081, test_loss: 28.267372 (Saved)\n",
      "epoch: 3500, train_loss: 15.522696, test_loss: 28.206083 (Saved)\n",
      "epoch: 3600, train_loss: 15.766648, test_loss: 28.337986\n",
      "epoch: 3700, train_loss: 14.637488, test_loss: 28.333824\n",
      "epoch: 3800, train_loss: 14.405666, test_loss: 28.379438\n",
      "epoch: 3900, train_loss: 14.747795, test_loss: 28.348114\n",
      "epoch: 4000, train_loss: 13.747495, test_loss: 28.209719\n",
      "epoch: 4100, train_loss: 13.740284, test_loss: 28.004419 (Saved)\n",
      "epoch: 4200, train_loss: 12.886660, test_loss: 27.423435 (Saved)\n",
      "epoch: 4300, train_loss: 12.204679, test_loss: 27.592299\n",
      "epoch: 4400, train_loss: 11.756309, test_loss: 27.941105\n",
      "epoch: 4500, train_loss: 11.292123, test_loss: 28.107084\n",
      "epoch: 4600, train_loss: 11.199921, test_loss: 28.439087\n",
      "epoch: 4700, train_loss: 10.628414, test_loss: 28.896494\n",
      "epoch: 4800, train_loss: 10.367247, test_loss: 28.941849\n",
      "epoch: 4900, train_loss: 9.942292, test_loss: 29.083117\n",
      "epoch: 5000, train_loss: 9.606755, test_loss: 28.963764\n",
      "epoch: 5100, train_loss: 9.285296, test_loss: 28.902853\n",
      "epoch: 5200, train_loss: 9.102101, test_loss: 28.866827\n",
      "epoch: 5300, train_loss: 8.875103, test_loss: 28.763142\n",
      "epoch: 5400, train_loss: 8.786840, test_loss: 28.440287\n",
      "epoch: 5500, train_loss: 8.621243, test_loss: 28.178881\n",
      "epoch: 5600, train_loss: 8.519092, test_loss: 27.953907\n",
      "epoch: 5700, train_loss: 8.435520, test_loss: 27.887802\n",
      "epoch: 5800, train_loss: 8.260888, test_loss: 27.620777\n",
      "epoch: 5900, train_loss: 8.101947, test_loss: 27.441755\n",
      "epoch: 6000, train_loss: 8.196964, test_loss: 27.375364 (Saved)\n",
      "epoch: 6100, train_loss: 7.869676, test_loss: 27.066616 (Saved)\n",
      "epoch: 6200, train_loss: 7.870754, test_loss: 27.137527\n",
      "epoch: 6300, train_loss: 7.791184, test_loss: 26.814014 (Saved)\n",
      "epoch: 6400, train_loss: 7.555978, test_loss: 26.919270\n",
      "epoch: 6500, train_loss: 7.476558, test_loss: 26.964870\n",
      "epoch: 6600, train_loss: 7.448522, test_loss: 26.927233\n",
      "epoch: 6700, train_loss: 7.305995, test_loss: 27.158085\n",
      "epoch: 6800, train_loss: 7.178228, test_loss: 26.764631 (Saved)\n",
      "epoch: 6900, train_loss: 7.149428, test_loss: 26.756350 (Saved)\n",
      "epoch: 7000, train_loss: 7.068928, test_loss: 26.819824\n",
      "epoch: 7100, train_loss: 7.015289, test_loss: 26.770691\n",
      "epoch: 7200, train_loss: 7.107350, test_loss: 26.759733\n",
      "epoch: 7300, train_loss: 6.899531, test_loss: 26.801033\n",
      "epoch: 7400, train_loss: 6.646548, test_loss: 26.762627\n",
      "epoch: 7500, train_loss: 6.673465, test_loss: 26.646358 (Saved)\n",
      "epoch: 7600, train_loss: 6.545052, test_loss: 26.435957 (Saved)\n",
      "epoch: 7700, train_loss: 6.449537, test_loss: 26.496861\n",
      "epoch: 7800, train_loss: 6.485956, test_loss: 26.479305\n",
      "epoch: 7900, train_loss: 6.336252, test_loss: 26.387903 (Saved)\n",
      "epoch: 8000, train_loss: 6.318659, test_loss: 26.599710\n",
      "epoch: 8100, train_loss: 6.189335, test_loss: 26.403791\n",
      "epoch: 8200, train_loss: 6.030376, test_loss: 26.433895\n",
      "epoch: 8300, train_loss: 6.049507, test_loss: 26.375830 (Saved)\n",
      "epoch: 8400, train_loss: 6.126412, test_loss: 26.260447 (Saved)\n",
      "epoch: 8500, train_loss: 5.976662, test_loss: 26.342321\n",
      "epoch: 8600, train_loss: 5.933465, test_loss: 26.271631\n",
      "epoch: 8700, train_loss: 5.782228, test_loss: 26.287901\n",
      "epoch: 8800, train_loss: 5.948712, test_loss: 26.192890 (Saved)\n",
      "epoch: 8900, train_loss: 5.732413, test_loss: 26.335756\n",
      "epoch: 9000, train_loss: 5.793026, test_loss: 26.164145 (Saved)\n",
      "epoch: 9100, train_loss: 5.691272, test_loss: 26.123283 (Saved)\n",
      "epoch: 9200, train_loss: 5.760888, test_loss: 26.245216\n",
      "epoch: 9300, train_loss: 5.550837, test_loss: 26.205385\n",
      "epoch: 9400, train_loss: 5.631534, test_loss: 26.143997\n",
      "epoch: 9500, train_loss: 5.496107, test_loss: 26.153858\n",
      "epoch: 9600, train_loss: 5.546252, test_loss: 26.108370 (Saved)\n",
      "epoch: 9700, train_loss: 5.473562, test_loss: 26.052221 (Saved)\n",
      "epoch: 9800, train_loss: 5.486977, test_loss: 25.923830 (Saved)\n",
      "epoch: 9900, train_loss: 5.386154, test_loss: 26.121881\n",
      "epoch: 10000, train_loss: 5.402817, test_loss: 26.081356\n",
      "epoch: 10100, train_loss: 5.325708, test_loss: 26.325098\n",
      "epoch: 10200, train_loss: 5.316377, test_loss: 26.063505\n",
      "epoch: 10300, train_loss: 5.255934, test_loss: 26.022438\n",
      "epoch: 10400, train_loss: 5.209440, test_loss: 25.872820 (Saved)\n",
      "epoch: 10500, train_loss: 5.121982, test_loss: 25.937559\n",
      "epoch: 10600, train_loss: 5.178423, test_loss: 26.049980\n",
      "epoch: 10700, train_loss: 5.057452, test_loss: 25.924707\n",
      "epoch: 10800, train_loss: 4.984634, test_loss: 25.775208 (Saved)\n",
      "epoch: 10900, train_loss: 4.856041, test_loss: 25.580379 (Saved)\n",
      "epoch: 11000, train_loss: 4.882810, test_loss: 25.479996 (Saved)\n",
      "epoch: 11100, train_loss: 4.974791, test_loss: 25.517347\n",
      "epoch: 11200, train_loss: 4.858308, test_loss: 25.484758\n",
      "epoch: 11300, train_loss: 4.681673, test_loss: 25.537479\n",
      "epoch: 11400, train_loss: 4.842018, test_loss: 25.362013 (Saved)\n",
      "epoch: 11500, train_loss: 4.751126, test_loss: 25.313093 (Saved)\n",
      "epoch: 11600, train_loss: 4.664631, test_loss: 25.370323\n",
      "epoch: 11700, train_loss: 4.570336, test_loss: 25.149035 (Saved)\n",
      "epoch: 11800, train_loss: 4.604377, test_loss: 25.223505\n",
      "epoch: 11900, train_loss: 4.613280, test_loss: 25.124390 (Saved)\n",
      "epoch: 12000, train_loss: 4.619545, test_loss: 25.034710 (Saved)\n",
      "epoch: 12100, train_loss: 4.589031, test_loss: 25.002516 (Saved)\n",
      "epoch: 12200, train_loss: 4.615262, test_loss: 25.034807\n",
      "epoch: 12300, train_loss: 4.540075, test_loss: 25.096544\n",
      "epoch: 12400, train_loss: 4.538203, test_loss: 24.946377 (Saved)\n",
      "epoch: 12500, train_loss: 4.542262, test_loss: 24.829151 (Saved)\n",
      "epoch: 12600, train_loss: 4.435954, test_loss: 24.698835 (Saved)\n",
      "epoch: 12700, train_loss: 4.454811, test_loss: 24.709343\n",
      "epoch: 12800, train_loss: 4.457763, test_loss: 24.672459 (Saved)\n",
      "epoch: 12900, train_loss: 4.504634, test_loss: 24.639717 (Saved)\n",
      "epoch: 13000, train_loss: 4.396132, test_loss: 24.688286\n",
      "epoch: 13100, train_loss: 4.332215, test_loss: 24.599117 (Saved)\n",
      "epoch: 13200, train_loss: 4.389752, test_loss: 24.686953\n",
      "epoch: 13300, train_loss: 4.288710, test_loss: 24.541653 (Saved)\n",
      "epoch: 13400, train_loss: 4.414183, test_loss: 24.597727\n",
      "epoch: 13500, train_loss: 4.277179, test_loss: 24.477480 (Saved)\n",
      "epoch: 13600, train_loss: 4.377395, test_loss: 24.441950 (Saved)\n",
      "epoch: 13700, train_loss: 4.313046, test_loss: 24.357340 (Saved)\n",
      "epoch: 13800, train_loss: 4.288206, test_loss: 24.410593\n",
      "epoch: 13900, train_loss: 4.242501, test_loss: 24.516554\n",
      "epoch: 14000, train_loss: 4.220071, test_loss: 24.310093 (Saved)\n",
      "epoch: 14100, train_loss: 4.356600, test_loss: 24.378851\n",
      "epoch: 14200, train_loss: 4.132116, test_loss: 24.217394 (Saved)\n",
      "epoch: 14300, train_loss: 4.240569, test_loss: 24.192774 (Saved)\n",
      "epoch: 14400, train_loss: 4.257286, test_loss: 24.273134\n",
      "epoch: 14500, train_loss: 4.246933, test_loss: 24.135033 (Saved)\n",
      "epoch: 14600, train_loss: 4.202953, test_loss: 24.074093 (Saved)\n",
      "epoch: 14700, train_loss: 4.237813, test_loss: 23.964806 (Saved)\n",
      "epoch: 14800, train_loss: 4.135654, test_loss: 24.037006\n",
      "epoch: 14900, train_loss: 4.229479, test_loss: 24.111376\n",
      "epoch: 15000, train_loss: 4.130493, test_loss: 23.989805\n",
      "epoch: 15100, train_loss: 4.169907, test_loss: 24.122486\n",
      "epoch: 15200, train_loss: 3.995410, test_loss: 24.019043\n",
      "epoch: 15300, train_loss: 4.060597, test_loss: 24.093363\n",
      "epoch: 15400, train_loss: 4.010221, test_loss: 24.054750\n",
      "epoch: 15500, train_loss: 3.959996, test_loss: 24.217958\n",
      "epoch: 15600, train_loss: 3.966970, test_loss: 24.114540\n",
      "epoch: 15700, train_loss: 4.005949, test_loss: 24.151653\n",
      "epoch: 15800, train_loss: 4.033319, test_loss: 24.138010\n",
      "epoch: 15900, train_loss: 3.948491, test_loss: 24.063059\n",
      "epoch: 16000, train_loss: 3.935997, test_loss: 24.104269\n",
      "epoch: 16100, train_loss: 3.976228, test_loss: 24.166573\n",
      "epoch: 16200, train_loss: 3.947570, test_loss: 24.108223\n",
      "epoch: 16300, train_loss: 3.923027, test_loss: 24.076626\n",
      "epoch: 16400, train_loss: 3.884194, test_loss: 24.072426\n",
      "epoch: 16500, train_loss: 3.954055, test_loss: 24.065098\n",
      "epoch: 16600, train_loss: 3.937307, test_loss: 24.102722\n",
      "epoch: 16700, train_loss: 3.931829, test_loss: 23.993422\n",
      "epoch: 16800, train_loss: 3.912806, test_loss: 23.935646 (Saved)\n",
      "epoch: 16900, train_loss: 3.841918, test_loss: 23.914356 (Saved)\n",
      "epoch: 17000, train_loss: 3.888864, test_loss: 23.969131\n",
      "epoch: 17100, train_loss: 3.847895, test_loss: 24.099075\n",
      "epoch: 17200, train_loss: 3.852389, test_loss: 23.883522 (Saved)\n",
      "epoch: 17300, train_loss: 3.869324, test_loss: 23.942461\n",
      "epoch: 17400, train_loss: 3.868628, test_loss: 23.834383 (Saved)\n",
      "epoch: 17500, train_loss: 3.763076, test_loss: 23.786419 (Saved)\n",
      "epoch: 17600, train_loss: 3.814534, test_loss: 23.830027\n",
      "epoch: 17700, train_loss: 3.849139, test_loss: 23.922951\n",
      "epoch: 17800, train_loss: 3.840679, test_loss: 23.875797\n",
      "epoch: 17900, train_loss: 3.855478, test_loss: 23.859085\n",
      "epoch: 18000, train_loss: 3.708342, test_loss: 23.961205\n",
      "epoch: 18100, train_loss: 3.854380, test_loss: 23.809708\n",
      "epoch: 18200, train_loss: 3.859797, test_loss: 23.817610\n",
      "epoch: 18300, train_loss: 3.700276, test_loss: 23.819159\n",
      "epoch: 18400, train_loss: 3.738622, test_loss: 23.803053\n",
      "epoch: 18500, train_loss: 3.716991, test_loss: 23.939417\n",
      "epoch: 18600, train_loss: 3.754441, test_loss: 23.830154\n",
      "epoch: 18700, train_loss: 3.719079, test_loss: 23.767376 (Saved)\n",
      "epoch: 18800, train_loss: 3.694100, test_loss: 23.876301\n",
      "epoch: 18900, train_loss: 3.857146, test_loss: 23.934050\n",
      "epoch: 19000, train_loss: 3.763299, test_loss: 23.675730 (Saved)\n",
      "epoch: 19100, train_loss: 3.641331, test_loss: 23.741880\n",
      "epoch: 19200, train_loss: 3.785531, test_loss: 23.675566 (Saved)\n",
      "epoch: 19300, train_loss: 3.644615, test_loss: 23.762510\n",
      "epoch: 19400, train_loss: 3.701111, test_loss: 23.717449\n",
      "epoch: 19500, train_loss: 3.661452, test_loss: 23.603022 (Saved)\n",
      "epoch: 19600, train_loss: 3.674123, test_loss: 23.638851\n",
      "epoch: 19700, train_loss: 3.668989, test_loss: 23.549665 (Saved)\n",
      "epoch: 19800, train_loss: 3.547001, test_loss: 23.542387 (Saved)\n",
      "epoch: 19900, train_loss: 3.668544, test_loss: 23.525618 (Saved)\n",
      "epoch: 20000, train_loss: 3.620675, test_loss: 23.586859\n",
      "epoch: 20100, train_loss: 3.604270, test_loss: 23.481731 (Saved)\n",
      "epoch: 20200, train_loss: 3.610137, test_loss: 23.550974\n",
      "epoch: 20300, train_loss: 3.591408, test_loss: 23.585272\n",
      "epoch: 20400, train_loss: 3.531290, test_loss: 23.552141\n",
      "epoch: 20500, train_loss: 3.591653, test_loss: 23.546875\n",
      "epoch: 20600, train_loss: 3.539309, test_loss: 23.477299 (Saved)\n",
      "epoch: 20700, train_loss: 3.526614, test_loss: 23.801350\n",
      "epoch: 20800, train_loss: 3.489620, test_loss: 23.574516\n",
      "epoch: 20900, train_loss: 3.592495, test_loss: 23.655312\n",
      "epoch: 21000, train_loss: 3.606003, test_loss: 23.383078 (Saved)\n",
      "epoch: 21100, train_loss: 3.561898, test_loss: 23.593245\n",
      "epoch: 21200, train_loss: 3.516748, test_loss: 23.388725\n",
      "epoch: 21300, train_loss: 3.544161, test_loss: 23.461153\n",
      "epoch: 21400, train_loss: 3.566235, test_loss: 23.423399\n",
      "epoch: 21500, train_loss: 3.447991, test_loss: 23.376001 (Saved)\n",
      "epoch: 21600, train_loss: 3.543660, test_loss: 23.279364 (Saved)\n",
      "epoch: 21700, train_loss: 3.508420, test_loss: 23.273409 (Saved)\n",
      "epoch: 21800, train_loss: 3.483307, test_loss: 23.434343\n",
      "epoch: 21900, train_loss: 3.508217, test_loss: 23.314720\n",
      "epoch: 22000, train_loss: 3.462899, test_loss: 23.159555 (Saved)\n",
      "epoch: 22100, train_loss: 3.512577, test_loss: 23.300011\n",
      "epoch: 22200, train_loss: 3.449629, test_loss: 23.246441\n",
      "epoch: 22300, train_loss: 3.459001, test_loss: 23.372723\n",
      "epoch: 22400, train_loss: 3.437482, test_loss: 23.396254\n",
      "epoch: 22500, train_loss: 3.497484, test_loss: 23.296242\n",
      "epoch: 22600, train_loss: 3.365293, test_loss: 23.304464\n",
      "epoch: 22700, train_loss: 3.402390, test_loss: 23.469376\n",
      "epoch: 22800, train_loss: 3.413032, test_loss: 23.434855\n",
      "epoch: 22900, train_loss: 3.413577, test_loss: 23.421272\n",
      "epoch: 23000, train_loss: 3.421764, test_loss: 23.457125\n",
      "epoch: 23100, train_loss: 3.389127, test_loss: 23.400303\n",
      "epoch: 23200, train_loss: 3.367703, test_loss: 23.321888\n",
      "epoch: 23300, train_loss: 3.365787, test_loss: 23.279100\n",
      "epoch: 23400, train_loss: 3.319789, test_loss: 23.277187\n",
      "epoch: 23500, train_loss: 3.301055, test_loss: 23.254963\n",
      "epoch: 23600, train_loss: 3.327212, test_loss: 23.143526 (Saved)\n",
      "epoch: 23700, train_loss: 3.273158, test_loss: 23.089819 (Saved)\n",
      "epoch: 23800, train_loss: 3.291812, test_loss: 23.105473\n",
      "epoch: 23900, train_loss: 3.328974, test_loss: 23.087008 (Saved)\n",
      "epoch: 24000, train_loss: 3.253036, test_loss: 23.057571 (Saved)\n",
      "epoch: 24100, train_loss: 3.256843, test_loss: 23.017733 (Saved)\n",
      "epoch: 24200, train_loss: 3.251686, test_loss: 22.976641 (Saved)\n",
      "epoch: 24300, train_loss: 3.198826, test_loss: 22.827393 (Saved)\n",
      "epoch: 24400, train_loss: 3.214734, test_loss: 22.860758\n",
      "epoch: 24500, train_loss: 3.257420, test_loss: 22.806747 (Saved)\n",
      "epoch: 24600, train_loss: 3.191484, test_loss: 22.656326 (Saved)\n",
      "epoch: 24700, train_loss: 3.266217, test_loss: 22.614397 (Saved)\n",
      "epoch: 24800, train_loss: 3.168309, test_loss: 22.604456 (Saved)\n",
      "epoch: 24900, train_loss: 3.174445, test_loss: 22.516090 (Saved)\n",
      "epoch: 25000, train_loss: 3.151867, test_loss: 22.418238 (Saved)\n",
      "epoch: 25100, train_loss: 3.150363, test_loss: 22.434942\n",
      "epoch: 25200, train_loss: 3.096211, test_loss: 22.413513 (Saved)\n",
      "epoch: 25300, train_loss: 3.099285, test_loss: 22.410812 (Saved)\n",
      "epoch: 25400, train_loss: 3.187361, test_loss: 22.460669\n",
      "epoch: 25500, train_loss: 3.058537, test_loss: 22.423132\n",
      "epoch: 25600, train_loss: 3.140884, test_loss: 22.371222 (Saved)\n",
      "epoch: 25700, train_loss: 3.085848, test_loss: 22.289728 (Saved)\n",
      "epoch: 25800, train_loss: 3.046505, test_loss: 22.337025\n",
      "epoch: 25900, train_loss: 3.054682, test_loss: 22.334154\n",
      "epoch: 26000, train_loss: 3.076082, test_loss: 22.309208\n",
      "epoch: 26100, train_loss: 3.046748, test_loss: 22.277794 (Saved)\n",
      "epoch: 26200, train_loss: 3.008786, test_loss: 22.383213\n",
      "epoch: 26300, train_loss: 3.051861, test_loss: 22.416449\n",
      "epoch: 26400, train_loss: 2.963955, test_loss: 22.291109\n",
      "epoch: 26500, train_loss: 3.043681, test_loss: 22.285578\n",
      "epoch: 26600, train_loss: 2.953897, test_loss: 22.302198\n",
      "epoch: 26700, train_loss: 2.976628, test_loss: 22.271856 (Saved)\n",
      "epoch: 26800, train_loss: 2.949735, test_loss: 22.271307 (Saved)\n",
      "epoch: 26900, train_loss: 3.010777, test_loss: 22.170853 (Saved)\n",
      "epoch: 27000, train_loss: 2.953905, test_loss: 22.186583\n",
      "epoch: 27100, train_loss: 2.948534, test_loss: 22.156773 (Saved)\n",
      "epoch: 27200, train_loss: 2.903711, test_loss: 22.057373 (Saved)\n",
      "epoch: 27300, train_loss: 2.923510, test_loss: 22.169092\n",
      "epoch: 27400, train_loss: 2.925910, test_loss: 22.094334\n",
      "epoch: 27500, train_loss: 2.960405, test_loss: 22.053228 (Saved)\n",
      "epoch: 27600, train_loss: 2.934371, test_loss: 22.051140 (Saved)\n",
      "epoch: 27700, train_loss: 2.915189, test_loss: 21.975779 (Saved)\n",
      "epoch: 27800, train_loss: 2.916786, test_loss: 21.948927 (Saved)\n",
      "epoch: 27900, train_loss: 2.932347, test_loss: 22.003174\n",
      "epoch: 28000, train_loss: 2.830395, test_loss: 21.936060 (Saved)\n",
      "epoch: 28100, train_loss: 2.831447, test_loss: 22.039463\n",
      "epoch: 28200, train_loss: 2.802238, test_loss: 21.980173\n",
      "epoch: 28300, train_loss: 2.877484, test_loss: 21.877478 (Saved)\n",
      "epoch: 28400, train_loss: 2.867716, test_loss: 21.996014\n",
      "epoch: 28500, train_loss: 2.820959, test_loss: 21.880386\n",
      "epoch: 28600, train_loss: 2.833596, test_loss: 21.802130 (Saved)\n",
      "epoch: 28700, train_loss: 2.836682, test_loss: 21.716763 (Saved)\n",
      "epoch: 28800, train_loss: 2.960992, test_loss: 21.916792\n",
      "epoch: 28900, train_loss: 2.774318, test_loss: 21.953773\n",
      "epoch: 29000, train_loss: 2.809997, test_loss: 21.761803\n",
      "epoch: 29100, train_loss: 2.797456, test_loss: 21.654228 (Saved)\n",
      "epoch: 29200, train_loss: 2.772173, test_loss: 21.756296\n",
      "epoch: 29300, train_loss: 2.838782, test_loss: 21.779001\n",
      "epoch: 29400, train_loss: 2.883599, test_loss: 21.622280 (Saved)\n",
      "epoch: 29500, train_loss: 2.759495, test_loss: 21.791912\n",
      "epoch: 29600, train_loss: 2.722447, test_loss: 21.548256 (Saved)\n",
      "epoch: 29700, train_loss: 2.723519, test_loss: 21.789280\n",
      "epoch: 29800, train_loss: 2.770758, test_loss: 21.739548\n",
      "epoch: 29900, train_loss: 2.792650, test_loss: 21.675188\n",
      "epoch: 30000, train_loss: 2.790470, test_loss: 21.780710\n",
      "epoch: 30100, train_loss: 2.731390, test_loss: 21.687056\n",
      "epoch: 30200, train_loss: 2.774096, test_loss: 21.824217\n",
      "epoch: 30300, train_loss: 2.740912, test_loss: 21.834188\n",
      "epoch: 30400, train_loss: 2.753706, test_loss: 21.739279\n",
      "epoch: 30500, train_loss: 2.718984, test_loss: 21.802711\n",
      "epoch: 30600, train_loss: 2.779580, test_loss: 22.129412\n",
      "epoch: 30700, train_loss: 2.738893, test_loss: 21.845665\n",
      "epoch: 30800, train_loss: 2.737489, test_loss: 21.808985\n",
      "epoch: 30900, train_loss: 2.725013, test_loss: 21.884829\n",
      "epoch: 31000, train_loss: 2.666201, test_loss: 22.063591\n",
      "epoch: 31100, train_loss: 2.706551, test_loss: 21.837406\n",
      "epoch: 31200, train_loss: 2.721103, test_loss: 21.976982\n",
      "epoch: 31300, train_loss: 2.691270, test_loss: 21.962330\n",
      "epoch: 31400, train_loss: 2.678700, test_loss: 21.924316\n",
      "epoch: 31500, train_loss: 2.690573, test_loss: 22.145226\n",
      "epoch: 31600, train_loss: 2.638216, test_loss: 21.959721\n",
      "epoch: 31700, train_loss: 2.651624, test_loss: 22.034725\n",
      "epoch: 31800, train_loss: 2.641191, test_loss: 21.948748\n",
      "epoch: 31900, train_loss: 2.747430, test_loss: 22.132429\n",
      "epoch: 32000, train_loss: 2.633592, test_loss: 21.999561\n",
      "epoch: 32100, train_loss: 2.635681, test_loss: 22.155201\n",
      "epoch: 32200, train_loss: 2.633084, test_loss: 22.149851\n",
      "epoch: 32300, train_loss: 2.593101, test_loss: 22.037392\n",
      "epoch: 32400, train_loss: 2.587447, test_loss: 22.147617\n",
      "epoch: 32500, train_loss: 2.596463, test_loss: 22.193485\n",
      "epoch: 32600, train_loss: 2.575891, test_loss: 22.010181\n",
      "epoch: 32700, train_loss: 2.636331, test_loss: 22.152205\n",
      "epoch: 32800, train_loss: 2.601543, test_loss: 22.180466\n",
      "epoch: 32900, train_loss: 2.559823, test_loss: 22.298180\n",
      "epoch: 33000, train_loss: 2.548644, test_loss: 22.348173\n",
      "epoch: 33100, train_loss: 2.550184, test_loss: 22.232031\n",
      "epoch: 33200, train_loss: 2.541210, test_loss: 22.466402\n",
      "epoch: 33300, train_loss: 2.587951, test_loss: 22.279549\n",
      "epoch: 33400, train_loss: 2.619462, test_loss: 22.271627\n",
      "epoch: 33500, train_loss: 2.667192, test_loss: 22.466492\n",
      "epoch: 33600, train_loss: 2.575073, test_loss: 22.426085\n",
      "epoch: 33700, train_loss: 2.489734, test_loss: 22.424080\n",
      "epoch: 33800, train_loss: 2.572037, test_loss: 22.419767\n",
      "epoch: 33900, train_loss: 2.507816, test_loss: 22.416447\n",
      "epoch: 34000, train_loss: 2.498082, test_loss: 22.369642\n",
      "epoch: 34100, train_loss: 2.559701, test_loss: 22.333773\n",
      "epoch: 34200, train_loss: 2.546098, test_loss: 22.231272\n",
      "epoch: 34300, train_loss: 2.491631, test_loss: 22.402958\n",
      "epoch: 34400, train_loss: 2.512031, test_loss: 22.486965\n",
      "epoch: 34500, train_loss: 2.466991, test_loss: 22.415531\n",
      "epoch: 34600, train_loss: 2.550220, test_loss: 22.342222\n",
      "epoch: 34700, train_loss: 2.484900, test_loss: 22.353930\n",
      "epoch: 34800, train_loss: 2.449571, test_loss: 22.508480\n",
      "epoch: 34900, train_loss: 2.474192, test_loss: 22.491993\n",
      "epoch: 35000, train_loss: 2.454992, test_loss: 22.610121\n",
      "epoch: 35100, train_loss: 2.464048, test_loss: 22.547771\n",
      "epoch: 35200, train_loss: 2.423082, test_loss: 22.561438\n",
      "epoch: 35300, train_loss: 2.420981, test_loss: 22.411020\n",
      "epoch: 35400, train_loss: 2.488988, test_loss: 22.507597\n",
      "epoch: 35500, train_loss: 2.455514, test_loss: 22.562803\n",
      "epoch: 35600, train_loss: 2.488434, test_loss: 22.691563\n",
      "epoch: 35700, train_loss: 2.511481, test_loss: 22.684956\n",
      "epoch: 35800, train_loss: 2.470187, test_loss: 22.679752\n",
      "epoch: 35900, train_loss: 2.414919, test_loss: 22.584803\n",
      "epoch: 36000, train_loss: 2.493952, test_loss: 22.733829\n",
      "epoch: 36100, train_loss: 2.396337, test_loss: 22.625944\n",
      "epoch: 36200, train_loss: 2.463135, test_loss: 22.797920\n",
      "epoch: 36300, train_loss: 2.400608, test_loss: 22.671858\n",
      "epoch: 36400, train_loss: 2.398646, test_loss: 22.675869\n",
      "epoch: 36500, train_loss: 2.442584, test_loss: 22.642843\n",
      "epoch: 36600, train_loss: 2.384140, test_loss: 22.857119\n",
      "epoch: 36700, train_loss: 2.430890, test_loss: 22.895645\n",
      "epoch: 36800, train_loss: 2.369941, test_loss: 22.920919\n",
      "epoch: 36900, train_loss: 2.395522, test_loss: 22.808699\n",
      "epoch: 37000, train_loss: 2.393079, test_loss: 22.967588\n",
      "epoch: 37100, train_loss: 2.417415, test_loss: 22.755110\n",
      "epoch: 37200, train_loss: 2.462167, test_loss: 22.727552\n",
      "epoch: 37300, train_loss: 2.404788, test_loss: 22.965597\n",
      "epoch: 37400, train_loss: 2.352559, test_loss: 22.942867\n",
      "epoch: 37500, train_loss: 2.392423, test_loss: 22.888668\n",
      "epoch: 37600, train_loss: 2.351178, test_loss: 22.790462\n",
      "epoch: 37700, train_loss: 2.353581, test_loss: 22.950373\n",
      "epoch: 37800, train_loss: 2.337466, test_loss: 22.919945\n",
      "epoch: 37900, train_loss: 2.349644, test_loss: 22.878975\n",
      "epoch: 38000, train_loss: 2.368482, test_loss: 23.042381\n",
      "epoch: 38100, train_loss: 2.310324, test_loss: 22.882982\n",
      "epoch: 38200, train_loss: 2.319550, test_loss: 22.840256\n",
      "epoch: 38300, train_loss: 2.368565, test_loss: 22.844589\n",
      "epoch: 38400, train_loss: 2.340197, test_loss: 23.155533\n",
      "epoch: 38500, train_loss: 2.349920, test_loss: 23.047779\n",
      "epoch: 38600, train_loss: 2.317544, test_loss: 22.893547\n",
      "epoch: 38700, train_loss: 2.347534, test_loss: 22.810509\n",
      "epoch: 38800, train_loss: 2.374007, test_loss: 22.845901\n",
      "epoch: 38900, train_loss: 2.338057, test_loss: 22.940590\n",
      "epoch: 39000, train_loss: 2.256423, test_loss: 22.888271\n",
      "epoch: 39100, train_loss: 2.277496, test_loss: 22.931381\n",
      "epoch: 39200, train_loss: 2.362518, test_loss: 22.917027\n",
      "epoch: 39300, train_loss: 2.314578, test_loss: 22.998997\n",
      "epoch: 39400, train_loss: 2.301271, test_loss: 23.071020\n",
      "epoch: 39500, train_loss: 2.247818, test_loss: 23.067322\n",
      "epoch: 39600, train_loss: 2.294382, test_loss: 22.990822\n",
      "epoch: 39700, train_loss: 2.343079, test_loss: 23.051651\n",
      "epoch: 39800, train_loss: 2.271565, test_loss: 23.116632\n",
      "epoch: 39900, train_loss: 2.303775, test_loss: 23.165569\n",
      "epoch: 40000, train_loss: 2.259609, test_loss: 23.107204\n",
      "epoch: 40100, train_loss: 2.388294, test_loss: 22.970783\n",
      "epoch: 40200, train_loss: 2.344874, test_loss: 23.044989\n",
      "epoch: 40300, train_loss: 2.380101, test_loss: 22.870953\n",
      "epoch: 40400, train_loss: 2.296886, test_loss: 23.108778\n",
      "epoch: 40500, train_loss: 2.263070, test_loss: 23.066124\n",
      "epoch: 40600, train_loss: 2.355814, test_loss: 23.360338\n",
      "epoch: 40700, train_loss: 2.271740, test_loss: 23.098482\n",
      "epoch: 40800, train_loss: 2.316194, test_loss: 23.021568\n",
      "epoch: 40900, train_loss: 2.230481, test_loss: 23.080261\n",
      "epoch: 41000, train_loss: 2.273662, test_loss: 23.342146\n",
      "epoch: 41100, train_loss: 2.301704, test_loss: 23.224310\n",
      "epoch: 41200, train_loss: 2.240865, test_loss: 23.140175\n",
      "epoch: 41300, train_loss: 2.263209, test_loss: 23.216599\n",
      "epoch: 41400, train_loss: 2.290322, test_loss: 23.208277\n",
      "epoch: 41500, train_loss: 2.304127, test_loss: 23.052538\n",
      "epoch: 41600, train_loss: 2.247720, test_loss: 23.061804\n",
      "epoch: 41700, train_loss: 2.275388, test_loss: 23.029072\n",
      "epoch: 41800, train_loss: 2.234601, test_loss: 23.163439\n",
      "epoch: 41900, train_loss: 2.257696, test_loss: 23.177605\n",
      "epoch: 42000, train_loss: 2.233313, test_loss: 23.210390\n",
      "epoch: 42100, train_loss: 2.236029, test_loss: 22.966202\n",
      "epoch: 42200, train_loss: 2.207897, test_loss: 23.040020\n",
      "epoch: 42300, train_loss: 2.262916, test_loss: 23.105343\n",
      "epoch: 42400, train_loss: 2.205762, test_loss: 23.133570\n",
      "epoch: 42500, train_loss: 2.275484, test_loss: 23.058628\n",
      "epoch: 42600, train_loss: 2.223509, test_loss: 23.085632\n",
      "epoch: 42700, train_loss: 2.281499, test_loss: 23.017466\n",
      "epoch: 42800, train_loss: 2.211139, test_loss: 23.217777\n",
      "epoch: 42900, train_loss: 2.231384, test_loss: 23.045700\n",
      "epoch: 43000, train_loss: 2.260740, test_loss: 23.018024\n",
      "epoch: 43100, train_loss: 2.257656, test_loss: 22.950954\n",
      "epoch: 43200, train_loss: 2.219445, test_loss: 23.007750\n",
      "epoch: 43300, train_loss: 2.175740, test_loss: 23.022476\n",
      "epoch: 43400, train_loss: 2.215212, test_loss: 22.975872\n",
      "epoch: 43500, train_loss: 2.254153, test_loss: 23.047325\n",
      "epoch: 43600, train_loss: 2.166414, test_loss: 22.905828\n",
      "epoch: 43700, train_loss: 2.235773, test_loss: 22.818663\n",
      "epoch: 43800, train_loss: 2.166668, test_loss: 22.974873\n",
      "epoch: 43900, train_loss: 2.193415, test_loss: 22.960348\n",
      "epoch: 44000, train_loss: 2.213745, test_loss: 22.925325\n",
      "epoch: 44100, train_loss: 2.195844, test_loss: 22.944498\n",
      "epoch: 44200, train_loss: 2.184723, test_loss: 23.000387\n",
      "epoch: 44300, train_loss: 2.168643, test_loss: 22.993208\n",
      "epoch: 44400, train_loss: 2.212449, test_loss: 22.891502\n",
      "epoch: 44500, train_loss: 2.216773, test_loss: 22.864046\n",
      "epoch: 44600, train_loss: 2.204057, test_loss: 23.000126\n",
      "epoch: 44700, train_loss: 2.170481, test_loss: 23.228573\n",
      "epoch: 44800, train_loss: 2.210589, test_loss: 22.948236\n",
      "epoch: 44900, train_loss: 2.171762, test_loss: 22.997797\n",
      "epoch: 45000, train_loss: 2.145660, test_loss: 22.999214\n",
      "epoch: 45100, train_loss: 2.205179, test_loss: 23.127375\n",
      "epoch: 45200, train_loss: 2.116166, test_loss: 22.950912\n",
      "epoch: 45300, train_loss: 2.181440, test_loss: 22.905375\n",
      "epoch: 45400, train_loss: 2.136269, test_loss: 22.918495\n",
      "epoch: 45500, train_loss: 2.182978, test_loss: 22.908907\n",
      "epoch: 45600, train_loss: 2.153939, test_loss: 23.138718\n",
      "epoch: 45700, train_loss: 2.165910, test_loss: 22.935017\n",
      "epoch: 45800, train_loss: 2.165629, test_loss: 23.005112\n",
      "epoch: 45900, train_loss: 2.158735, test_loss: 23.110315\n",
      "epoch: 46000, train_loss: 2.176716, test_loss: 22.996649\n",
      "epoch: 46100, train_loss: 2.107920, test_loss: 23.190580\n",
      "epoch: 46200, train_loss: 2.147069, test_loss: 23.000401\n",
      "epoch: 46300, train_loss: 2.177915, test_loss: 22.976955\n",
      "epoch: 46400, train_loss: 2.185513, test_loss: 23.025509\n",
      "epoch: 46500, train_loss: 2.189911, test_loss: 23.256649\n",
      "epoch: 46600, train_loss: 2.146132, test_loss: 22.895758\n",
      "epoch: 46700, train_loss: 2.129053, test_loss: 22.898075\n",
      "epoch: 46800, train_loss: 2.205287, test_loss: 22.985239\n",
      "epoch: 46900, train_loss: 2.182138, test_loss: 22.851889\n",
      "epoch: 47000, train_loss: 2.148238, test_loss: 22.976427\n",
      "epoch: 47100, train_loss: 2.134934, test_loss: 23.167473\n",
      "epoch: 47200, train_loss: 2.162853, test_loss: 23.186085\n",
      "epoch: 47300, train_loss: 2.159880, test_loss: 22.995411\n",
      "epoch: 47400, train_loss: 2.116526, test_loss: 23.071537\n",
      "epoch: 47500, train_loss: 2.133909, test_loss: 22.850071\n",
      "epoch: 47600, train_loss: 2.203609, test_loss: 22.909962\n",
      "epoch: 47700, train_loss: 2.059680, test_loss: 23.088381\n",
      "epoch: 47800, train_loss: 2.067322, test_loss: 23.035610\n",
      "epoch: 47900, train_loss: 2.078231, test_loss: 23.018032\n",
      "epoch: 48000, train_loss: 2.090230, test_loss: 22.973064\n",
      "epoch: 48100, train_loss: 2.088393, test_loss: 23.074038\n",
      "epoch: 48200, train_loss: 2.068156, test_loss: 23.023191\n",
      "epoch: 48300, train_loss: 2.110105, test_loss: 23.074772\n",
      "epoch: 48400, train_loss: 2.113215, test_loss: 23.097048\n",
      "epoch: 48500, train_loss: 2.133556, test_loss: 23.027956\n",
      "epoch: 48600, train_loss: 2.187674, test_loss: 22.943447\n",
      "epoch: 48700, train_loss: 2.121576, test_loss: 23.001238\n",
      "epoch: 48800, train_loss: 2.179200, test_loss: 23.066643\n",
      "epoch: 48900, train_loss: 2.039140, test_loss: 23.189575\n",
      "epoch: 49000, train_loss: 2.088953, test_loss: 23.100084\n",
      "epoch: 49100, train_loss: 2.088693, test_loss: 23.180799\n",
      "epoch: 49200, train_loss: 2.058122, test_loss: 22.985334\n",
      "epoch: 49300, train_loss: 2.082852, test_loss: 23.212666\n",
      "epoch: 49400, train_loss: 2.071111, test_loss: 23.010906\n",
      "epoch: 49500, train_loss: 2.117286, test_loss: 23.047647\n",
      "epoch: 49600, train_loss: 2.049733, test_loss: 23.067480\n",
      "epoch: 49700, train_loss: 2.043296, test_loss: 23.193518\n",
      "epoch: 49800, train_loss: 2.045363, test_loss: 23.040115\n",
      "epoch: 49900, train_loss: 2.023098, test_loss: 23.006525\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Saving ONNX model: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Exported graph: graph(%onnx::Sub_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(4, 2, strides=[2, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(20, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(80, 20, strides=[20, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(80, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.weight : Float(20, 80, strides=[80, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.weight : Float(4, 20, strides=[20, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.weight : Float(2, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/Constant_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 0.0000 -665.6403 [ CUDAFloatType{2} ], onnx_name=\"/Constant\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:16:0\n",
      "  %/Sub_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Sub[onnx_name=\"/Sub\"](%onnx::Sub_0, %/Constant_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:16:0\n",
      "  %/Constant_1_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1200.0000  1514.1685 [ CUDAFloatType{2} ], onnx_name=\"/Constant_1\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:16:0\n",
      "  %/Div_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Div[onnx_name=\"/Div\"](%/Sub_output_0, %/Constant_1_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:16:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Div_output_0, %fc1.weight, %fc1.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc1 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh\"](%/fc1/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1971:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Tanh_output_0, %fc2.weight, %fc2.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc2 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_1_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_1\"](%/fc2/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1971:0\n",
      "  %/fc3/Gemm_output_0 : Float(1, 80, strides=[80, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Tanh_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc3 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_output_0 : Float(1, 80, strides=[80, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/Relu\"](%/fc3/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/fc4/Gemm_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc4/Gemm\"](%/Relu_output_0, %fc4.weight, %fc4.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc4 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_1_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/Relu_1\"](%/fc4/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/fc5/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc5/Gemm\"](%/Relu_1_output_0, %fc5.weight, %fc5.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc5 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc6/Gemm_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc6/Gemm\"](%/fc5/Gemm_output_0, %fc6.weight, %fc6.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc6 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_2_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1236   488 [ CUDAFloatType{2} ], onnx_name=\"/Constant_2\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:23:0\n",
      "  %/Mul_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Mul[onnx_name=\"/Mul\"](%/fc6/Gemm_output_0, %/Constant_2_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:23:0\n",
      "  %/Constant_3_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value=  36  204 [ CUDAFloatType{2} ], onnx_name=\"/Constant_3\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:23:0\n",
      "  %30 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/Add\"](%/Mul_output_0, %/Constant_3_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_41174/1725647797.py:23:0\n",
      "  return (%30)\n",
      "\n",
      "Saved ONNX model: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'cm2pixel_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 474.3416,  158.1139],\n",
      "        [ 200.0000,  300.0000],\n",
      "        [ 789.1151, -131.5192],\n",
      "        [ 450.0000,  200.0000],\n",
      "        [ 200.0000, -250.0000],\n",
      "        [ 447.2136,  223.6068],\n",
      "        [ 450.0000,  300.0000],\n",
      "        [ 100.0000,  -25.0000],\n",
      "        [  25.0000,    0.0000],\n",
      "        [  50.0000,  100.0000],\n",
      "        [ 100.0000,  -75.0000],\n",
      "        [ 848.5281, -848.5281],\n",
      "        [ 100.0000, -125.0000],\n",
      "        [ 450.0000, -300.0000],\n",
      "        [ 500.0000,    0.0000],\n",
      "        [  75.0000, -100.0000],\n",
      "        [ 350.0000,  250.0000],\n",
      "        [ 250.0000,  200.0000],\n",
      "        [ 150.0000, -250.0000],\n",
      "        [  75.0000,  -50.0000],\n",
      "        [  25.0000,   50.0000],\n",
      "        [  50.0000,  -50.0000],\n",
      "        [   0.0000,    0.0000],\n",
      "        [ 100.0000,  200.0000],\n",
      "        [ 350.0000,  150.0000],\n",
      "        [ 250.0000,  250.0000],\n",
      "        [  50.0000,  -25.0000],\n",
      "        [ 100.0000,  -50.0000],\n",
      "        [ 350.0000, -200.0000]], device='cuda:0')\n",
      "y\n",
      "tensor([[ 464.,  270.],\n",
      "        [  30.,  362.],\n",
      "        [ 736.,  224.],\n",
      "        [ 410.,  270.],\n",
      "        [1164.,  342.],\n",
      "        [ 378.,  274.],\n",
      "        [ 298.,  272.],\n",
      "        [ 716.,  442.],\n",
      "        [ 640.,  598.],\n",
      "        [ 252.,  532.],\n",
      "        [ 872.,  442.],\n",
      "        [1220.,  214.],\n",
      "        [1024.,  432.],\n",
      "        [ 990.,  260.],\n",
      "        [ 638.,  258.],\n",
      "        [ 984.,  474.],\n",
      "        [ 292.,  298.],\n",
      "        [ 286.,  332.],\n",
      "        [1268.,  376.],\n",
      "        [ 816.,  478.],\n",
      "        [ 412.,  598.],\n",
      "        [ 838.,  528.],\n",
      "        [ 642.,  690.],\n",
      "        [  46.,  452.],\n",
      "        [ 432.,  294.],\n",
      "        [ 204.,  336.],\n",
      "        [ 740.,  530.],\n",
      "        [ 794.,  440.],\n",
      "        [ 924.,  284.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[ 464.3090,  265.6960],\n",
      "        [  28.4942,  362.7834],\n",
      "        [ 720.7133,  225.2882],\n",
      "        [ 404.7933,  270.7776],\n",
      "        [1170.0146,  343.7982],\n",
      "        [ 377.8727,  272.1961],\n",
      "        [ 298.1281,  275.1052],\n",
      "        [ 720.2849,  442.7612],\n",
      "        [ 644.1127,  599.6411],\n",
      "        [ 259.8332,  533.2324],\n",
      "        [ 870.9009,  439.1331],\n",
      "        [1235.4231,  211.2980],\n",
      "        [1023.3771,  436.4967],\n",
      "        [ 991.3395,  263.2150],\n",
      "        [ 641.7874,  258.8061],\n",
      "        [ 988.1372,  474.3725],\n",
      "        [ 293.6062,  297.3988],\n",
      "        [ 292.2640,  330.5732],\n",
      "        [1272.6041,  377.2126],\n",
      "        [ 807.1605,  478.9154],\n",
      "        [ 427.6544,  598.8593],\n",
      "        [ 835.6615,  528.2292],\n",
      "        [ 639.0104,  691.2290],\n",
      "        [  43.3898,  452.2180],\n",
      "        [ 424.9987,  295.1471],\n",
      "        [ 197.8286,  331.5218],\n",
      "        [ 733.2286,  531.9705],\n",
      "        [ 796.4650,  440.9869],\n",
      "        [ 923.9793,  284.8123]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[ -0.3090,   4.3040],\n",
      "        [  1.5058,  -0.7834],\n",
      "        [ 15.2867,  -1.2882],\n",
      "        [  5.2067,  -0.7776],\n",
      "        [ -6.0146,  -1.7982],\n",
      "        [  0.1273,   1.8039],\n",
      "        [ -0.1281,  -3.1052],\n",
      "        [ -4.2849,  -0.7612],\n",
      "        [ -4.1127,  -1.6411],\n",
      "        [ -7.8332,  -1.2324],\n",
      "        [  1.0991,   2.8669],\n",
      "        [-15.4231,   2.7020],\n",
      "        [  0.6229,  -4.4967],\n",
      "        [ -1.3395,  -3.2150],\n",
      "        [ -3.7874,  -0.8061],\n",
      "        [ -4.1372,  -0.3725],\n",
      "        [ -1.6062,   0.6012],\n",
      "        [ -6.2640,   1.4268],\n",
      "        [ -4.6041,  -1.2126],\n",
      "        [  8.8395,  -0.9154],\n",
      "        [-15.6544,  -0.8593],\n",
      "        [  2.3385,  -0.2292],\n",
      "        [  2.9896,  -1.2290],\n",
      "        [  2.6102,  -0.2180],\n",
      "        [  7.0013,  -1.1471],\n",
      "        [  6.1714,   4.4782],\n",
      "        [  6.7714,  -1.9705],\n",
      "        [ -2.4650,  -0.9869],\n",
      "        [  0.0207,  -0.8123]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "rmse: 4.803587\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    rmse = torch.sqrt(torch.mean(torch.square(y - y_pred)))\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    print('rmse: {:.6f}'.format(rmse))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pixel_x  pixel_y         cm_x        cm_y\n",
      "59       872      260   450.000000 -200.000000\n",
      "144     1132      286   353.553391 -353.553391\n",
      "132      730      260   493.196962  -82.199494\n",
      "141      986      272   416.025147 -277.350098\n",
      "40      1272      342   200.000000 -300.000000\n",
      "60       934      260   450.000000 -250.000000\n",
      "69       444      532    50.000000   50.000000\n",
      "125      338      214  1073.312629  536.656315\n",
      "130      100      256   565.685425  565.685425\n",
      "116      640      204  1200.000000    0.000000\n",
      "77       562      446   100.000000   25.000000\n",
      "75       300      486    75.000000  100.000000\n",
      "101      110      336   250.000000  300.000000\n",
      "56       698      262   450.000000  -50.000000\n",
      "14      1244      520    50.000000 -150.000000\n",
      "57       756      262   450.000000 -100.000000\n",
      "43       822      324   250.000000 -100.000000\n",
      "98       374      332   250.000000  150.000000\n",
      "15       640      482    75.000000    0.000000\n",
      "45      1000      324   250.000000 -200.000000\n",
      "112      354      272   450.000000  250.000000\n",
      "127      270      246   665.640235  443.760157\n",
      "35       746      352   200.000000  -50.000000\n",
      "135      818      262   474.341649 -158.113883\n",
      "31      1014      382   150.000000 -150.000000\n",
      "54      1064      278   350.000000 -300.000000\n",
      "46      1092      318   250.000000 -250.000000\n",
      "70       346      534    50.000000   75.000000\n",
      "6        874      594    25.000000  -50.000000\n",
      "38      1058      344   200.000000 -200.000000\n",
      "72       158      532    50.000000  125.000000\n",
      "19       642      444   100.000000    0.000000\n",
      "137      840      204  1138.419958 -379.473319\n",
      "76       138      488    75.000000  150.000000\n",
      "108      582      268   450.000000   50.000000\n",
      "139      926      230   715.541753 -357.770876\n",
      "134      738      204  1183.672709 -197.278785\n",
      "124      354      236   715.541753  357.770876\n",
      "138      904      266   447.213595 -223.606798\n",
      "103      500      292   350.000000  100.000000\n",
      "88       158      398   150.000000  200.000000\n",
      "23       948      438   100.000000 -100.000000\n",
      "44       910      324   250.000000 -150.000000\n",
      "117      552      264   493.196962   82.199494\n",
      "119      536      206  1183.672709  197.278785\n",
      "94       132      362   200.000000  250.000000\n",
      "63       364      688     0.000000   50.000000\n",
      "1        780      690     0.000000  -25.000000\n",
      "11       938      524    50.000000  -75.000000\n",
      "68       542      532    50.000000   25.000000\n",
      "126      298      284   416.025147  277.350098\n",
      "4       1212      678     0.000000 -100.000000\n",
      "34       642      354   200.000000    0.000000\n",
      "115      640      224   800.000000    0.000000\n",
      "91       434      358   200.000000  100.000000\n",
      "74       472      484    75.000000   50.000000\n",
      "140      940      206  1073.312629 -536.656315\n",
      "105      362      296   350.000000  200.000000\n",
      "128      246      220   998.460353  665.640235\n",
      "97       462      328   250.000000  100.000000\n",
      "129      150      300   353.553391  353.553391\n",
      "145     1180      244   565.685425 -565.685425\n",
      "121      450      232   758.946638  252.982213\n",
      "3       1066      684     0.000000  -75.000000\n",
      "102      570      290   350.000000   50.000000\n",
      "143     1032      208   998.460353 -665.640235\n",
      "49       710      288   350.000000  -50.000000\n",
      "131       64      230   848.528137  848.528137\n",
      "42       730      324   250.000000  -50.000000\n",
      "2        922      688     0.000000  -50.000000\n",
      "136      832      226   758.946638 -252.982213\n",
      "58       816      260   450.000000 -150.000000\n",
      "50       782      286   350.000000 -100.000000\n",
      "29       768      386   150.000000  -50.000000\n",
      "64       226      690     0.000000   75.000000\n",
      "26      1184      432   100.000000 -175.000000\n",
      "36       852      348   200.000000 -100.000000\n",
      "41       642      326   250.000000    0.000000\n",
      "30       888      386   150.000000 -100.000000\n",
      "51       852      286   350.000000 -150.000000\n",
      "90       538      356   200.000000   50.000000\n",
      "62       502      692     0.000000   25.000000\n",
      "48       642      290   350.000000    0.000000\n",
      "47      1188      314   250.000000 -300.000000\n",
      "87       276      398   150.000000  150.000000\n",
      "27      1258      430   100.000000 -200.000000\n",
      "12      1040      522    50.000000 -100.000000\n",
      "73        60      534    50.000000  150.000000\n",
      "67       186      598    25.000000  100.000000\n",
      "55       642      264   450.000000    0.000000\n",
      "13      1144      520    50.000000 -125.000000\n",
      "110      466      270   450.000000  150.000000\n",
      "81       264      452   100.000000  125.000000\n",
      "92       332      358   200.000000  150.000000\n",
      "122      438      210  1138.419958  379.473319\n",
      "142     1014      232   665.640235 -443.760157\n",
      "85       520      394   150.000000   50.000000\n",
      "79       416      446   100.000000   75.000000\n",
      "32      1138      378   150.000000 -200.000000\n",
      "109      524      268   450.000000  100.000000\n",
      "107      222      298   350.000000  300.000000\n",
      "89        36      400   150.000000  250.000000\n",
      "80       340      448   100.000000  100.000000\n",
      "28       642      390   150.000000    0.000000\n",
      "53       996      280   350.000000 -250.000000\n",
      "83       122      452   100.000000  175.000000\n",
      "18      1164      470    75.000000 -150.000000\n",
      "65        90      690     0.000000  100.000000\n",
      "78       488      446   100.000000   50.000000\n",
      "96       552      328   250.000000   50.000000\n",
      "7       1110      584    25.000000 -100.000000\n",
      "93       234      362   200.000000  200.000000\n",
      "86       396      394   150.000000  100.000000\n",
      "37       954      348   200.000000 -150.000000\n",
      "82       194      452   100.000000  150.000000\n",
      "118      542      228   789.115139  131.519190\n",
      "25      1098      434   100.000000 -150.000000\n",
      "8        642      534    50.000000    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Show all training data as table\n",
    "pd.set_option('display.max_rows', len(df_train))\n",
    "print(df_train)\n",
    "# Export training data as CSV file\n",
    "df_train.to_csv(os.path.join(model_directory, 'dataset_train_cm2pixel.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pixel_x  pixel_y        cm_x        cm_y\n",
      "0        642      690    0.000000    0.000000\n",
      "5        640      598   25.000000    0.000000\n",
      "9        740      530   50.000000  -25.000000\n",
      "10       838      528   50.000000  -50.000000\n",
      "16       816      478   75.000000  -50.000000\n",
      "17       984      474   75.000000 -100.000000\n",
      "20       716      442  100.000000  -25.000000\n",
      "21       794      440  100.000000  -50.000000\n",
      "22       872      442  100.000000  -75.000000\n",
      "24      1024      432  100.000000 -125.000000\n",
      "33      1268      376  150.000000 -250.000000\n",
      "39      1164      342  200.000000 -250.000000\n",
      "52       924      284  350.000000 -200.000000\n",
      "61       990      260  450.000000 -300.000000\n",
      "66       412      598   25.000000   50.000000\n",
      "71       252      532   50.000000  100.000000\n",
      "84        46      452  100.000000  200.000000\n",
      "95        30      362  200.000000  300.000000\n",
      "99       286      332  250.000000  200.000000\n",
      "100      204      336  250.000000  250.000000\n",
      "104      432      294  350.000000  150.000000\n",
      "106      292      298  350.000000  250.000000\n",
      "111      410      270  450.000000  200.000000\n",
      "113      298      272  450.000000  300.000000\n",
      "114      638      258  500.000000    0.000000\n",
      "120      464      270  474.341649  158.113883\n",
      "123      378      274  447.213595  223.606798\n",
      "133      736      224  789.115139 -131.519190\n",
      "146     1220      214  848.528137 -848.528137\n"
     ]
    }
   ],
   "source": [
    "# Show all test data as table\n",
    "pd.set_option('display.max_rows', len(df_test))\n",
    "print(df_test)\n",
    "# Export test data as CSV file\n",
    "df_test.to_csv(os.path.join(model_directory, 'dataset_test_cm2pixel.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
