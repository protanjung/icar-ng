{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 06/11/2023 at 22:13:14.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/icar/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/icar/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/icar/icar-ng-data/model\n",
      "Model file: cm2pixel_model.pt\n",
      "Model path: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'cm2pixel_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([   0.0000, -848.5281]), max_x: tensor([1183.6727,  848.5281])\n",
      "min_y: tensor([ 30., 204.]), max_y: tensor([1268.,  692.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 2:4].values\n",
    "        y = self.dataframe.iloc[:, 0:2].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 118\n",
      "dataset_test: 29\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 20)\n",
    "        self.fc3 = nn.Linear(20, 80)\n",
    "        self.fc4 = nn.Linear(80, 20)\n",
    "        self.fc5 = nn.Linear(20, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n",
      "Loaded model: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 134501.785156, test_loss: 55761.058594 (Saved)\n",
      "epoch: 100, train_loss: 35172.310547, test_loss: 13474.715820 (Saved)\n",
      "epoch: 200, train_loss: 8552.485596, test_loss: 3704.319336 (Saved)\n",
      "epoch: 300, train_loss: 3103.734741, test_loss: 1667.260132 (Saved)\n",
      "epoch: 400, train_loss: 1375.375305, test_loss: 800.460388 (Saved)\n",
      "epoch: 500, train_loss: 673.516144, test_loss: 380.012024 (Saved)\n",
      "epoch: 600, train_loss: 382.810699, test_loss: 213.968170 (Saved)\n",
      "epoch: 700, train_loss: 273.191597, test_loss: 150.821182 (Saved)\n",
      "epoch: 800, train_loss: 229.603897, test_loss: 124.082581 (Saved)\n",
      "epoch: 900, train_loss: 203.173988, test_loss: 108.347488 (Saved)\n",
      "epoch: 1000, train_loss: 181.315079, test_loss: 96.464447 (Saved)\n",
      "epoch: 1100, train_loss: 168.681999, test_loss: 86.958275 (Saved)\n",
      "epoch: 1200, train_loss: 153.439651, test_loss: 79.178795 (Saved)\n",
      "epoch: 1300, train_loss: 141.075928, test_loss: 72.659805 (Saved)\n",
      "epoch: 1400, train_loss: 129.835388, test_loss: 67.258720 (Saved)\n",
      "epoch: 1500, train_loss: 121.671261, test_loss: 62.617294 (Saved)\n",
      "epoch: 1600, train_loss: 117.147247, test_loss: 58.827118 (Saved)\n",
      "epoch: 1700, train_loss: 107.567184, test_loss: 55.576027 (Saved)\n",
      "epoch: 1800, train_loss: 99.670963, test_loss: 52.327614 (Saved)\n",
      "epoch: 1900, train_loss: 97.644123, test_loss: 49.424263 (Saved)\n",
      "epoch: 2000, train_loss: 91.601654, test_loss: 46.704754 (Saved)\n",
      "epoch: 2100, train_loss: 86.343071, test_loss: 44.291286 (Saved)\n",
      "epoch: 2200, train_loss: 80.797554, test_loss: 42.479122 (Saved)\n",
      "epoch: 2300, train_loss: 75.856365, test_loss: 40.981033 (Saved)\n",
      "epoch: 2400, train_loss: 74.068035, test_loss: 39.451717 (Saved)\n",
      "epoch: 2500, train_loss: 66.414539, test_loss: 38.448750 (Saved)\n",
      "epoch: 2600, train_loss: 67.056292, test_loss: 37.287663 (Saved)\n",
      "epoch: 2700, train_loss: 60.459745, test_loss: 37.218815 (Saved)\n",
      "epoch: 2800, train_loss: 58.379360, test_loss: 36.831314 (Saved)\n",
      "epoch: 2900, train_loss: 54.127159, test_loss: 36.097134 (Saved)\n",
      "epoch: 3000, train_loss: 53.847660, test_loss: 35.502842 (Saved)\n",
      "epoch: 3100, train_loss: 51.100183, test_loss: 34.558659 (Saved)\n",
      "epoch: 3200, train_loss: 45.756597, test_loss: 33.326973 (Saved)\n",
      "epoch: 3300, train_loss: 44.443731, test_loss: 32.405624 (Saved)\n",
      "epoch: 3400, train_loss: 43.210943, test_loss: 31.579992 (Saved)\n",
      "epoch: 3500, train_loss: 42.461269, test_loss: 30.954721 (Saved)\n",
      "epoch: 3600, train_loss: 38.684099, test_loss: 29.968876 (Saved)\n",
      "epoch: 3700, train_loss: 34.994113, test_loss: 29.089237 (Saved)\n",
      "epoch: 3800, train_loss: 33.023694, test_loss: 28.426548 (Saved)\n",
      "epoch: 3900, train_loss: 32.789228, test_loss: 28.351315 (Saved)\n",
      "epoch: 4000, train_loss: 31.609419, test_loss: 27.777184 (Saved)\n",
      "epoch: 4100, train_loss: 30.696294, test_loss: 27.050768 (Saved)\n",
      "epoch: 4200, train_loss: 29.685591, test_loss: 26.555340 (Saved)\n",
      "epoch: 4300, train_loss: 28.322266, test_loss: 26.809055\n",
      "epoch: 4400, train_loss: 27.050311, test_loss: 26.582142\n",
      "epoch: 4500, train_loss: 26.385555, test_loss: 26.233601 (Saved)\n",
      "epoch: 4600, train_loss: 26.067366, test_loss: 25.950768 (Saved)\n",
      "epoch: 4700, train_loss: 25.100931, test_loss: 26.064550\n",
      "epoch: 4800, train_loss: 23.995461, test_loss: 25.724611 (Saved)\n",
      "epoch: 4900, train_loss: 23.780540, test_loss: 25.746962\n",
      "epoch: 5000, train_loss: 22.865798, test_loss: 25.634800 (Saved)\n",
      "epoch: 5100, train_loss: 22.566502, test_loss: 25.535833 (Saved)\n",
      "epoch: 5200, train_loss: 21.865561, test_loss: 25.676043\n",
      "epoch: 5300, train_loss: 21.171646, test_loss: 25.498713 (Saved)\n",
      "epoch: 5400, train_loss: 21.217495, test_loss: 25.539076\n",
      "epoch: 5500, train_loss: 20.885707, test_loss: 25.594013\n",
      "epoch: 5600, train_loss: 20.591706, test_loss: 25.457014 (Saved)\n",
      "epoch: 5700, train_loss: 19.892912, test_loss: 25.441689 (Saved)\n",
      "epoch: 5800, train_loss: 19.851984, test_loss: 25.704035\n",
      "epoch: 5900, train_loss: 19.357556, test_loss: 25.574131\n",
      "epoch: 6000, train_loss: 19.736846, test_loss: 25.791267\n",
      "epoch: 6100, train_loss: 18.813291, test_loss: 25.619625\n",
      "epoch: 6200, train_loss: 18.658766, test_loss: 25.338303 (Saved)\n",
      "epoch: 6300, train_loss: 18.828902, test_loss: 25.461088\n",
      "epoch: 6400, train_loss: 18.661547, test_loss: 25.460222\n",
      "epoch: 6500, train_loss: 18.253619, test_loss: 25.285116 (Saved)\n",
      "epoch: 6600, train_loss: 17.796403, test_loss: 25.178066 (Saved)\n",
      "epoch: 6700, train_loss: 17.035260, test_loss: 25.107510 (Saved)\n",
      "epoch: 6800, train_loss: 17.736450, test_loss: 25.088358 (Saved)\n",
      "epoch: 6900, train_loss: 17.331542, test_loss: 24.720951 (Saved)\n",
      "epoch: 7000, train_loss: 17.048198, test_loss: 24.810854\n",
      "epoch: 7100, train_loss: 16.725173, test_loss: 24.634682 (Saved)\n",
      "epoch: 7200, train_loss: 16.658086, test_loss: 24.557308 (Saved)\n",
      "epoch: 7300, train_loss: 16.937920, test_loss: 24.719637\n",
      "epoch: 7400, train_loss: 16.722419, test_loss: 24.162165 (Saved)\n",
      "epoch: 7500, train_loss: 16.276112, test_loss: 24.058439 (Saved)\n",
      "epoch: 7600, train_loss: 16.399061, test_loss: 24.188820\n",
      "epoch: 7700, train_loss: 16.261507, test_loss: 24.274103\n",
      "epoch: 7800, train_loss: 16.159387, test_loss: 24.159298\n",
      "epoch: 7900, train_loss: 16.206782, test_loss: 24.032408 (Saved)\n",
      "epoch: 8000, train_loss: 15.887484, test_loss: 23.704241 (Saved)\n",
      "epoch: 8100, train_loss: 15.981997, test_loss: 23.897503\n",
      "epoch: 8200, train_loss: 15.666514, test_loss: 23.561897 (Saved)\n",
      "epoch: 8300, train_loss: 15.255244, test_loss: 23.699345\n",
      "epoch: 8400, train_loss: 15.570604, test_loss: 23.512577 (Saved)\n",
      "epoch: 8500, train_loss: 15.659147, test_loss: 23.356236 (Saved)\n",
      "epoch: 8600, train_loss: 15.447710, test_loss: 23.418070\n",
      "epoch: 8700, train_loss: 15.094587, test_loss: 23.231718 (Saved)\n",
      "epoch: 8800, train_loss: 14.703372, test_loss: 23.640110\n",
      "epoch: 8900, train_loss: 14.972278, test_loss: 23.361597\n",
      "epoch: 9000, train_loss: 14.903752, test_loss: 23.056400 (Saved)\n",
      "epoch: 9100, train_loss: 14.545527, test_loss: 23.421059\n",
      "epoch: 9200, train_loss: 14.475671, test_loss: 23.019123 (Saved)\n",
      "epoch: 9300, train_loss: 14.863742, test_loss: 22.815268 (Saved)\n",
      "epoch: 9400, train_loss: 14.512224, test_loss: 22.866093\n",
      "epoch: 9500, train_loss: 14.380406, test_loss: 22.549929 (Saved)\n",
      "epoch: 9600, train_loss: 14.486519, test_loss: 22.487263 (Saved)\n",
      "epoch: 9700, train_loss: 14.814187, test_loss: 22.874363\n",
      "epoch: 9800, train_loss: 13.862332, test_loss: 22.401779 (Saved)\n",
      "epoch: 9900, train_loss: 14.030202, test_loss: 22.522305\n",
      "epoch: 10000, train_loss: 14.120429, test_loss: 22.142488 (Saved)\n",
      "epoch: 10100, train_loss: 13.835454, test_loss: 22.403240\n",
      "epoch: 10200, train_loss: 13.904963, test_loss: 22.266703\n",
      "epoch: 10300, train_loss: 13.665725, test_loss: 22.134621 (Saved)\n",
      "epoch: 10400, train_loss: 13.852849, test_loss: 22.041388 (Saved)\n",
      "epoch: 10500, train_loss: 13.709837, test_loss: 22.108992\n",
      "epoch: 10600, train_loss: 14.195205, test_loss: 21.829790 (Saved)\n",
      "epoch: 10700, train_loss: 13.545616, test_loss: 22.011545\n",
      "epoch: 10800, train_loss: 13.373864, test_loss: 22.014317\n",
      "epoch: 10900, train_loss: 13.690066, test_loss: 21.961824\n",
      "epoch: 11000, train_loss: 13.523283, test_loss: 22.134436\n",
      "epoch: 11100, train_loss: 13.552053, test_loss: 21.700310 (Saved)\n",
      "epoch: 11200, train_loss: 13.421289, test_loss: 21.955338\n",
      "epoch: 11300, train_loss: 13.329808, test_loss: 21.834066\n",
      "epoch: 11400, train_loss: 13.137859, test_loss: 21.687500 (Saved)\n",
      "epoch: 11500, train_loss: 13.197180, test_loss: 21.821512\n",
      "epoch: 11600, train_loss: 13.051181, test_loss: 21.568708 (Saved)\n",
      "epoch: 11700, train_loss: 13.092856, test_loss: 21.722771\n",
      "epoch: 11800, train_loss: 13.568189, test_loss: 21.275061 (Saved)\n",
      "epoch: 11900, train_loss: 12.743440, test_loss: 21.388451\n",
      "epoch: 12000, train_loss: 12.864478, test_loss: 21.418402\n",
      "epoch: 12100, train_loss: 12.851463, test_loss: 21.633062\n",
      "epoch: 12200, train_loss: 12.438147, test_loss: 21.523451\n",
      "epoch: 12300, train_loss: 12.554773, test_loss: 21.383289\n",
      "epoch: 12400, train_loss: 13.000396, test_loss: 21.273581 (Saved)\n",
      "epoch: 12500, train_loss: 12.463894, test_loss: 21.405605\n",
      "epoch: 12600, train_loss: 12.583456, test_loss: 21.320986\n",
      "epoch: 12700, train_loss: 12.380448, test_loss: 21.293751\n",
      "epoch: 12800, train_loss: 12.502072, test_loss: 21.396986\n",
      "epoch: 12900, train_loss: 12.187304, test_loss: 21.412577\n",
      "epoch: 13000, train_loss: 12.310508, test_loss: 21.124517 (Saved)\n",
      "epoch: 13100, train_loss: 12.144896, test_loss: 21.375969\n",
      "epoch: 13200, train_loss: 12.463841, test_loss: 21.214697\n",
      "epoch: 13300, train_loss: 12.359759, test_loss: 21.159554\n",
      "epoch: 13400, train_loss: 11.957117, test_loss: 20.975010 (Saved)\n",
      "epoch: 13500, train_loss: 11.946770, test_loss: 21.250572\n",
      "epoch: 13600, train_loss: 11.889146, test_loss: 21.188158\n",
      "epoch: 13700, train_loss: 12.204196, test_loss: 20.931198 (Saved)\n",
      "epoch: 13800, train_loss: 11.945207, test_loss: 21.108828\n",
      "epoch: 13900, train_loss: 11.790993, test_loss: 21.188761\n",
      "epoch: 14000, train_loss: 11.527493, test_loss: 21.094587\n",
      "epoch: 14100, train_loss: 11.833261, test_loss: 21.076832\n",
      "epoch: 14200, train_loss: 11.545115, test_loss: 21.163572\n",
      "epoch: 14300, train_loss: 11.555886, test_loss: 20.897678 (Saved)\n",
      "epoch: 14400, train_loss: 12.111650, test_loss: 21.353868\n",
      "epoch: 14500, train_loss: 11.608624, test_loss: 21.046463\n",
      "epoch: 14600, train_loss: 11.592647, test_loss: 20.837769 (Saved)\n",
      "epoch: 14700, train_loss: 11.360534, test_loss: 21.023323\n",
      "epoch: 14800, train_loss: 11.458599, test_loss: 21.181320\n",
      "epoch: 14900, train_loss: 11.419394, test_loss: 21.126259\n",
      "epoch: 15000, train_loss: 11.428917, test_loss: 20.904558\n",
      "epoch: 15100, train_loss: 11.357881, test_loss: 21.000341\n",
      "epoch: 15200, train_loss: 11.239960, test_loss: 20.987558\n",
      "epoch: 15300, train_loss: 11.191518, test_loss: 21.061275\n",
      "epoch: 15400, train_loss: 11.704980, test_loss: 21.288843\n",
      "epoch: 15500, train_loss: 11.250399, test_loss: 21.014694\n",
      "epoch: 15600, train_loss: 11.323021, test_loss: 20.762873 (Saved)\n",
      "epoch: 15700, train_loss: 11.114182, test_loss: 20.885157\n",
      "epoch: 15800, train_loss: 11.044568, test_loss: 21.037075\n",
      "epoch: 15900, train_loss: 11.067059, test_loss: 21.129215\n",
      "epoch: 16000, train_loss: 11.112117, test_loss: 20.676744 (Saved)\n",
      "epoch: 16100, train_loss: 10.846552, test_loss: 20.843111\n",
      "epoch: 16200, train_loss: 10.649282, test_loss: 20.671724 (Saved)\n",
      "epoch: 16300, train_loss: 10.393895, test_loss: 20.606895 (Saved)\n",
      "epoch: 16400, train_loss: 10.549115, test_loss: 20.848394\n",
      "epoch: 16500, train_loss: 10.625037, test_loss: 20.872585\n",
      "epoch: 16600, train_loss: 10.448401, test_loss: 20.374966 (Saved)\n",
      "epoch: 16700, train_loss: 10.201675, test_loss: 20.595587\n",
      "epoch: 16800, train_loss: 10.292385, test_loss: 20.546820\n",
      "epoch: 16900, train_loss: 10.140187, test_loss: 20.424059\n",
      "epoch: 17000, train_loss: 10.169024, test_loss: 20.582066\n",
      "epoch: 17100, train_loss: 9.856580, test_loss: 20.405169\n",
      "epoch: 17200, train_loss: 10.150938, test_loss: 20.575731\n",
      "epoch: 17300, train_loss: 9.878082, test_loss: 20.143381 (Saved)\n",
      "epoch: 17400, train_loss: 9.721000, test_loss: 20.403305\n",
      "epoch: 17500, train_loss: 9.919443, test_loss: 20.016088 (Saved)\n",
      "epoch: 17600, train_loss: 9.784041, test_loss: 20.176466\n",
      "epoch: 17700, train_loss: 9.742482, test_loss: 20.366703\n",
      "epoch: 17800, train_loss: 9.733011, test_loss: 20.418533\n",
      "epoch: 17900, train_loss: 9.615180, test_loss: 20.258654\n",
      "epoch: 18000, train_loss: 9.488640, test_loss: 20.050171\n",
      "epoch: 18100, train_loss: 9.825419, test_loss: 19.829645 (Saved)\n",
      "epoch: 18200, train_loss: 9.715657, test_loss: 19.952364\n",
      "epoch: 18300, train_loss: 9.655410, test_loss: 19.971170\n",
      "epoch: 18400, train_loss: 9.545199, test_loss: 20.041092\n",
      "epoch: 18500, train_loss: 9.476475, test_loss: 20.159325\n",
      "epoch: 18600, train_loss: 9.476443, test_loss: 20.081619\n",
      "epoch: 18700, train_loss: 9.340990, test_loss: 19.938978\n",
      "epoch: 18800, train_loss: 9.566555, test_loss: 19.921732\n",
      "epoch: 18900, train_loss: 9.261930, test_loss: 20.195927\n",
      "epoch: 19000, train_loss: 9.388269, test_loss: 20.011307\n",
      "epoch: 19100, train_loss: 9.332554, test_loss: 20.079281\n",
      "epoch: 19200, train_loss: 9.153893, test_loss: 20.012188\n",
      "epoch: 19300, train_loss: 9.176641, test_loss: 20.145891\n",
      "epoch: 19400, train_loss: 9.177114, test_loss: 20.016422\n",
      "epoch: 19500, train_loss: 9.353533, test_loss: 19.940378\n",
      "epoch: 19600, train_loss: 9.210916, test_loss: 20.090290\n",
      "epoch: 19700, train_loss: 9.189748, test_loss: 20.089613\n",
      "epoch: 19800, train_loss: 9.387833, test_loss: 19.944141\n",
      "epoch: 19900, train_loss: 8.947288, test_loss: 19.820675 (Saved)\n",
      "epoch: 20000, train_loss: 9.138176, test_loss: 19.811565 (Saved)\n",
      "epoch: 20100, train_loss: 8.820253, test_loss: 19.899746\n",
      "epoch: 20200, train_loss: 9.051983, test_loss: 20.183662\n",
      "epoch: 20300, train_loss: 9.014911, test_loss: 20.078867\n",
      "epoch: 20400, train_loss: 8.706733, test_loss: 19.984995\n",
      "epoch: 20500, train_loss: 8.838610, test_loss: 19.995691\n",
      "epoch: 20600, train_loss: 8.934521, test_loss: 20.027319\n",
      "epoch: 20700, train_loss: 8.849967, test_loss: 19.830959\n",
      "epoch: 20800, train_loss: 8.831810, test_loss: 20.043255\n",
      "epoch: 20900, train_loss: 9.087807, test_loss: 20.124189\n",
      "epoch: 21000, train_loss: 8.783832, test_loss: 19.958336\n",
      "epoch: 21100, train_loss: 8.961109, test_loss: 19.660479 (Saved)\n",
      "epoch: 21200, train_loss: 8.673449, test_loss: 19.922565\n",
      "epoch: 21300, train_loss: 8.710167, test_loss: 19.674152\n",
      "epoch: 21400, train_loss: 8.813746, test_loss: 19.744585\n",
      "epoch: 21500, train_loss: 8.519288, test_loss: 19.768591\n",
      "epoch: 21600, train_loss: 8.695497, test_loss: 19.686813\n",
      "epoch: 21700, train_loss: 8.670323, test_loss: 19.863802\n",
      "epoch: 21800, train_loss: 8.674194, test_loss: 19.845211\n",
      "epoch: 21900, train_loss: 8.592716, test_loss: 19.773344\n",
      "epoch: 22000, train_loss: 8.550653, test_loss: 19.806540\n",
      "epoch: 22100, train_loss: 8.469005, test_loss: 19.595371 (Saved)\n",
      "epoch: 22200, train_loss: 8.490973, test_loss: 19.887449\n",
      "epoch: 22300, train_loss: 8.453012, test_loss: 19.759518\n",
      "epoch: 22400, train_loss: 8.565247, test_loss: 19.657492\n",
      "epoch: 22500, train_loss: 8.359298, test_loss: 19.895487\n",
      "epoch: 22600, train_loss: 8.349455, test_loss: 19.716412\n",
      "epoch: 22700, train_loss: 8.353693, test_loss: 19.829290\n",
      "epoch: 22800, train_loss: 8.233438, test_loss: 19.775276\n",
      "epoch: 22900, train_loss: 8.251274, test_loss: 19.660463\n",
      "epoch: 23000, train_loss: 8.199347, test_loss: 19.645514\n",
      "epoch: 23100, train_loss: 8.427085, test_loss: 19.874338\n",
      "epoch: 23200, train_loss: 8.226342, test_loss: 19.589865 (Saved)\n",
      "epoch: 23300, train_loss: 7.848606, test_loss: 19.652811\n",
      "epoch: 23400, train_loss: 7.812056, test_loss: 19.903723\n",
      "epoch: 23500, train_loss: 7.638288, test_loss: 19.764910\n",
      "epoch: 23600, train_loss: 7.663040, test_loss: 19.771601\n",
      "epoch: 23700, train_loss: 7.588553, test_loss: 19.960651\n",
      "epoch: 23800, train_loss: 7.449368, test_loss: 20.065220\n",
      "epoch: 23900, train_loss: 7.548714, test_loss: 19.750496\n",
      "epoch: 24000, train_loss: 7.397383, test_loss: 19.725019\n",
      "epoch: 24100, train_loss: 7.306009, test_loss: 19.511684 (Saved)\n",
      "epoch: 24200, train_loss: 7.200483, test_loss: 19.520372\n",
      "epoch: 24300, train_loss: 7.180905, test_loss: 19.293455 (Saved)\n",
      "epoch: 24400, train_loss: 7.111262, test_loss: 19.379601\n",
      "epoch: 24500, train_loss: 7.275701, test_loss: 19.530252\n",
      "epoch: 24600, train_loss: 7.059992, test_loss: 19.058327 (Saved)\n",
      "epoch: 24700, train_loss: 7.018002, test_loss: 19.123701\n",
      "epoch: 24800, train_loss: 6.912344, test_loss: 19.074068\n",
      "epoch: 24900, train_loss: 6.866897, test_loss: 19.245838\n",
      "epoch: 25000, train_loss: 7.017510, test_loss: 19.336662\n",
      "epoch: 25100, train_loss: 6.808992, test_loss: 19.243711\n",
      "epoch: 25200, train_loss: 6.743372, test_loss: 19.317942\n",
      "epoch: 25300, train_loss: 6.900441, test_loss: 19.260780\n",
      "epoch: 25400, train_loss: 6.750791, test_loss: 19.335289\n",
      "epoch: 25500, train_loss: 6.743476, test_loss: 19.463316\n",
      "epoch: 25600, train_loss: 6.582390, test_loss: 19.228619\n",
      "epoch: 25700, train_loss: 6.605317, test_loss: 19.155642\n",
      "epoch: 25800, train_loss: 6.553066, test_loss: 19.224407\n",
      "epoch: 25900, train_loss: 6.597750, test_loss: 19.152672\n",
      "epoch: 26000, train_loss: 6.665759, test_loss: 19.208601\n",
      "epoch: 26100, train_loss: 6.680340, test_loss: 19.300240\n",
      "epoch: 26200, train_loss: 6.457808, test_loss: 19.259218\n",
      "epoch: 26300, train_loss: 6.481827, test_loss: 19.226566\n",
      "epoch: 26400, train_loss: 6.497932, test_loss: 19.130600\n",
      "epoch: 26500, train_loss: 6.329437, test_loss: 19.429665\n",
      "epoch: 26600, train_loss: 6.537888, test_loss: 19.475466\n",
      "epoch: 26700, train_loss: 6.460919, test_loss: 19.225809\n",
      "epoch: 26800, train_loss: 6.333439, test_loss: 19.195604\n",
      "epoch: 26900, train_loss: 6.431442, test_loss: 19.001709 (Saved)\n",
      "epoch: 27000, train_loss: 6.405731, test_loss: 19.389055\n",
      "epoch: 27100, train_loss: 6.301076, test_loss: 19.238930\n",
      "epoch: 27200, train_loss: 6.383848, test_loss: 19.376127\n",
      "epoch: 27300, train_loss: 6.126094, test_loss: 19.547810\n",
      "epoch: 27400, train_loss: 6.508917, test_loss: 19.055794\n",
      "epoch: 27500, train_loss: 6.066014, test_loss: 19.448311\n",
      "epoch: 27600, train_loss: 6.124023, test_loss: 19.184031\n",
      "epoch: 27700, train_loss: 6.112484, test_loss: 19.544439\n",
      "epoch: 27800, train_loss: 5.944124, test_loss: 19.289284\n",
      "epoch: 27900, train_loss: 6.010419, test_loss: 19.481386\n",
      "epoch: 28000, train_loss: 5.921318, test_loss: 19.526712\n",
      "epoch: 28100, train_loss: 6.024258, test_loss: 19.499693\n",
      "epoch: 28200, train_loss: 5.947490, test_loss: 19.503511\n",
      "epoch: 28300, train_loss: 5.950120, test_loss: 19.415096\n",
      "epoch: 28400, train_loss: 5.916929, test_loss: 19.285585\n",
      "epoch: 28500, train_loss: 5.830681, test_loss: 19.463259\n",
      "epoch: 28600, train_loss: 5.748068, test_loss: 19.381548\n",
      "epoch: 28700, train_loss: 5.659367, test_loss: 19.423693\n",
      "epoch: 28800, train_loss: 5.556621, test_loss: 19.647921\n",
      "epoch: 28900, train_loss: 5.640768, test_loss: 19.371401\n",
      "epoch: 29000, train_loss: 5.518509, test_loss: 19.560106\n",
      "epoch: 29100, train_loss: 5.544592, test_loss: 19.500914\n",
      "epoch: 29200, train_loss: 5.584585, test_loss: 19.417122\n",
      "epoch: 29300, train_loss: 5.445097, test_loss: 19.594564\n",
      "epoch: 29400, train_loss: 5.404196, test_loss: 19.485954\n",
      "epoch: 29500, train_loss: 5.360183, test_loss: 19.419329\n",
      "epoch: 29600, train_loss: 5.409567, test_loss: 19.588957\n",
      "epoch: 29700, train_loss: 5.481354, test_loss: 19.299519\n",
      "epoch: 29800, train_loss: 5.288912, test_loss: 19.598186\n",
      "epoch: 29900, train_loss: 5.383385, test_loss: 19.650526\n",
      "epoch: 30000, train_loss: 5.266138, test_loss: 19.625010\n",
      "epoch: 30100, train_loss: 5.265577, test_loss: 19.655418\n",
      "epoch: 30200, train_loss: 5.220803, test_loss: 19.600397\n",
      "epoch: 30300, train_loss: 5.183837, test_loss: 19.599741\n",
      "epoch: 30400, train_loss: 5.187989, test_loss: 19.687412\n",
      "epoch: 30500, train_loss: 5.133599, test_loss: 19.566639\n",
      "epoch: 30600, train_loss: 5.134153, test_loss: 19.841057\n",
      "epoch: 30700, train_loss: 5.193804, test_loss: 20.164373\n",
      "epoch: 30800, train_loss: 5.062742, test_loss: 19.809221\n",
      "epoch: 30900, train_loss: 5.046476, test_loss: 19.840862\n",
      "epoch: 31000, train_loss: 5.175774, test_loss: 20.286503\n",
      "epoch: 31100, train_loss: 5.066593, test_loss: 20.136271\n",
      "epoch: 31200, train_loss: 5.043506, test_loss: 20.015036\n",
      "epoch: 31300, train_loss: 4.943539, test_loss: 19.757347\n",
      "epoch: 31400, train_loss: 4.939801, test_loss: 19.987577\n",
      "epoch: 31500, train_loss: 5.116152, test_loss: 20.134466\n",
      "epoch: 31600, train_loss: 4.915581, test_loss: 20.205223\n",
      "epoch: 31700, train_loss: 4.986578, test_loss: 20.013048\n",
      "epoch: 31800, train_loss: 4.983417, test_loss: 20.215420\n",
      "epoch: 31900, train_loss: 4.965364, test_loss: 19.890118\n",
      "epoch: 32000, train_loss: 4.985963, test_loss: 19.964357\n",
      "epoch: 32100, train_loss: 4.904907, test_loss: 20.085016\n",
      "epoch: 32200, train_loss: 4.934435, test_loss: 20.284056\n",
      "epoch: 32300, train_loss: 4.912708, test_loss: 20.275793\n",
      "epoch: 32400, train_loss: 4.813917, test_loss: 20.233887\n",
      "epoch: 32500, train_loss: 4.830611, test_loss: 20.136530\n",
      "epoch: 32600, train_loss: 4.746176, test_loss: 20.336592\n",
      "epoch: 32700, train_loss: 4.898341, test_loss: 20.112013\n",
      "epoch: 32800, train_loss: 4.945793, test_loss: 20.480995\n",
      "epoch: 32900, train_loss: 4.778848, test_loss: 20.171522\n",
      "epoch: 33000, train_loss: 4.781566, test_loss: 20.234144\n",
      "epoch: 33100, train_loss: 4.735650, test_loss: 20.374598\n",
      "epoch: 33200, train_loss: 4.773306, test_loss: 20.254522\n",
      "epoch: 33300, train_loss: 4.857688, test_loss: 20.206486\n",
      "epoch: 33400, train_loss: 4.722908, test_loss: 20.305984\n",
      "epoch: 33500, train_loss: 4.740048, test_loss: 20.271486\n",
      "epoch: 33600, train_loss: 4.744499, test_loss: 20.298836\n",
      "epoch: 33700, train_loss: 4.712453, test_loss: 20.452583\n",
      "epoch: 33800, train_loss: 4.783394, test_loss: 20.611832\n",
      "epoch: 33900, train_loss: 4.575466, test_loss: 20.416689\n",
      "epoch: 34000, train_loss: 4.705750, test_loss: 20.489035\n",
      "epoch: 34100, train_loss: 4.625652, test_loss: 20.407181\n",
      "epoch: 34200, train_loss: 4.671245, test_loss: 20.400618\n",
      "epoch: 34300, train_loss: 4.611657, test_loss: 20.362839\n",
      "epoch: 34400, train_loss: 4.537008, test_loss: 20.482477\n",
      "epoch: 34500, train_loss: 4.633198, test_loss: 20.450087\n",
      "epoch: 34600, train_loss: 4.671639, test_loss: 20.625227\n",
      "epoch: 34700, train_loss: 4.482287, test_loss: 20.520060\n",
      "epoch: 34800, train_loss: 4.594092, test_loss: 20.432108\n",
      "epoch: 34900, train_loss: 4.574924, test_loss: 20.487642\n",
      "epoch: 35000, train_loss: 4.529612, test_loss: 20.691542\n",
      "epoch: 35100, train_loss: 4.659512, test_loss: 20.720013\n",
      "epoch: 35200, train_loss: 4.578377, test_loss: 20.363468\n",
      "epoch: 35300, train_loss: 4.550221, test_loss: 20.672926\n",
      "epoch: 35400, train_loss: 4.436845, test_loss: 20.752522\n",
      "epoch: 35500, train_loss: 4.590886, test_loss: 20.611708\n",
      "epoch: 35600, train_loss: 4.529748, test_loss: 20.785633\n",
      "epoch: 35700, train_loss: 4.524889, test_loss: 20.775169\n",
      "epoch: 35800, train_loss: 4.519718, test_loss: 20.782526\n",
      "epoch: 35900, train_loss: 4.611618, test_loss: 21.047762\n",
      "epoch: 36000, train_loss: 4.426625, test_loss: 21.006630\n",
      "epoch: 36100, train_loss: 4.510294, test_loss: 20.768126\n",
      "epoch: 36200, train_loss: 4.459579, test_loss: 20.894939\n",
      "epoch: 36300, train_loss: 4.465834, test_loss: 20.908165\n",
      "epoch: 36400, train_loss: 4.468264, test_loss: 20.913671\n",
      "epoch: 36500, train_loss: 4.525745, test_loss: 21.259226\n",
      "epoch: 36600, train_loss: 4.352753, test_loss: 20.726986\n",
      "epoch: 36700, train_loss: 4.336908, test_loss: 20.857403\n",
      "epoch: 36800, train_loss: 4.389841, test_loss: 20.946821\n",
      "epoch: 36900, train_loss: 4.431451, test_loss: 20.986717\n",
      "epoch: 37000, train_loss: 4.476164, test_loss: 21.034054\n",
      "epoch: 37100, train_loss: 4.382216, test_loss: 20.950649\n",
      "epoch: 37200, train_loss: 4.432946, test_loss: 20.972797\n",
      "epoch: 37300, train_loss: 4.386926, test_loss: 20.926590\n",
      "epoch: 37400, train_loss: 4.257563, test_loss: 21.049770\n",
      "epoch: 37500, train_loss: 4.342854, test_loss: 21.074503\n",
      "epoch: 37600, train_loss: 4.322042, test_loss: 20.999472\n",
      "epoch: 37700, train_loss: 4.303809, test_loss: 21.060812\n",
      "epoch: 37800, train_loss: 4.303025, test_loss: 21.118074\n",
      "epoch: 37900, train_loss: 4.276180, test_loss: 21.068840\n",
      "epoch: 38000, train_loss: 4.361717, test_loss: 21.102884\n",
      "epoch: 38100, train_loss: 4.268635, test_loss: 21.044155\n",
      "epoch: 38200, train_loss: 4.366373, test_loss: 21.083252\n",
      "epoch: 38300, train_loss: 4.373506, test_loss: 21.018286\n",
      "epoch: 38400, train_loss: 4.307349, test_loss: 21.069983\n",
      "epoch: 38500, train_loss: 4.254471, test_loss: 21.229193\n",
      "epoch: 38600, train_loss: 4.229975, test_loss: 21.238609\n",
      "epoch: 38700, train_loss: 4.322708, test_loss: 21.186623\n",
      "epoch: 38800, train_loss: 4.310640, test_loss: 20.984922\n",
      "epoch: 38900, train_loss: 4.249188, test_loss: 21.145777\n",
      "epoch: 39000, train_loss: 4.206827, test_loss: 21.002092\n",
      "epoch: 39100, train_loss: 4.244520, test_loss: 21.172237\n",
      "epoch: 39200, train_loss: 4.330715, test_loss: 21.310385\n",
      "epoch: 39300, train_loss: 4.197781, test_loss: 21.228136\n",
      "epoch: 39400, train_loss: 4.373079, test_loss: 21.162737\n",
      "epoch: 39500, train_loss: 4.248749, test_loss: 21.123278\n",
      "epoch: 39600, train_loss: 4.262042, test_loss: 21.236580\n",
      "epoch: 39700, train_loss: 4.200726, test_loss: 21.216099\n",
      "epoch: 39800, train_loss: 4.266757, test_loss: 21.034239\n",
      "epoch: 39900, train_loss: 4.187980, test_loss: 21.194965\n",
      "epoch: 40000, train_loss: 4.186998, test_loss: 21.138813\n",
      "epoch: 40100, train_loss: 4.186017, test_loss: 21.220076\n",
      "epoch: 40200, train_loss: 4.174440, test_loss: 21.327435\n",
      "epoch: 40300, train_loss: 4.157555, test_loss: 21.342054\n",
      "epoch: 40400, train_loss: 4.323195, test_loss: 21.180529\n",
      "epoch: 40500, train_loss: 4.070045, test_loss: 21.171257\n",
      "epoch: 40600, train_loss: 4.218512, test_loss: 21.443359\n",
      "epoch: 40700, train_loss: 4.102878, test_loss: 21.304121\n",
      "epoch: 40800, train_loss: 4.188962, test_loss: 21.138121\n",
      "epoch: 40900, train_loss: 4.284934, test_loss: 21.156996\n",
      "epoch: 41000, train_loss: 4.116811, test_loss: 21.260275\n",
      "epoch: 41100, train_loss: 4.138188, test_loss: 20.995361\n",
      "epoch: 41200, train_loss: 4.092609, test_loss: 21.415123\n",
      "epoch: 41300, train_loss: 4.227089, test_loss: 21.165874\n",
      "epoch: 41400, train_loss: 4.067072, test_loss: 21.190252\n",
      "epoch: 41500, train_loss: 4.077786, test_loss: 21.503376\n",
      "epoch: 41600, train_loss: 4.050510, test_loss: 21.591316\n",
      "epoch: 41700, train_loss: 4.016553, test_loss: 21.394930\n",
      "epoch: 41800, train_loss: 4.058647, test_loss: 21.376493\n",
      "epoch: 41900, train_loss: 4.065909, test_loss: 21.480570\n",
      "epoch: 42000, train_loss: 4.032164, test_loss: 21.506571\n",
      "epoch: 42100, train_loss: 4.018116, test_loss: 21.698709\n",
      "epoch: 42200, train_loss: 4.154506, test_loss: 21.396286\n",
      "epoch: 42300, train_loss: 4.053855, test_loss: 21.523088\n",
      "epoch: 42400, train_loss: 3.997884, test_loss: 21.435669\n",
      "epoch: 42500, train_loss: 4.045771, test_loss: 21.299438\n",
      "epoch: 42600, train_loss: 4.024642, test_loss: 21.475960\n",
      "epoch: 42700, train_loss: 4.039320, test_loss: 21.553648\n",
      "epoch: 42800, train_loss: 3.999122, test_loss: 21.226748\n",
      "epoch: 42900, train_loss: 4.015885, test_loss: 21.440353\n",
      "epoch: 43000, train_loss: 3.952991, test_loss: 21.307032\n",
      "epoch: 43100, train_loss: 4.013461, test_loss: 21.215242\n",
      "epoch: 43200, train_loss: 3.857959, test_loss: 21.431112\n",
      "epoch: 43300, train_loss: 3.996731, test_loss: 21.296665\n",
      "epoch: 43400, train_loss: 3.925555, test_loss: 21.381405\n",
      "epoch: 43500, train_loss: 4.079854, test_loss: 21.347841\n",
      "epoch: 43600, train_loss: 3.968158, test_loss: 21.376040\n",
      "epoch: 43700, train_loss: 3.977889, test_loss: 21.377501\n",
      "epoch: 43800, train_loss: 3.961040, test_loss: 21.642424\n",
      "epoch: 43900, train_loss: 3.993044, test_loss: 21.357708\n",
      "epoch: 44000, train_loss: 3.941998, test_loss: 21.525335\n",
      "epoch: 44100, train_loss: 4.004041, test_loss: 21.296341\n",
      "epoch: 44200, train_loss: 4.015994, test_loss: 21.251980\n",
      "epoch: 44300, train_loss: 3.868071, test_loss: 21.257410\n",
      "epoch: 44400, train_loss: 3.891048, test_loss: 21.115883\n",
      "epoch: 44500, train_loss: 3.895210, test_loss: 21.264809\n",
      "epoch: 44600, train_loss: 3.819245, test_loss: 21.260565\n",
      "epoch: 44700, train_loss: 3.898610, test_loss: 21.389935\n",
      "epoch: 44800, train_loss: 3.933573, test_loss: 21.376360\n",
      "epoch: 44900, train_loss: 3.890705, test_loss: 21.447863\n",
      "epoch: 45000, train_loss: 3.931712, test_loss: 21.484827\n",
      "epoch: 45100, train_loss: 3.892886, test_loss: 21.423130\n",
      "epoch: 45200, train_loss: 3.832309, test_loss: 21.297323\n",
      "epoch: 45300, train_loss: 3.877901, test_loss: 21.371599\n",
      "epoch: 45400, train_loss: 3.905792, test_loss: 21.292887\n",
      "epoch: 45500, train_loss: 3.903076, test_loss: 21.351284\n",
      "epoch: 45600, train_loss: 3.896529, test_loss: 21.307566\n",
      "epoch: 45700, train_loss: 3.851334, test_loss: 21.129047\n",
      "epoch: 45800, train_loss: 3.817382, test_loss: 21.203548\n",
      "epoch: 45900, train_loss: 3.892097, test_loss: 21.544092\n",
      "epoch: 46000, train_loss: 3.817593, test_loss: 21.300423\n",
      "epoch: 46100, train_loss: 3.748322, test_loss: 21.149401\n",
      "epoch: 46200, train_loss: 3.854982, test_loss: 21.280098\n",
      "epoch: 46300, train_loss: 3.899894, test_loss: 21.344833\n",
      "epoch: 46400, train_loss: 3.880865, test_loss: 21.200306\n",
      "epoch: 46500, train_loss: 3.859816, test_loss: 21.265091\n",
      "epoch: 46600, train_loss: 3.803946, test_loss: 21.326138\n",
      "epoch: 46700, train_loss: 3.772343, test_loss: 21.412374\n",
      "epoch: 46800, train_loss: 3.834246, test_loss: 21.155413\n",
      "epoch: 46900, train_loss: 3.860983, test_loss: 21.244923\n",
      "epoch: 47000, train_loss: 3.822358, test_loss: 21.234924\n",
      "epoch: 47100, train_loss: 3.838386, test_loss: 21.321966\n",
      "epoch: 47200, train_loss: 3.890659, test_loss: 21.438597\n",
      "epoch: 47300, train_loss: 3.728286, test_loss: 21.174030\n",
      "epoch: 47400, train_loss: 3.737142, test_loss: 21.312626\n",
      "epoch: 47500, train_loss: 3.834058, test_loss: 21.068832\n",
      "epoch: 47600, train_loss: 3.801983, test_loss: 21.206369\n",
      "epoch: 47700, train_loss: 3.822055, test_loss: 21.244692\n",
      "epoch: 47800, train_loss: 3.784625, test_loss: 21.255857\n",
      "epoch: 47900, train_loss: 3.747927, test_loss: 21.165012\n",
      "epoch: 48000, train_loss: 3.806423, test_loss: 21.221754\n",
      "epoch: 48100, train_loss: 3.869216, test_loss: 21.182823\n",
      "epoch: 48200, train_loss: 3.703714, test_loss: 21.237434\n",
      "epoch: 48300, train_loss: 3.794620, test_loss: 21.282148\n",
      "epoch: 48400, train_loss: 3.801295, test_loss: 21.275520\n",
      "epoch: 48500, train_loss: 3.772422, test_loss: 21.271187\n",
      "epoch: 48600, train_loss: 3.767152, test_loss: 21.243559\n",
      "epoch: 48700, train_loss: 3.692103, test_loss: 21.460234\n",
      "epoch: 48800, train_loss: 3.822746, test_loss: 21.144863\n",
      "epoch: 48900, train_loss: 3.779810, test_loss: 21.078697\n",
      "epoch: 49000, train_loss: 3.728449, test_loss: 21.515333\n",
      "epoch: 49100, train_loss: 3.767089, test_loss: 21.237083\n",
      "epoch: 49200, train_loss: 3.635017, test_loss: 21.462328\n",
      "epoch: 49300, train_loss: 3.762881, test_loss: 21.373957\n",
      "epoch: 49400, train_loss: 3.823887, test_loss: 21.300158\n",
      "epoch: 49500, train_loss: 3.664403, test_loss: 21.179085\n",
      "epoch: 49600, train_loss: 3.796684, test_loss: 21.242899\n",
      "epoch: 49700, train_loss: 3.717869, test_loss: 21.193174\n",
      "epoch: 49800, train_loss: 3.757493, test_loss: 21.387589\n",
      "epoch: 49900, train_loss: 3.725700, test_loss: 21.409431\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Saving ONNX model: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Exported graph: graph(%onnx::Sub_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(4, 2, strides=[2, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(20, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(80, 20, strides=[20, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(80, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.weight : Float(20, 80, strides=[80, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.bias : Float(20, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.weight : Float(4, 20, strides=[20, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.weight : Float(2, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/Constant_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 0.0000 -848.5281 [ CUDAFloatType{2} ], onnx_name=\"/Constant\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:16:0\n",
      "  %/Sub_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Sub[onnx_name=\"/Sub\"](%onnx::Sub_0, %/Constant_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:16:0\n",
      "  %/Constant_1_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1183.6727  1697.0563 [ CUDAFloatType{2} ], onnx_name=\"/Constant_1\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:16:0\n",
      "  %/Div_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Div[onnx_name=\"/Div\"](%/Sub_output_0, %/Constant_1_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:16:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Div_output_0, %fc1.weight, %fc1.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc1 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh\"](%/fc1/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1971:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Tanh_output_0, %fc2.weight, %fc2.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc2 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_1_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_1\"](%/fc2/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1971:0\n",
      "  %/fc3/Gemm_output_0 : Float(1, 80, strides=[80, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Tanh_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc3 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_output_0 : Float(1, 80, strides=[80, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/Relu\"](%/fc3/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/fc4/Gemm_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc4/Gemm\"](%/Relu_output_0, %fc4.weight, %fc4.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc4 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Relu_1_output_0 : Float(1, 20, strides=[20, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/Relu_1\"](%/fc4/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/fc5/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc5/Gemm\"](%/Relu_1_output_0, %fc5.weight, %fc5.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc5 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc6/Gemm_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc6/Gemm\"](%/fc5/Gemm_output_0, %fc6.weight, %fc6.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc6 # /home/icar/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_2_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1238   488 [ CUDAFloatType{2} ], onnx_name=\"/Constant_2\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:23:0\n",
      "  %/Mul_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Mul[onnx_name=\"/Mul\"](%/fc6/Gemm_output_0, %/Constant_2_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:23:0\n",
      "  %/Constant_3_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value=  30  204 [ CUDAFloatType{2} ], onnx_name=\"/Constant_3\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:23:0\n",
      "  %30 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/Add\"](%/Mul_output_0, %/Constant_3_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_290784/1725647797.py:23:0\n",
      "  return (%30)\n",
      "\n",
      "Saved ONNX model: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'cm2pixel_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 350.0000,  100.0000],\n",
      "        [ 998.4603,  665.6403],\n",
      "        [ 100.0000,  175.0000],\n",
      "        [ 250.0000, -100.0000],\n",
      "        [1200.0000,    0.0000],\n",
      "        [  50.0000,   50.0000],\n",
      "        [ 350.0000, -200.0000],\n",
      "        [ 200.0000, -300.0000],\n",
      "        [   0.0000,  -75.0000],\n",
      "        [  75.0000,   50.0000],\n",
      "        [ 100.0000, -125.0000],\n",
      "        [ 250.0000, -300.0000],\n",
      "        [ 416.0251,  277.3501],\n",
      "        [ 350.0000, -100.0000],\n",
      "        [ 447.2136, -223.6068],\n",
      "        [ 715.5417, -357.7709],\n",
      "        [ 100.0000, -175.0000],\n",
      "        [ 350.0000,  150.0000],\n",
      "        [ 200.0000,  150.0000],\n",
      "        [ 350.0000,  -50.0000],\n",
      "        [ 250.0000,   50.0000],\n",
      "        [  50.0000, -100.0000],\n",
      "        [ 250.0000,  250.0000],\n",
      "        [ 150.0000,  250.0000],\n",
      "        [ 998.4603, -665.6403],\n",
      "        [ 450.0000,    0.0000],\n",
      "        [ 100.0000,    0.0000],\n",
      "        [ 474.3416,  158.1139],\n",
      "        [ 450.0000, -200.0000]], device='cuda:0')\n",
      "y\n",
      "tensor([[ 500.,  292.],\n",
      "        [ 246.,  220.],\n",
      "        [ 122.,  452.],\n",
      "        [ 822.,  324.],\n",
      "        [ 640.,  204.],\n",
      "        [ 444.,  532.],\n",
      "        [ 924.,  284.],\n",
      "        [1272.,  342.],\n",
      "        [1066.,  684.],\n",
      "        [ 472.,  484.],\n",
      "        [1024.,  432.],\n",
      "        [1188.,  314.],\n",
      "        [ 298.,  284.],\n",
      "        [ 782.,  286.],\n",
      "        [ 904.,  266.],\n",
      "        [ 926.,  230.],\n",
      "        [1184.,  432.],\n",
      "        [ 432.,  294.],\n",
      "        [ 332.,  358.],\n",
      "        [ 710.,  288.],\n",
      "        [ 552.,  328.],\n",
      "        [1040.,  522.],\n",
      "        [ 204.,  336.],\n",
      "        [  36.,  400.],\n",
      "        [1032.,  208.],\n",
      "        [ 642.,  264.],\n",
      "        [ 642.,  444.],\n",
      "        [ 464.,  270.],\n",
      "        [ 872.,  260.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[ 503.5138,  292.0036],\n",
      "        [ 257.8304,  216.3457],\n",
      "        [ 119.8478,  451.4090],\n",
      "        [ 822.6161,  322.7130],\n",
      "        [ 630.3757,  205.7268],\n",
      "        [ 445.8705,  534.0175],\n",
      "        [ 921.6047,  286.0067],\n",
      "        [1278.3829,  341.2339],\n",
      "        [1069.9456,  683.3346],\n",
      "        [ 463.6099,  486.1717],\n",
      "        [1020.3949,  436.5154],\n",
      "        [1175.3363,  319.7419],\n",
      "        [ 304.1833,  281.2268],\n",
      "        [ 785.2568,  288.5344],\n",
      "        [ 906.3627,  260.2971],\n",
      "        [ 934.4810,  228.2210],\n",
      "        [1178.2324,  431.2178],\n",
      "        [ 429.1342,  293.8703],\n",
      "        [ 325.1584,  359.3651],\n",
      "        [ 708.4999,  289.9393],\n",
      "        [ 554.6127,  327.2776],\n",
      "        [1039.6799,  523.0022],\n",
      "        [ 193.2001,  331.7912],\n",
      "        [  38.2278,  399.4535],\n",
      "        [1025.5212,  211.0294],\n",
      "        [ 635.5088,  268.0193],\n",
      "        [ 644.2143,  444.7671],\n",
      "        [ 465.4287,  265.0684],\n",
      "        [ 876.6675,  259.6939]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[-3.5138e+00, -3.6011e-03],\n",
      "        [-1.1830e+01,  3.6543e+00],\n",
      "        [ 2.1522e+00,  5.9100e-01],\n",
      "        [-6.1615e-01,  1.2870e+00],\n",
      "        [ 9.6243e+00, -1.7268e+00],\n",
      "        [-1.8705e+00, -2.0175e+00],\n",
      "        [ 2.3953e+00, -2.0067e+00],\n",
      "        [-6.3829e+00,  7.6605e-01],\n",
      "        [-3.9456e+00,  6.6541e-01],\n",
      "        [ 8.3901e+00, -2.1717e+00],\n",
      "        [ 3.6051e+00, -4.5154e+00],\n",
      "        [ 1.2664e+01, -5.7419e+00],\n",
      "        [-6.1833e+00,  2.7732e+00],\n",
      "        [-3.2568e+00, -2.5344e+00],\n",
      "        [-2.3627e+00,  5.7029e+00],\n",
      "        [-8.4810e+00,  1.7790e+00],\n",
      "        [ 5.7676e+00,  7.8223e-01],\n",
      "        [ 2.8658e+00,  1.2967e-01],\n",
      "        [ 6.8416e+00, -1.3651e+00],\n",
      "        [ 1.5001e+00, -1.9393e+00],\n",
      "        [-2.6127e+00,  7.2238e-01],\n",
      "        [ 3.2007e-01, -1.0022e+00],\n",
      "        [ 1.0800e+01,  4.2088e+00],\n",
      "        [-2.2278e+00,  5.4651e-01],\n",
      "        [ 6.4788e+00, -3.0294e+00],\n",
      "        [ 6.4912e+00, -4.0193e+00],\n",
      "        [-2.2143e+00, -7.6709e-01],\n",
      "        [-1.4287e+00,  4.9316e+00],\n",
      "        [-4.6675e+00,  3.0606e-01]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
