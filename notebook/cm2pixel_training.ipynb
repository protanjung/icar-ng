{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 06/11/2023 at 21:46:22.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/icar/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/icar/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/icar/icar-ng-data/model\n",
      "Model file: cm2pixel_model.pt\n",
      "Model path: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'cm2pixel_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([   0.0000, -665.6403]), max_x: tensor([1200.0000,  848.5281])\n",
      "min_y: tensor([ 30., 204.]), max_y: tensor([1272.,  692.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 2:4].values\n",
    "        y = self.dataframe.iloc[:, 0:2].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 118\n",
      "dataset_test: 29\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 20)\n",
    "        self.fc3 = nn.Linear(20, 80)\n",
    "        self.fc4 = nn.Linear(80, 20)\n",
    "        self.fc5 = nn.Linear(20, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n",
      "Loaded model: /home/icar/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 362789.187500, test_loss: 128736.757812 (Saved)\n",
      "epoch: 100, train_loss: 295456.921875, test_loss: 94952.304688 (Saved)\n",
      "epoch: 200, train_loss: 249925.171875, test_loss: 78893.031250 (Saved)\n",
      "epoch: 300, train_loss: 217964.890625, test_loss: 68458.742188 (Saved)\n",
      "epoch: 400, train_loss: 180029.445312, test_loss: 61140.601562 (Saved)\n",
      "epoch: 500, train_loss: 157320.835938, test_loss: 55609.027344 (Saved)\n",
      "epoch: 600, train_loss: 136375.972656, test_loss: 50733.558594 (Saved)\n",
      "epoch: 700, train_loss: 118432.917969, test_loss: 46996.863281 (Saved)\n",
      "epoch: 800, train_loss: 103949.160156, test_loss: 43664.476562 (Saved)\n",
      "epoch: 900, train_loss: 91680.980469, test_loss: 38875.640625 (Saved)\n",
      "epoch: 1000, train_loss: 77516.207031, test_loss: 33705.539062 (Saved)\n",
      "epoch: 1100, train_loss: 62097.507812, test_loss: 27999.960938 (Saved)\n",
      "epoch: 1200, train_loss: 49479.753906, test_loss: 23256.404297 (Saved)\n",
      "epoch: 1300, train_loss: 42825.185547, test_loss: 20575.015625 (Saved)\n",
      "epoch: 1400, train_loss: 38577.396484, test_loss: 18369.185547 (Saved)\n",
      "epoch: 1500, train_loss: 35747.036133, test_loss: 16566.539062 (Saved)\n",
      "epoch: 1600, train_loss: 32140.896484, test_loss: 14927.911133 (Saved)\n",
      "epoch: 1700, train_loss: 28386.768555, test_loss: 13482.189453 (Saved)\n",
      "epoch: 1800, train_loss: 25773.620117, test_loss: 12220.985352 (Saved)\n",
      "epoch: 1900, train_loss: 23813.034180, test_loss: 11160.014648 (Saved)\n",
      "epoch: 2000, train_loss: 21442.373047, test_loss: 10293.101562 (Saved)\n",
      "epoch: 2100, train_loss: 20909.589844, test_loss: 9589.375000 (Saved)\n",
      "epoch: 2200, train_loss: 18597.871094, test_loss: 8953.955078 (Saved)\n",
      "epoch: 2300, train_loss: 17842.309570, test_loss: 8366.705078 (Saved)\n",
      "epoch: 2400, train_loss: 16352.377930, test_loss: 7792.634766 (Saved)\n",
      "epoch: 2500, train_loss: 14724.313232, test_loss: 7286.026367 (Saved)\n",
      "epoch: 2600, train_loss: 14113.835938, test_loss: 6792.535156 (Saved)\n",
      "epoch: 2700, train_loss: 13340.758301, test_loss: 6303.038574 (Saved)\n",
      "epoch: 2800, train_loss: 12382.591797, test_loss: 5817.552734 (Saved)\n",
      "epoch: 2900, train_loss: 11227.914062, test_loss: 5330.874512 (Saved)\n",
      "epoch: 3000, train_loss: 10159.738770, test_loss: 4892.291504 (Saved)\n",
      "epoch: 3100, train_loss: 9651.056152, test_loss: 4508.791992 (Saved)\n",
      "epoch: 3200, train_loss: 8417.278809, test_loss: 4126.611816 (Saved)\n",
      "epoch: 3300, train_loss: 7617.861816, test_loss: 3761.026611 (Saved)\n",
      "epoch: 3400, train_loss: 6361.634766, test_loss: 3464.901367 (Saved)\n",
      "epoch: 3500, train_loss: 5189.510010, test_loss: 3057.187744 (Saved)\n",
      "epoch: 3600, train_loss: 4638.177734, test_loss: 2679.099121 (Saved)\n",
      "epoch: 3700, train_loss: 3793.672974, test_loss: 2356.206299 (Saved)\n",
      "epoch: 3800, train_loss: 3328.973267, test_loss: 2102.810791 (Saved)\n",
      "epoch: 3900, train_loss: 3009.005859, test_loss: 1899.836426 (Saved)\n",
      "epoch: 4000, train_loss: 2780.433960, test_loss: 1753.207520 (Saved)\n",
      "epoch: 4100, train_loss: 2547.601196, test_loss: 1628.768066 (Saved)\n",
      "epoch: 4200, train_loss: 2414.831055, test_loss: 1526.934570 (Saved)\n",
      "epoch: 4300, train_loss: 2261.321411, test_loss: 1464.295288 (Saved)\n",
      "epoch: 4400, train_loss: 2040.951599, test_loss: 1402.249268 (Saved)\n",
      "epoch: 4500, train_loss: 1942.669434, test_loss: 1343.013916 (Saved)\n",
      "epoch: 4600, train_loss: 1813.833557, test_loss: 1289.299438 (Saved)\n",
      "epoch: 4700, train_loss: 1755.964905, test_loss: 1233.250000 (Saved)\n",
      "epoch: 4800, train_loss: 1616.979553, test_loss: 1180.941040 (Saved)\n",
      "epoch: 4900, train_loss: 1542.014648, test_loss: 1129.569092 (Saved)\n",
      "epoch: 5000, train_loss: 1432.053467, test_loss: 1083.974609 (Saved)\n",
      "epoch: 5100, train_loss: 1372.457581, test_loss: 1044.462158 (Saved)\n",
      "epoch: 5200, train_loss: 1318.725647, test_loss: 1006.478699 (Saved)\n",
      "epoch: 5300, train_loss: 1269.338043, test_loss: 969.577698 (Saved)\n",
      "epoch: 5400, train_loss: 1220.497314, test_loss: 922.547302 (Saved)\n",
      "epoch: 5500, train_loss: 1068.295624, test_loss: 885.543640 (Saved)\n",
      "epoch: 5600, train_loss: 1031.841797, test_loss: 874.427124 (Saved)\n",
      "epoch: 5700, train_loss: 959.141479, test_loss: 859.305481 (Saved)\n",
      "epoch: 5800, train_loss: 923.505829, test_loss: 832.227661 (Saved)\n",
      "epoch: 5900, train_loss: 888.101105, test_loss: 805.068298 (Saved)\n",
      "epoch: 6000, train_loss: 842.068970, test_loss: 770.648682 (Saved)\n",
      "epoch: 6100, train_loss: 795.763763, test_loss: 742.111267 (Saved)\n",
      "epoch: 6200, train_loss: 764.804840, test_loss: 710.052246 (Saved)\n",
      "epoch: 6300, train_loss: 715.195038, test_loss: 682.540283 (Saved)\n",
      "epoch: 6400, train_loss: 690.379364, test_loss: 660.887817 (Saved)\n",
      "epoch: 6500, train_loss: 648.366241, test_loss: 635.339050 (Saved)\n",
      "epoch: 6600, train_loss: 642.018372, test_loss: 606.830383 (Saved)\n",
      "epoch: 6700, train_loss: 596.338623, test_loss: 585.507751 (Saved)\n",
      "epoch: 6800, train_loss: 560.346436, test_loss: 565.882324 (Saved)\n",
      "epoch: 6900, train_loss: 539.527893, test_loss: 550.258179 (Saved)\n",
      "epoch: 7000, train_loss: 519.747223, test_loss: 532.857239 (Saved)\n",
      "epoch: 7100, train_loss: 495.666611, test_loss: 517.538391 (Saved)\n",
      "epoch: 7200, train_loss: 470.571426, test_loss: 503.873627 (Saved)\n",
      "epoch: 7300, train_loss: 445.459534, test_loss: 488.883484 (Saved)\n",
      "epoch: 7400, train_loss: 433.965103, test_loss: 477.880737 (Saved)\n",
      "epoch: 7500, train_loss: 410.899414, test_loss: 465.895477 (Saved)\n",
      "epoch: 7600, train_loss: 400.106796, test_loss: 451.412170 (Saved)\n",
      "epoch: 7700, train_loss: 393.682106, test_loss: 438.707153 (Saved)\n",
      "epoch: 7800, train_loss: 357.967361, test_loss: 432.655182 (Saved)\n",
      "epoch: 7900, train_loss: 357.215469, test_loss: 418.220917 (Saved)\n",
      "epoch: 8000, train_loss: 338.066879, test_loss: 407.320221 (Saved)\n",
      "epoch: 8100, train_loss: 321.017883, test_loss: 397.112549 (Saved)\n",
      "epoch: 8200, train_loss: 314.296860, test_loss: 381.751099 (Saved)\n",
      "epoch: 8300, train_loss: 303.693367, test_loss: 371.625336 (Saved)\n",
      "epoch: 8400, train_loss: 284.816254, test_loss: 360.917847 (Saved)\n",
      "epoch: 8500, train_loss: 276.182449, test_loss: 350.478180 (Saved)\n",
      "epoch: 8600, train_loss: 262.252319, test_loss: 342.512909 (Saved)\n",
      "epoch: 8700, train_loss: 251.435936, test_loss: 338.232330 (Saved)\n",
      "epoch: 8800, train_loss: 245.209503, test_loss: 330.491882 (Saved)\n",
      "epoch: 8900, train_loss: 238.886169, test_loss: 325.823639 (Saved)\n",
      "epoch: 9000, train_loss: 228.632500, test_loss: 318.532379 (Saved)\n",
      "epoch: 9100, train_loss: 225.021744, test_loss: 312.567322 (Saved)\n",
      "epoch: 9200, train_loss: 209.569191, test_loss: 306.580139 (Saved)\n",
      "epoch: 9300, train_loss: 208.600456, test_loss: 302.904205 (Saved)\n",
      "epoch: 9400, train_loss: 200.740959, test_loss: 299.399048 (Saved)\n",
      "epoch: 9500, train_loss: 198.402000, test_loss: 295.419861 (Saved)\n",
      "epoch: 9600, train_loss: 195.353477, test_loss: 293.471375 (Saved)\n",
      "epoch: 9700, train_loss: 190.578476, test_loss: 289.573242 (Saved)\n",
      "epoch: 9800, train_loss: 185.814873, test_loss: 287.022064 (Saved)\n",
      "epoch: 9900, train_loss: 177.819107, test_loss: 283.418701 (Saved)\n",
      "epoch: 10000, train_loss: 179.825233, test_loss: 279.982300 (Saved)\n",
      "epoch: 10100, train_loss: 172.563995, test_loss: 277.156219 (Saved)\n",
      "epoch: 10200, train_loss: 164.220650, test_loss: 273.914917 (Saved)\n",
      "epoch: 10300, train_loss: 167.120682, test_loss: 269.669830 (Saved)\n",
      "epoch: 10400, train_loss: 159.486259, test_loss: 265.412109 (Saved)\n",
      "epoch: 10500, train_loss: 159.215782, test_loss: 263.412140 (Saved)\n",
      "epoch: 10600, train_loss: 156.659866, test_loss: 261.347290 (Saved)\n",
      "epoch: 10700, train_loss: 152.303116, test_loss: 259.881500 (Saved)\n",
      "epoch: 10800, train_loss: 149.525749, test_loss: 260.092590\n",
      "epoch: 10900, train_loss: 145.963905, test_loss: 256.696686 (Saved)\n",
      "epoch: 11000, train_loss: 143.042084, test_loss: 254.496216 (Saved)\n",
      "epoch: 11100, train_loss: 142.592384, test_loss: 251.833176 (Saved)\n",
      "epoch: 11200, train_loss: 138.284443, test_loss: 250.963928 (Saved)\n",
      "epoch: 11300, train_loss: 138.667961, test_loss: 247.903458 (Saved)\n",
      "epoch: 11400, train_loss: 135.593384, test_loss: 249.998749\n",
      "epoch: 11500, train_loss: 133.744659, test_loss: 248.045227\n",
      "epoch: 11600, train_loss: 129.755970, test_loss: 245.622025 (Saved)\n",
      "epoch: 11700, train_loss: 127.747723, test_loss: 244.518188 (Saved)\n",
      "epoch: 11800, train_loss: 127.576912, test_loss: 240.964783 (Saved)\n",
      "epoch: 11900, train_loss: 127.057625, test_loss: 239.053558 (Saved)\n",
      "epoch: 12000, train_loss: 122.342087, test_loss: 237.655151 (Saved)\n",
      "epoch: 12100, train_loss: 121.449570, test_loss: 234.653320 (Saved)\n",
      "epoch: 12200, train_loss: 119.632137, test_loss: 234.597061 (Saved)\n",
      "epoch: 12300, train_loss: 118.377712, test_loss: 233.140289 (Saved)\n",
      "epoch: 12400, train_loss: 115.115543, test_loss: 231.445480 (Saved)\n",
      "epoch: 12500, train_loss: 117.171513, test_loss: 231.343292 (Saved)\n",
      "epoch: 12600, train_loss: 112.529045, test_loss: 228.916855 (Saved)\n",
      "epoch: 12700, train_loss: 109.608494, test_loss: 227.687805 (Saved)\n",
      "epoch: 12800, train_loss: 110.132118, test_loss: 227.191254 (Saved)\n",
      "epoch: 12900, train_loss: 107.527672, test_loss: 225.210434 (Saved)\n",
      "epoch: 13000, train_loss: 107.016991, test_loss: 223.314789 (Saved)\n",
      "epoch: 13100, train_loss: 105.216644, test_loss: 223.150864 (Saved)\n",
      "epoch: 13200, train_loss: 103.882366, test_loss: 221.773285 (Saved)\n",
      "epoch: 13300, train_loss: 101.940346, test_loss: 220.267273 (Saved)\n",
      "epoch: 13400, train_loss: 99.861023, test_loss: 219.014832 (Saved)\n",
      "epoch: 13500, train_loss: 99.450058, test_loss: 217.844223 (Saved)\n",
      "epoch: 13600, train_loss: 96.954464, test_loss: 216.931595 (Saved)\n",
      "epoch: 13700, train_loss: 95.436775, test_loss: 216.194656 (Saved)\n",
      "epoch: 13800, train_loss: 93.651920, test_loss: 214.845108 (Saved)\n",
      "epoch: 13900, train_loss: 89.895679, test_loss: 213.314240 (Saved)\n",
      "epoch: 14000, train_loss: 90.561867, test_loss: 214.187683\n",
      "epoch: 14100, train_loss: 90.425743, test_loss: 211.465042 (Saved)\n",
      "epoch: 14200, train_loss: 88.101021, test_loss: 212.086182\n",
      "epoch: 14300, train_loss: 87.572876, test_loss: 211.484085\n",
      "epoch: 14400, train_loss: 87.158997, test_loss: 209.828293 (Saved)\n",
      "epoch: 14500, train_loss: 86.323742, test_loss: 210.034180\n",
      "epoch: 14600, train_loss: 84.661510, test_loss: 208.208572 (Saved)\n",
      "epoch: 14700, train_loss: 82.054821, test_loss: 210.427582\n",
      "epoch: 14800, train_loss: 80.797722, test_loss: 209.761353\n",
      "epoch: 14900, train_loss: 78.329111, test_loss: 208.339462\n",
      "epoch: 15000, train_loss: 78.042202, test_loss: 208.913483\n",
      "epoch: 15100, train_loss: 77.908009, test_loss: 208.322098\n",
      "epoch: 15200, train_loss: 76.802471, test_loss: 208.404953\n",
      "epoch: 15300, train_loss: 75.696217, test_loss: 207.960464 (Saved)\n",
      "epoch: 15400, train_loss: 74.563820, test_loss: 206.472839 (Saved)\n",
      "epoch: 15500, train_loss: 74.229809, test_loss: 206.879028\n",
      "epoch: 15600, train_loss: 73.394413, test_loss: 205.320969 (Saved)\n",
      "epoch: 15700, train_loss: 72.934078, test_loss: 205.012161 (Saved)\n",
      "epoch: 15800, train_loss: 70.962257, test_loss: 204.229202 (Saved)\n",
      "epoch: 15900, train_loss: 70.748451, test_loss: 204.372025\n",
      "epoch: 16000, train_loss: 69.995735, test_loss: 202.651031 (Saved)\n",
      "epoch: 16100, train_loss: 69.015850, test_loss: 202.246735 (Saved)\n",
      "epoch: 16200, train_loss: 69.241898, test_loss: 202.046738 (Saved)\n",
      "epoch: 16300, train_loss: 68.278011, test_loss: 202.435394\n",
      "epoch: 16400, train_loss: 66.178633, test_loss: 199.942429 (Saved)\n",
      "epoch: 16500, train_loss: 67.235863, test_loss: 201.355835\n",
      "epoch: 16600, train_loss: 66.975105, test_loss: 200.906891\n",
      "epoch: 16700, train_loss: 63.711725, test_loss: 199.823212 (Saved)\n",
      "epoch: 16800, train_loss: 63.832561, test_loss: 199.137314 (Saved)\n",
      "epoch: 16900, train_loss: 62.520712, test_loss: 199.316940\n",
      "epoch: 17000, train_loss: 63.274897, test_loss: 198.042221 (Saved)\n",
      "epoch: 17100, train_loss: 62.975199, test_loss: 198.187881\n",
      "epoch: 17200, train_loss: 60.863548, test_loss: 198.341995\n",
      "epoch: 17300, train_loss: 60.094067, test_loss: 199.232300\n",
      "epoch: 17400, train_loss: 60.911652, test_loss: 197.201477 (Saved)\n",
      "epoch: 17500, train_loss: 61.421383, test_loss: 197.909210\n",
      "epoch: 17600, train_loss: 59.585909, test_loss: 191.060715 (Saved)\n",
      "epoch: 17700, train_loss: 60.141804, test_loss: 190.654251 (Saved)\n",
      "epoch: 17800, train_loss: 59.007067, test_loss: 190.610794 (Saved)\n",
      "epoch: 17900, train_loss: 58.454529, test_loss: 190.560181 (Saved)\n",
      "epoch: 18000, train_loss: 57.144657, test_loss: 189.433578 (Saved)\n",
      "epoch: 18100, train_loss: 57.838778, test_loss: 189.687607\n",
      "epoch: 18200, train_loss: 56.840273, test_loss: 189.523056\n",
      "epoch: 18300, train_loss: 55.424809, test_loss: 188.948608 (Saved)\n",
      "epoch: 18400, train_loss: 54.843775, test_loss: 189.049911\n",
      "epoch: 18500, train_loss: 55.360210, test_loss: 186.897827 (Saved)\n",
      "epoch: 18600, train_loss: 55.376179, test_loss: 188.081253\n",
      "epoch: 18700, train_loss: 54.170528, test_loss: 188.618347\n",
      "epoch: 18800, train_loss: 53.490038, test_loss: 189.561188\n",
      "epoch: 18900, train_loss: 53.542498, test_loss: 188.541885\n",
      "epoch: 19000, train_loss: 52.973724, test_loss: 187.910309\n",
      "epoch: 19100, train_loss: 52.873680, test_loss: 188.667328\n",
      "epoch: 19200, train_loss: 52.247702, test_loss: 187.587631\n",
      "epoch: 19300, train_loss: 52.136087, test_loss: 187.579254\n",
      "epoch: 19400, train_loss: 51.931307, test_loss: 187.563553\n",
      "epoch: 19500, train_loss: 50.716232, test_loss: 187.446014\n",
      "epoch: 19600, train_loss: 50.622450, test_loss: 187.195145\n",
      "epoch: 19700, train_loss: 51.692547, test_loss: 180.663467 (Saved)\n",
      "epoch: 19800, train_loss: 49.710394, test_loss: 180.718079\n",
      "epoch: 19900, train_loss: 49.453981, test_loss: 182.038193\n",
      "epoch: 20000, train_loss: 49.932571, test_loss: 180.522446 (Saved)\n",
      "epoch: 20100, train_loss: 48.945869, test_loss: 180.340439 (Saved)\n",
      "epoch: 20200, train_loss: 48.706827, test_loss: 180.725922\n",
      "epoch: 20300, train_loss: 48.198524, test_loss: 178.935577 (Saved)\n",
      "epoch: 20400, train_loss: 49.116390, test_loss: 180.177429\n",
      "epoch: 20500, train_loss: 46.976236, test_loss: 178.308228 (Saved)\n",
      "epoch: 20600, train_loss: 47.508986, test_loss: 176.376312 (Saved)\n",
      "epoch: 20700, train_loss: 47.910460, test_loss: 176.889008\n",
      "epoch: 20800, train_loss: 46.324463, test_loss: 176.014542 (Saved)\n",
      "epoch: 20900, train_loss: 46.215494, test_loss: 174.263901 (Saved)\n",
      "epoch: 21000, train_loss: 46.886288, test_loss: 175.506531\n",
      "epoch: 21100, train_loss: 46.316460, test_loss: 174.886398\n",
      "epoch: 21200, train_loss: 45.190264, test_loss: 173.562469 (Saved)\n",
      "epoch: 21300, train_loss: 44.914314, test_loss: 173.873550\n",
      "epoch: 21400, train_loss: 44.114151, test_loss: 173.301117 (Saved)\n",
      "epoch: 21500, train_loss: 43.696196, test_loss: 172.493195 (Saved)\n",
      "epoch: 21600, train_loss: 44.269747, test_loss: 172.106949 (Saved)\n",
      "epoch: 21700, train_loss: 43.122398, test_loss: 172.306778\n",
      "epoch: 21800, train_loss: 43.861349, test_loss: 171.538559 (Saved)\n",
      "epoch: 21900, train_loss: 43.891785, test_loss: 170.948441 (Saved)\n",
      "epoch: 22000, train_loss: 42.982271, test_loss: 172.030624\n",
      "epoch: 22100, train_loss: 43.485535, test_loss: 170.652664 (Saved)\n",
      "epoch: 22200, train_loss: 42.212831, test_loss: 169.939224 (Saved)\n",
      "epoch: 22300, train_loss: 42.615894, test_loss: 170.195312\n",
      "epoch: 22400, train_loss: 41.178654, test_loss: 169.650299 (Saved)\n",
      "epoch: 22500, train_loss: 41.481688, test_loss: 167.929169 (Saved)\n",
      "epoch: 22600, train_loss: 41.604311, test_loss: 167.780502 (Saved)\n",
      "epoch: 22700, train_loss: 41.157183, test_loss: 167.892914\n",
      "epoch: 22800, train_loss: 40.761366, test_loss: 167.061996 (Saved)\n",
      "epoch: 22900, train_loss: 40.708006, test_loss: 166.754547 (Saved)\n",
      "epoch: 23000, train_loss: 39.137854, test_loss: 166.558929 (Saved)\n",
      "epoch: 23100, train_loss: 39.419893, test_loss: 165.613968 (Saved)\n",
      "epoch: 23200, train_loss: 38.738862, test_loss: 165.191025 (Saved)\n",
      "epoch: 23300, train_loss: 39.132318, test_loss: 165.119217 (Saved)\n",
      "epoch: 23400, train_loss: 38.582672, test_loss: 164.081070 (Saved)\n",
      "epoch: 23500, train_loss: 38.872990, test_loss: 164.512924\n",
      "epoch: 23600, train_loss: 37.664296, test_loss: 162.556931 (Saved)\n",
      "epoch: 23700, train_loss: 38.102661, test_loss: 161.467667 (Saved)\n",
      "epoch: 23800, train_loss: 37.325350, test_loss: 160.561661 (Saved)\n",
      "epoch: 23900, train_loss: 37.868692, test_loss: 160.754684\n",
      "epoch: 24000, train_loss: 36.866455, test_loss: 161.443161\n",
      "epoch: 24100, train_loss: 36.324409, test_loss: 160.687866\n",
      "epoch: 24200, train_loss: 36.048666, test_loss: 160.267365 (Saved)\n",
      "epoch: 24300, train_loss: 35.905966, test_loss: 161.534714\n",
      "epoch: 24400, train_loss: 35.579464, test_loss: 154.888672 (Saved)\n",
      "epoch: 24500, train_loss: 34.799546, test_loss: 155.150391\n",
      "epoch: 24600, train_loss: 35.821354, test_loss: 154.118958 (Saved)\n",
      "epoch: 24700, train_loss: 34.831129, test_loss: 154.592026\n",
      "epoch: 24800, train_loss: 34.960218, test_loss: 154.924866\n",
      "epoch: 24900, train_loss: 33.710732, test_loss: 154.373413\n",
      "epoch: 25000, train_loss: 33.785950, test_loss: 155.038834\n",
      "epoch: 25100, train_loss: 34.126041, test_loss: 153.863815 (Saved)\n",
      "epoch: 25200, train_loss: 33.809417, test_loss: 153.545914 (Saved)\n",
      "epoch: 25300, train_loss: 32.655738, test_loss: 153.971069\n",
      "epoch: 25400, train_loss: 33.334147, test_loss: 153.671951\n",
      "epoch: 25500, train_loss: 32.395175, test_loss: 153.948151\n",
      "epoch: 25600, train_loss: 32.636069, test_loss: 153.557724\n",
      "epoch: 25700, train_loss: 32.421419, test_loss: 153.408798 (Saved)\n",
      "epoch: 25800, train_loss: 32.287560, test_loss: 153.543427\n",
      "epoch: 25900, train_loss: 32.067217, test_loss: 153.949524\n",
      "epoch: 26000, train_loss: 31.967772, test_loss: 152.275452 (Saved)\n",
      "epoch: 26100, train_loss: 31.653983, test_loss: 152.316910\n",
      "epoch: 26200, train_loss: 31.295533, test_loss: 152.260513 (Saved)\n",
      "epoch: 26300, train_loss: 31.386992, test_loss: 152.114441 (Saved)\n",
      "epoch: 26400, train_loss: 30.992109, test_loss: 151.943542 (Saved)\n",
      "epoch: 26500, train_loss: 30.560388, test_loss: 152.023590\n",
      "epoch: 26600, train_loss: 30.743043, test_loss: 151.851089 (Saved)\n",
      "epoch: 26700, train_loss: 30.165456, test_loss: 151.427963 (Saved)\n",
      "epoch: 26800, train_loss: 30.127453, test_loss: 151.347641 (Saved)\n",
      "epoch: 26900, train_loss: 29.500840, test_loss: 151.826508\n",
      "epoch: 27000, train_loss: 29.568084, test_loss: 150.773422 (Saved)\n",
      "epoch: 27100, train_loss: 29.056815, test_loss: 150.802917\n",
      "epoch: 27200, train_loss: 28.988659, test_loss: 150.108734 (Saved)\n",
      "epoch: 27300, train_loss: 29.066707, test_loss: 150.093292 (Saved)\n",
      "epoch: 27400, train_loss: 28.894114, test_loss: 149.181854 (Saved)\n",
      "epoch: 27500, train_loss: 29.045343, test_loss: 149.097458 (Saved)\n",
      "epoch: 27600, train_loss: 28.452581, test_loss: 147.589951 (Saved)\n",
      "epoch: 27700, train_loss: 28.291980, test_loss: 148.465897\n",
      "epoch: 27800, train_loss: 27.846060, test_loss: 148.615265\n",
      "epoch: 27900, train_loss: 28.484684, test_loss: 147.309830 (Saved)\n",
      "epoch: 28000, train_loss: 27.582229, test_loss: 142.076889 (Saved)\n",
      "epoch: 28100, train_loss: 28.019500, test_loss: 142.215546\n",
      "epoch: 28200, train_loss: 27.152671, test_loss: 141.751373 (Saved)\n",
      "epoch: 28300, train_loss: 27.298491, test_loss: 140.674210 (Saved)\n",
      "epoch: 28400, train_loss: 27.429542, test_loss: 140.685043\n",
      "epoch: 28500, train_loss: 27.309615, test_loss: 140.828171\n",
      "epoch: 28600, train_loss: 26.551036, test_loss: 139.893066 (Saved)\n",
      "epoch: 28700, train_loss: 26.541923, test_loss: 139.694611 (Saved)\n",
      "epoch: 28800, train_loss: 26.287517, test_loss: 139.486649 (Saved)\n",
      "epoch: 28900, train_loss: 26.785332, test_loss: 139.237701 (Saved)\n",
      "epoch: 29000, train_loss: 25.899649, test_loss: 139.061447 (Saved)\n",
      "epoch: 29100, train_loss: 25.921397, test_loss: 137.620300 (Saved)\n",
      "epoch: 29200, train_loss: 25.472624, test_loss: 138.842560\n",
      "epoch: 29300, train_loss: 25.673784, test_loss: 137.959885\n",
      "epoch: 29400, train_loss: 25.880486, test_loss: 136.618881 (Saved)\n",
      "epoch: 29500, train_loss: 25.257867, test_loss: 136.877243\n",
      "epoch: 29600, train_loss: 25.056724, test_loss: 136.828064\n",
      "epoch: 29700, train_loss: 25.275363, test_loss: 136.643463\n",
      "epoch: 29800, train_loss: 24.774078, test_loss: 136.229752 (Saved)\n",
      "epoch: 29900, train_loss: 24.940359, test_loss: 136.528870\n",
      "epoch: 30000, train_loss: 24.893916, test_loss: 135.217468 (Saved)\n",
      "epoch: 30100, train_loss: 24.078211, test_loss: 135.984360\n",
      "epoch: 30200, train_loss: 24.002668, test_loss: 135.471222\n",
      "epoch: 30300, train_loss: 23.881266, test_loss: 134.141663 (Saved)\n",
      "epoch: 30400, train_loss: 23.945297, test_loss: 136.368622\n",
      "epoch: 30500, train_loss: 23.275830, test_loss: 134.760406\n",
      "epoch: 30600, train_loss: 23.555469, test_loss: 135.280289\n",
      "epoch: 30700, train_loss: 23.532527, test_loss: 134.348328\n",
      "epoch: 30800, train_loss: 23.842004, test_loss: 134.145584\n",
      "epoch: 30900, train_loss: 23.478858, test_loss: 134.949142\n",
      "epoch: 31000, train_loss: 23.483632, test_loss: 134.814651\n",
      "epoch: 31100, train_loss: 23.420950, test_loss: 134.078247 (Saved)\n",
      "epoch: 31200, train_loss: 23.251687, test_loss: 133.870575 (Saved)\n",
      "epoch: 31300, train_loss: 22.847943, test_loss: 133.938934\n",
      "epoch: 31400, train_loss: 22.613584, test_loss: 132.940475 (Saved)\n",
      "epoch: 31500, train_loss: 22.692045, test_loss: 133.584610\n",
      "epoch: 31600, train_loss: 22.848549, test_loss: 133.054077\n",
      "epoch: 31700, train_loss: 22.264389, test_loss: 133.104126\n",
      "epoch: 31800, train_loss: 22.903788, test_loss: 133.371445\n",
      "epoch: 31900, train_loss: 22.480671, test_loss: 133.990204\n",
      "epoch: 32000, train_loss: 22.740541, test_loss: 133.846039\n",
      "epoch: 32100, train_loss: 22.492620, test_loss: 133.392349\n",
      "epoch: 32200, train_loss: 22.028868, test_loss: 133.128021\n",
      "epoch: 32300, train_loss: 21.731088, test_loss: 132.902542 (Saved)\n",
      "epoch: 32400, train_loss: 21.739279, test_loss: 133.127960\n",
      "epoch: 32500, train_loss: 21.603542, test_loss: 133.136612\n",
      "epoch: 32600, train_loss: 21.629389, test_loss: 133.314911\n",
      "epoch: 32700, train_loss: 21.267599, test_loss: 131.792847 (Saved)\n",
      "epoch: 32800, train_loss: 21.420810, test_loss: 127.869797 (Saved)\n",
      "epoch: 32900, train_loss: 21.565778, test_loss: 127.655090 (Saved)\n",
      "epoch: 33000, train_loss: 21.714969, test_loss: 128.174866\n",
      "epoch: 33100, train_loss: 21.378658, test_loss: 128.280243\n",
      "epoch: 33200, train_loss: 21.480378, test_loss: 128.665817\n",
      "epoch: 33300, train_loss: 21.196965, test_loss: 128.522354\n",
      "epoch: 33400, train_loss: 21.442644, test_loss: 128.197968\n",
      "epoch: 33500, train_loss: 21.100963, test_loss: 127.968933\n",
      "epoch: 33600, train_loss: 20.959579, test_loss: 128.535675\n",
      "epoch: 33700, train_loss: 21.024662, test_loss: 128.042694\n",
      "epoch: 33800, train_loss: 20.535830, test_loss: 128.641739\n",
      "epoch: 33900, train_loss: 21.130487, test_loss: 127.651505 (Saved)\n",
      "epoch: 34000, train_loss: 21.221911, test_loss: 128.442474\n",
      "epoch: 34100, train_loss: 20.679156, test_loss: 127.918404\n",
      "epoch: 34200, train_loss: 20.903310, test_loss: 128.459198\n",
      "epoch: 34300, train_loss: 20.765198, test_loss: 128.268875\n",
      "epoch: 34400, train_loss: 20.700333, test_loss: 127.381767 (Saved)\n",
      "epoch: 34500, train_loss: 20.405835, test_loss: 128.183868\n",
      "epoch: 34600, train_loss: 20.183213, test_loss: 127.113258 (Saved)\n",
      "epoch: 34700, train_loss: 20.260295, test_loss: 128.433334\n",
      "epoch: 34800, train_loss: 20.410310, test_loss: 128.693542\n",
      "epoch: 34900, train_loss: 20.015419, test_loss: 127.322838\n",
      "epoch: 35000, train_loss: 20.295354, test_loss: 128.317444\n",
      "epoch: 35100, train_loss: 20.363259, test_loss: 128.466599\n",
      "epoch: 35200, train_loss: 20.490997, test_loss: 127.799042\n",
      "epoch: 35300, train_loss: 19.750338, test_loss: 127.603546\n",
      "epoch: 35400, train_loss: 20.030279, test_loss: 127.080048 (Saved)\n",
      "epoch: 35500, train_loss: 20.315237, test_loss: 128.366669\n",
      "epoch: 35600, train_loss: 19.897388, test_loss: 128.008453\n",
      "epoch: 35700, train_loss: 20.047342, test_loss: 127.204079\n",
      "epoch: 35800, train_loss: 19.878413, test_loss: 127.490204\n",
      "epoch: 35900, train_loss: 19.487588, test_loss: 127.923660\n",
      "epoch: 36000, train_loss: 19.857312, test_loss: 128.817535\n",
      "epoch: 36100, train_loss: 19.711283, test_loss: 128.243484\n",
      "epoch: 36200, train_loss: 19.968243, test_loss: 127.879997\n",
      "epoch: 36300, train_loss: 19.520191, test_loss: 127.002357 (Saved)\n",
      "epoch: 36400, train_loss: 19.171340, test_loss: 127.571472\n",
      "epoch: 36500, train_loss: 19.546792, test_loss: 127.010017\n",
      "epoch: 36600, train_loss: 19.786216, test_loss: 127.592926\n",
      "epoch: 36700, train_loss: 19.277274, test_loss: 127.624969\n",
      "epoch: 36800, train_loss: 19.359522, test_loss: 127.406006\n",
      "epoch: 36900, train_loss: 19.489082, test_loss: 128.616074\n",
      "epoch: 37000, train_loss: 19.439003, test_loss: 127.510292\n",
      "epoch: 37100, train_loss: 19.040429, test_loss: 127.381142\n",
      "epoch: 37200, train_loss: 19.238784, test_loss: 127.645096\n",
      "epoch: 37300, train_loss: 18.658948, test_loss: 127.926636\n",
      "epoch: 37400, train_loss: 19.308034, test_loss: 127.976578\n",
      "epoch: 37500, train_loss: 18.954782, test_loss: 128.527664\n",
      "epoch: 37600, train_loss: 18.557057, test_loss: 127.774338\n",
      "epoch: 37700, train_loss: 18.992301, test_loss: 127.576477\n",
      "epoch: 37800, train_loss: 18.893032, test_loss: 127.404404\n",
      "epoch: 37900, train_loss: 18.696819, test_loss: 128.464340\n",
      "epoch: 38000, train_loss: 18.690949, test_loss: 127.933006\n",
      "epoch: 38100, train_loss: 18.386927, test_loss: 127.816978\n",
      "epoch: 38200, train_loss: 18.812995, test_loss: 127.409149\n",
      "epoch: 38300, train_loss: 18.379610, test_loss: 127.160957\n",
      "epoch: 38400, train_loss: 18.550931, test_loss: 127.300415\n",
      "epoch: 38500, train_loss: 18.523797, test_loss: 127.732994\n",
      "epoch: 38600, train_loss: 18.732206, test_loss: 127.797760\n",
      "epoch: 38700, train_loss: 18.274912, test_loss: 125.943665 (Saved)\n",
      "epoch: 38800, train_loss: 18.619574, test_loss: 125.789055 (Saved)\n",
      "epoch: 38900, train_loss: 18.016661, test_loss: 127.397095\n",
      "epoch: 39000, train_loss: 18.438329, test_loss: 126.928543\n",
      "epoch: 39100, train_loss: 18.525917, test_loss: 127.284645\n",
      "epoch: 39200, train_loss: 18.551605, test_loss: 126.854530\n",
      "epoch: 39300, train_loss: 18.230772, test_loss: 126.363892\n",
      "epoch: 39400, train_loss: 18.357570, test_loss: 125.764946 (Saved)\n",
      "epoch: 39500, train_loss: 18.443217, test_loss: 126.532242\n",
      "epoch: 39600, train_loss: 18.560115, test_loss: 128.091049\n",
      "epoch: 39700, train_loss: 18.625636, test_loss: 125.380592 (Saved)\n",
      "epoch: 39800, train_loss: 18.260674, test_loss: 126.505455\n",
      "epoch: 39900, train_loss: 17.938405, test_loss: 125.981750\n",
      "epoch: 40000, train_loss: 18.184702, test_loss: 125.851799\n",
      "epoch: 40100, train_loss: 18.291863, test_loss: 126.019386\n",
      "epoch: 40200, train_loss: 18.246175, test_loss: 125.899513\n",
      "epoch: 40300, train_loss: 18.057389, test_loss: 125.267204 (Saved)\n",
      "epoch: 40400, train_loss: 17.743619, test_loss: 125.164093 (Saved)\n",
      "epoch: 40500, train_loss: 17.780890, test_loss: 125.287865\n",
      "epoch: 40600, train_loss: 18.008155, test_loss: 125.701477\n",
      "epoch: 40700, train_loss: 18.089374, test_loss: 125.439896\n",
      "epoch: 40800, train_loss: 17.670602, test_loss: 125.552643\n",
      "epoch: 40900, train_loss: 17.784091, test_loss: 124.818596 (Saved)\n",
      "epoch: 41000, train_loss: 17.905803, test_loss: 124.947571\n",
      "epoch: 41100, train_loss: 17.758588, test_loss: 124.947060\n",
      "epoch: 41200, train_loss: 17.925953, test_loss: 125.145828\n",
      "epoch: 41300, train_loss: 17.991508, test_loss: 122.819321 (Saved)\n",
      "epoch: 41400, train_loss: 17.540844, test_loss: 123.910248\n",
      "epoch: 41500, train_loss: 17.863027, test_loss: 123.986076\n",
      "epoch: 41600, train_loss: 17.782269, test_loss: 123.835510\n",
      "epoch: 41700, train_loss: 17.213157, test_loss: 123.566757\n",
      "epoch: 41800, train_loss: 17.384880, test_loss: 122.933365\n",
      "epoch: 41900, train_loss: 17.511387, test_loss: 122.957573\n",
      "epoch: 42000, train_loss: 17.215892, test_loss: 123.284096\n",
      "epoch: 42100, train_loss: 17.366094, test_loss: 122.534866 (Saved)\n",
      "epoch: 42200, train_loss: 17.760122, test_loss: 121.694565 (Saved)\n",
      "epoch: 42300, train_loss: 17.426095, test_loss: 121.760437\n",
      "epoch: 42400, train_loss: 17.627329, test_loss: 122.790993\n",
      "epoch: 42500, train_loss: 17.029877, test_loss: 122.442650\n",
      "epoch: 42600, train_loss: 17.384921, test_loss: 122.055130\n",
      "epoch: 42700, train_loss: 17.518407, test_loss: 122.194115\n",
      "epoch: 42800, train_loss: 17.400516, test_loss: 120.973099 (Saved)\n",
      "epoch: 42900, train_loss: 17.442563, test_loss: 121.540688\n",
      "epoch: 43000, train_loss: 17.154863, test_loss: 122.293739\n",
      "epoch: 43100, train_loss: 16.866614, test_loss: 121.100258\n",
      "epoch: 43200, train_loss: 16.995852, test_loss: 121.187370\n",
      "epoch: 43300, train_loss: 17.157362, test_loss: 120.233131 (Saved)\n",
      "epoch: 43400, train_loss: 17.087152, test_loss: 121.251045\n",
      "epoch: 43500, train_loss: 17.400283, test_loss: 120.379883\n",
      "epoch: 43600, train_loss: 17.017226, test_loss: 120.468407\n",
      "epoch: 43700, train_loss: 17.113535, test_loss: 120.472092\n",
      "epoch: 43800, train_loss: 16.927960, test_loss: 120.405792\n",
      "epoch: 43900, train_loss: 17.065894, test_loss: 119.847778 (Saved)\n",
      "epoch: 44000, train_loss: 16.854652, test_loss: 119.879921\n",
      "epoch: 44100, train_loss: 17.021055, test_loss: 120.001312\n",
      "epoch: 44200, train_loss: 16.969299, test_loss: 120.219780\n",
      "epoch: 44300, train_loss: 16.889144, test_loss: 119.342598 (Saved)\n",
      "epoch: 44400, train_loss: 17.224514, test_loss: 119.683495\n",
      "epoch: 44500, train_loss: 16.971517, test_loss: 118.425385 (Saved)\n",
      "epoch: 44600, train_loss: 16.813673, test_loss: 118.811378\n",
      "epoch: 44700, train_loss: 16.898569, test_loss: 119.790070\n",
      "epoch: 44800, train_loss: 16.728627, test_loss: 118.039688 (Saved)\n",
      "epoch: 44900, train_loss: 17.162707, test_loss: 119.763580\n",
      "epoch: 45000, train_loss: 16.766522, test_loss: 118.557838\n",
      "epoch: 45100, train_loss: 16.472766, test_loss: 117.859154 (Saved)\n",
      "epoch: 45200, train_loss: 16.689829, test_loss: 118.821594\n",
      "epoch: 45300, train_loss: 16.691758, test_loss: 118.857216\n",
      "epoch: 45400, train_loss: 16.603999, test_loss: 118.527649\n",
      "epoch: 45500, train_loss: 16.357914, test_loss: 118.118744\n",
      "epoch: 45600, train_loss: 16.955654, test_loss: 118.875282\n",
      "epoch: 45700, train_loss: 16.489918, test_loss: 117.770035 (Saved)\n",
      "epoch: 45800, train_loss: 16.658798, test_loss: 117.485619 (Saved)\n",
      "epoch: 45900, train_loss: 16.248295, test_loss: 116.739624 (Saved)\n",
      "epoch: 46000, train_loss: 16.608995, test_loss: 117.834572\n",
      "epoch: 46100, train_loss: 16.931795, test_loss: 118.022156\n",
      "epoch: 46200, train_loss: 16.223639, test_loss: 118.523804\n",
      "epoch: 46300, train_loss: 16.311038, test_loss: 117.493889\n",
      "epoch: 46400, train_loss: 16.423330, test_loss: 117.865082\n",
      "epoch: 46500, train_loss: 16.353726, test_loss: 117.429619\n",
      "epoch: 46600, train_loss: 16.241673, test_loss: 117.803711\n",
      "epoch: 46700, train_loss: 16.261027, test_loss: 116.416618 (Saved)\n",
      "epoch: 46800, train_loss: 16.328900, test_loss: 116.373863 (Saved)\n",
      "epoch: 46900, train_loss: 16.310419, test_loss: 118.079727\n",
      "epoch: 47000, train_loss: 16.542401, test_loss: 117.361122\n",
      "epoch: 47100, train_loss: 16.246346, test_loss: 117.517677\n",
      "epoch: 47200, train_loss: 15.819289, test_loss: 116.230179 (Saved)\n",
      "epoch: 47300, train_loss: 16.264898, test_loss: 116.632980\n",
      "epoch: 47400, train_loss: 16.315077, test_loss: 116.334984\n",
      "epoch: 47500, train_loss: 16.528935, test_loss: 118.462311\n",
      "epoch: 47600, train_loss: 16.183112, test_loss: 116.242538\n",
      "epoch: 47700, train_loss: 16.317013, test_loss: 116.841225\n",
      "epoch: 47800, train_loss: 15.947585, test_loss: 116.125633 (Saved)\n",
      "epoch: 47900, train_loss: 16.177109, test_loss: 116.599831\n",
      "epoch: 48000, train_loss: 16.034451, test_loss: 115.480972 (Saved)\n",
      "epoch: 48100, train_loss: 16.396299, test_loss: 117.073883\n",
      "epoch: 48200, train_loss: 16.174513, test_loss: 115.507156\n",
      "epoch: 48300, train_loss: 16.057059, test_loss: 115.371597 (Saved)\n",
      "epoch: 48400, train_loss: 16.026141, test_loss: 116.085701\n",
      "epoch: 48500, train_loss: 16.085786, test_loss: 116.095398\n",
      "epoch: 48600, train_loss: 16.036595, test_loss: 115.276611 (Saved)\n",
      "epoch: 48700, train_loss: 16.230889, test_loss: 115.482857\n",
      "epoch: 48800, train_loss: 15.997719, test_loss: 115.511398\n",
      "epoch: 48900, train_loss: 15.972470, test_loss: 115.421333\n",
      "epoch: 49000, train_loss: 16.048540, test_loss: 115.523926\n",
      "epoch: 49100, train_loss: 15.642844, test_loss: 116.329773\n",
      "epoch: 49200, train_loss: 16.045639, test_loss: 114.968353 (Saved)\n",
      "epoch: 49300, train_loss: 15.819796, test_loss: 114.960571 (Saved)\n",
      "epoch: 49400, train_loss: 15.594499, test_loss: 115.769196\n",
      "epoch: 49500, train_loss: 15.697238, test_loss: 115.001045\n",
      "epoch: 49600, train_loss: 15.840740, test_loss: 115.161224\n",
      "epoch: 49700, train_loss: 15.798913, test_loss: 114.276428 (Saved)\n",
      "epoch: 49800, train_loss: 15.862854, test_loss: 115.157883\n",
      "epoch: 49900, train_loss: 15.443397, test_loss: 115.657089\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Saving ONNX model: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Failed to save ONNX model: /home/icar/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Module onnx is not installed!\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'cm2pixel_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 450.0000, -150.0000],\n",
      "        [ 789.1151,  131.5192],\n",
      "        [  25.0000,  -50.0000],\n",
      "        [ 848.5281, -848.5281],\n",
      "        [ 150.0000,  100.0000],\n",
      "        [ 447.2136,  223.6068],\n",
      "        [ 350.0000, -100.0000],\n",
      "        [ 100.0000, -100.0000],\n",
      "        [   0.0000,  100.0000],\n",
      "        [ 416.0251, -277.3501],\n",
      "        [ 447.2136, -223.6068],\n",
      "        [ 450.0000, -100.0000],\n",
      "        [ 350.0000, -250.0000],\n",
      "        [ 565.6854,  565.6854],\n",
      "        [ 350.0000, -300.0000],\n",
      "        [ 200.0000, -100.0000],\n",
      "        [ 998.4603,  665.6403],\n",
      "        [ 350.0000, -200.0000],\n",
      "        [  75.0000,  100.0000],\n",
      "        [  75.0000,  150.0000],\n",
      "        [ 350.0000, -150.0000],\n",
      "        [ 200.0000,  150.0000],\n",
      "        [   0.0000,  -25.0000],\n",
      "        [  75.0000,  -50.0000],\n",
      "        [ 450.0000, -250.0000],\n",
      "        [ 100.0000, -200.0000],\n",
      "        [ 758.9467,  252.9822],\n",
      "        [  75.0000,    0.0000],\n",
      "        [ 250.0000, -150.0000]], device='cuda:0')\n",
      "y\n",
      "tensor([[ 816.,  260.],\n",
      "        [ 542.,  228.],\n",
      "        [ 874.,  594.],\n",
      "        [1220.,  214.],\n",
      "        [ 396.,  394.],\n",
      "        [ 378.,  274.],\n",
      "        [ 782.,  286.],\n",
      "        [ 948.,  438.],\n",
      "        [  90.,  690.],\n",
      "        [ 986.,  272.],\n",
      "        [ 904.,  266.],\n",
      "        [ 756.,  262.],\n",
      "        [ 996.,  280.],\n",
      "        [ 100.,  256.],\n",
      "        [1064.,  278.],\n",
      "        [ 852.,  348.],\n",
      "        [ 246.,  220.],\n",
      "        [ 924.,  284.],\n",
      "        [ 300.,  486.],\n",
      "        [ 138.,  488.],\n",
      "        [ 852.,  286.],\n",
      "        [ 332.,  358.],\n",
      "        [ 780.,  690.],\n",
      "        [ 816.,  478.],\n",
      "        [ 934.,  260.],\n",
      "        [1258.,  430.],\n",
      "        [ 450.,  232.],\n",
      "        [ 640.,  482.],\n",
      "        [ 910.,  324.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[ 815.6083,  264.3934],\n",
      "        [ 536.1469,  221.5420],\n",
      "        [ 897.9988,  594.7275],\n",
      "        [1196.5262,  214.3218],\n",
      "        [ 407.3254,  391.1531],\n",
      "        [ 380.4035,  272.0204],\n",
      "        [ 771.3364,  288.2296],\n",
      "        [ 945.7410,  437.3868],\n",
      "        [ 140.6951,  691.8108],\n",
      "        [ 977.6650,  269.8150],\n",
      "        [ 903.8911,  263.8050],\n",
      "        [ 756.5709,  265.2458],\n",
      "        [ 983.1304,  284.8544],\n",
      "        [ 115.1759,  258.4509],\n",
      "        [1061.8424,  283.4113],\n",
      "        [ 864.2936,  348.4946],\n",
      "        [ 237.9399,  221.0038],\n",
      "        [ 904.9238,  286.3299],\n",
      "        [ 313.2870,  492.0794],\n",
      "        [ 151.7207,  490.6892],\n",
      "        [ 835.0283,  287.4161],\n",
      "        [ 347.1847,  358.4399],\n",
      "        [ 781.8406,  686.0546],\n",
      "        [ 805.7792,  482.8193],\n",
      "        [ 934.7810,  262.8230],\n",
      "        [1268.8721,  429.9367],\n",
      "        [ 438.9157,  228.2061],\n",
      "        [ 648.4033,  486.7392],\n",
      "        [ 908.8158,  321.0009]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[  0.3917,  -4.3934],\n",
      "        [  5.8531,   6.4580],\n",
      "        [-23.9988,  -0.7275],\n",
      "        [ 23.4738,  -0.3218],\n",
      "        [-11.3254,   2.8469],\n",
      "        [ -2.4035,   1.9796],\n",
      "        [ 10.6636,  -2.2296],\n",
      "        [  2.2590,   0.6132],\n",
      "        [-50.6951,  -1.8108],\n",
      "        [  8.3350,   2.1850],\n",
      "        [  0.1089,   2.1950],\n",
      "        [ -0.5709,  -3.2458],\n",
      "        [ 12.8696,  -4.8544],\n",
      "        [-15.1759,  -2.4509],\n",
      "        [  2.1576,  -5.4113],\n",
      "        [-12.2936,  -0.4946],\n",
      "        [  8.0601,  -1.0038],\n",
      "        [ 19.0762,  -2.3299],\n",
      "        [-13.2870,  -6.0794],\n",
      "        [-13.7207,  -2.6892],\n",
      "        [ 16.9717,  -1.4161],\n",
      "        [-15.1847,  -0.4399],\n",
      "        [ -1.8406,   3.9454],\n",
      "        [ 10.2208,  -4.8193],\n",
      "        [ -0.7810,  -2.8230],\n",
      "        [-10.8721,   0.0633],\n",
      "        [ 11.0843,   3.7939],\n",
      "        [ -8.4033,  -4.7392],\n",
      "        [  1.1842,   2.9991]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
