{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 19/09/2023 at 23:39:39.\n"
     ]
    }
   ],
   "source": [
    "print('This notebook was run on ' + time.strftime(\"%d/%m/%Y\") + ' at ' + time.strftime(\"%H:%M:%S\") + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/protanjung/icar-ng-data/dataset\n",
      "Dataset file: dataset.csv\n",
      "Dataset path: /home/protanjung/icar-ng-data/dataset/dataset.csv\n",
      "Model directory: /home/protanjung/icar-ng-data/model\n",
      "Model file: cm2pixel_model.pt\n",
      "Model path: /home/protanjung/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'dataset')\n",
    "print('Dataset directory: {}'.format(dataset_directory))\n",
    "\n",
    "dataset_file = 'dataset.csv'\n",
    "print('Dataset file: {}'.format(dataset_file))\n",
    "\n",
    "dataset_path = os.path.join(dataset_directory, dataset_file)\n",
    "print('Dataset path: {}'.format(dataset_path))\n",
    "\n",
    "# ======================================\n",
    "\n",
    "model_directory = os.path.join(os.getenv('HOME'), 'icar-ng-data', 'model')\n",
    "print('Model directory: {}'.format(model_directory))\n",
    "\n",
    "model_file = 'cm2pixel_model.pt'\n",
    "print('Model file: {}'.format(model_file))\n",
    "\n",
    "model_path = os.path.join(model_directory, model_file)\n",
    "print('Model path: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether dataset file exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print('Dataset file does not exist: {}'.format(dataset_path))\n",
    "    exit()\n",
    "\n",
    "# Check whether model directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    print('Created model directory: {}'.format(model_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_x: tensor([   0., -400.]), max_x: tensor([300., 450.])\n",
      "min_y: tensor([ 60., 274.]), max_y: tensor([1252.,  484.])\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(dataset_path)\n",
    "df_train = df_raw.sample(frac=0.8)\n",
    "df_test = df_raw.drop(df_train.index)\n",
    "\n",
    "min_x = np.min(df_train.iloc[:, 2:4].values, axis=0)\n",
    "min_x = torch.tensor(min_x, dtype=torch.float32)\n",
    "max_x = np.max(df_train.iloc[:, 2:4].values, axis=0)\n",
    "max_x = torch.tensor(max_x, dtype=torch.float32)\n",
    "min_y = np.min(df_train.iloc[:, 0:2].values, axis=0)\n",
    "min_y = torch.tensor(min_y, dtype=torch.float32)\n",
    "max_y = np.max(df_train.iloc[:, 0:2].values, axis=0)\n",
    "max_y = torch.tensor(max_y, dtype=torch.float32)\n",
    "\n",
    "print('min_x: {}, max_x: {}'.format(min_x, max_x))\n",
    "print('min_y: {}, max_y: {}'.format(min_y, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        x = self.dataframe.iloc[:, 2:4].values\n",
    "        y = self.dataframe.iloc[:, 0:2].values\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train: 73\n",
      "dataset_test: 18\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ForwardDataset(df_train)\n",
    "dataset_test = ForwardDataset(df_test)\n",
    "print('dataset_train: {}'.format(len(dataset_train)))\n",
    "print('dataset_test: {}'.format(len(dataset_test)))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, min_x, max_x, min_y, max_y):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "        self.min_y = min_y\n",
    "        self.max_y = max_y\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.fc5 = nn.Linear(16, 4)\n",
    "        self.fc6 = nn.Linear(4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.min_x) / (self.max_x - self.min_x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = x * (self.max_y - self.min_y) + self.min_y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/protanjung/icar-ng-data/model/cm2pixel_model.pt\n",
      "Loaded model: /home/protanjung/icar-ng-data/model/cm2pixel_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(2, 2, min_x.to(device), max_x.to(device), min_y.to(device), max_y.to(device)).to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        print('Loading model: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Loaded model: {}'.format(model_path))\n",
    "    except BaseException as e:\n",
    "        print('Failed to load model: {}'.format(model_path))\n",
    "        print(e)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 4443.757324, test_loss: 680.010071 (Saved)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, train_loss: 3562.234375, test_loss: 738.002197\n",
      "epoch: 200, train_loss: 3220.356140, test_loss: 643.224792 (Saved)\n",
      "epoch: 300, train_loss: 2749.510406, test_loss: 715.793152\n",
      "epoch: 400, train_loss: 5193.102661, test_loss: 628.590271 (Saved)\n",
      "epoch: 500, train_loss: 3500.878174, test_loss: 681.781555\n",
      "epoch: 600, train_loss: 4205.826660, test_loss: 738.569885\n",
      "epoch: 700, train_loss: 2649.658936, test_loss: 688.550537\n",
      "epoch: 800, train_loss: 7423.674805, test_loss: 702.905151\n",
      "epoch: 900, train_loss: 4612.421265, test_loss: 767.237610\n",
      "epoch: 1000, train_loss: 2926.886963, test_loss: 738.013000\n",
      "epoch: 1100, train_loss: 4519.364990, test_loss: 715.885864\n",
      "epoch: 1200, train_loss: 3284.110962, test_loss: 713.218872\n",
      "epoch: 1300, train_loss: 3595.769409, test_loss: 682.938782\n",
      "epoch: 1400, train_loss: 5339.396729, test_loss: 703.584839\n",
      "epoch: 1500, train_loss: 4980.088379, test_loss: 766.168640\n",
      "epoch: 1600, train_loss: 3426.385254, test_loss: 766.541260\n",
      "epoch: 1700, train_loss: 3724.228760, test_loss: 710.813354\n",
      "epoch: 1800, train_loss: 3220.829468, test_loss: 769.714172\n",
      "epoch: 1900, train_loss: 4380.906128, test_loss: 787.847534\n",
      "epoch: 2000, train_loss: 3745.470093, test_loss: 786.447083\n",
      "epoch: 2100, train_loss: 3767.473755, test_loss: 792.732605\n",
      "epoch: 2200, train_loss: 5172.663452, test_loss: 789.606323\n",
      "epoch: 2300, train_loss: 2696.518494, test_loss: 791.483643\n",
      "epoch: 2400, train_loss: 3891.004883, test_loss: 789.694214\n",
      "epoch: 2500, train_loss: 2766.470642, test_loss: 835.011292\n",
      "epoch: 2600, train_loss: 4316.688354, test_loss: 848.765564\n",
      "epoch: 2700, train_loss: 2507.011292, test_loss: 819.256531\n",
      "epoch: 2800, train_loss: 3309.509766, test_loss: 827.579651\n",
      "epoch: 2900, train_loss: 2681.335022, test_loss: 871.963562\n",
      "epoch: 3000, train_loss: 3206.807495, test_loss: 797.343872\n",
      "epoch: 3100, train_loss: 2766.128723, test_loss: 869.438782\n",
      "epoch: 3200, train_loss: 4451.755371, test_loss: 862.345825\n",
      "epoch: 3300, train_loss: 3281.642700, test_loss: 820.357849\n",
      "epoch: 3400, train_loss: 2818.196838, test_loss: 849.870667\n",
      "epoch: 3500, train_loss: 3412.571167, test_loss: 835.109253\n",
      "epoch: 3600, train_loss: 3185.516357, test_loss: 854.892090\n",
      "epoch: 3700, train_loss: 2478.187622, test_loss: 865.955200\n",
      "epoch: 3800, train_loss: 3287.677002, test_loss: 875.827576\n",
      "epoch: 3900, train_loss: 5216.389404, test_loss: 864.707214\n",
      "epoch: 4000, train_loss: 3745.770264, test_loss: 853.333130\n",
      "epoch: 4100, train_loss: 3095.689697, test_loss: 924.832703\n",
      "epoch: 4200, train_loss: 4075.818115, test_loss: 857.878723\n",
      "epoch: 4300, train_loss: 4164.785278, test_loss: 908.347900\n",
      "epoch: 4400, train_loss: 2525.714844, test_loss: 877.897095\n",
      "epoch: 4500, train_loss: 3283.731689, test_loss: 868.561951\n",
      "epoch: 4600, train_loss: 2856.396362, test_loss: 937.418091\n",
      "epoch: 4700, train_loss: 2696.703796, test_loss: 881.521118\n",
      "epoch: 4800, train_loss: 4931.285156, test_loss: 903.177979\n",
      "epoch: 4900, train_loss: 4466.104004, test_loss: 898.903381\n",
      "epoch: 5000, train_loss: 2368.294495, test_loss: 921.180481\n",
      "epoch: 5100, train_loss: 2588.062500, test_loss: 929.304565\n",
      "epoch: 5200, train_loss: 2706.992920, test_loss: 890.463440\n",
      "epoch: 5300, train_loss: 2607.115967, test_loss: 904.916321\n",
      "epoch: 5400, train_loss: 4405.468506, test_loss: 891.580994\n",
      "epoch: 5500, train_loss: 4241.745361, test_loss: 931.044739\n",
      "epoch: 5600, train_loss: 2270.277161, test_loss: 932.971130\n",
      "epoch: 5700, train_loss: 2380.756897, test_loss: 961.791687\n",
      "epoch: 5800, train_loss: 3724.905762, test_loss: 911.575989\n",
      "epoch: 5900, train_loss: 2930.455566, test_loss: 931.121338\n",
      "epoch: 6000, train_loss: 2578.204590, test_loss: 949.055359\n",
      "epoch: 6100, train_loss: 2431.731384, test_loss: 973.130005\n",
      "epoch: 6200, train_loss: 2205.494568, test_loss: 952.977966\n",
      "epoch: 6300, train_loss: 2568.536682, test_loss: 921.880859\n",
      "epoch: 6400, train_loss: 5367.774414, test_loss: 894.831909\n",
      "epoch: 6500, train_loss: 2771.292358, test_loss: 934.052307\n",
      "epoch: 6600, train_loss: 2553.087524, test_loss: 991.059143\n",
      "epoch: 6700, train_loss: 2970.393921, test_loss: 978.929688\n",
      "epoch: 6800, train_loss: 2216.618347, test_loss: 932.901062\n",
      "epoch: 6900, train_loss: 2163.209229, test_loss: 909.019348\n",
      "epoch: 7000, train_loss: 2813.537231, test_loss: 965.713440\n",
      "epoch: 7100, train_loss: 3211.933594, test_loss: 961.011292\n",
      "epoch: 7200, train_loss: 2946.523804, test_loss: 968.543823\n",
      "epoch: 7300, train_loss: 2540.398682, test_loss: 963.043213\n",
      "epoch: 7400, train_loss: 2644.024536, test_loss: 944.812927\n",
      "epoch: 7500, train_loss: 2739.135864, test_loss: 924.930969\n",
      "epoch: 7600, train_loss: 2393.226074, test_loss: 944.823242\n",
      "epoch: 7700, train_loss: 2678.998413, test_loss: 992.272888\n",
      "epoch: 7800, train_loss: 3197.122437, test_loss: 945.012024\n",
      "epoch: 7900, train_loss: 2333.747620, test_loss: 942.692810\n",
      "epoch: 8000, train_loss: 2418.621155, test_loss: 942.945862\n",
      "epoch: 8100, train_loss: 2192.856628, test_loss: 1007.574646\n",
      "epoch: 8200, train_loss: 4942.368896, test_loss: 1017.286804\n",
      "epoch: 8300, train_loss: 3446.035889, test_loss: 962.775391\n",
      "epoch: 8400, train_loss: 2201.887756, test_loss: 957.274292\n",
      "epoch: 8500, train_loss: 4604.017578, test_loss: 889.231262\n",
      "epoch: 8600, train_loss: 3289.158691, test_loss: 951.263550\n",
      "epoch: 8700, train_loss: 2918.907471, test_loss: 938.714844\n",
      "epoch: 8800, train_loss: 2462.389648, test_loss: 998.140442\n",
      "epoch: 8900, train_loss: 3036.606934, test_loss: 968.011963\n",
      "epoch: 9000, train_loss: 2404.530151, test_loss: 1006.294373\n",
      "epoch: 9100, train_loss: 2339.465393, test_loss: 975.092468\n",
      "epoch: 9200, train_loss: 3274.001831, test_loss: 962.966125\n",
      "epoch: 9300, train_loss: 3350.395752, test_loss: 988.971680\n",
      "epoch: 9400, train_loss: 3327.390015, test_loss: 951.558289\n",
      "epoch: 9500, train_loss: 3187.258179, test_loss: 939.707275\n",
      "epoch: 9600, train_loss: 2402.009277, test_loss: 931.464844\n",
      "epoch: 9700, train_loss: 2644.379150, test_loss: 914.396729\n",
      "epoch: 9800, train_loss: 4123.369324, test_loss: 914.761292\n",
      "epoch: 9900, train_loss: 2767.399536, test_loss: 979.820190\n",
      "epoch: 10000, train_loss: 3160.507202, test_loss: 933.907043\n",
      "epoch: 10100, train_loss: 1623.893341, test_loss: 992.222656\n",
      "epoch: 10200, train_loss: 2775.680542, test_loss: 926.022888\n",
      "epoch: 10300, train_loss: 2198.384155, test_loss: 941.245911\n",
      "epoch: 10400, train_loss: 2281.708435, test_loss: 921.184265\n",
      "epoch: 10500, train_loss: 2283.627747, test_loss: 989.470581\n",
      "epoch: 10600, train_loss: 1906.167969, test_loss: 968.450195\n",
      "epoch: 10700, train_loss: 3096.095459, test_loss: 962.232666\n",
      "epoch: 10800, train_loss: 2254.880554, test_loss: 954.497864\n",
      "epoch: 10900, train_loss: 2446.297241, test_loss: 995.092224\n",
      "epoch: 11000, train_loss: 2100.988403, test_loss: 863.403564\n",
      "epoch: 11100, train_loss: 1999.171936, test_loss: 915.846375\n",
      "epoch: 11200, train_loss: 1872.883423, test_loss: 968.180237\n",
      "epoch: 11300, train_loss: 2171.624023, test_loss: 954.392883\n",
      "epoch: 11400, train_loss: 1629.932465, test_loss: 879.791138\n",
      "epoch: 11500, train_loss: 2458.727783, test_loss: 918.319031\n",
      "epoch: 11600, train_loss: 3043.548096, test_loss: 922.630981\n",
      "epoch: 11700, train_loss: 2381.048584, test_loss: 925.878235\n",
      "epoch: 11800, train_loss: 1508.127319, test_loss: 895.013367\n",
      "epoch: 11900, train_loss: 1952.341248, test_loss: 854.028076\n",
      "epoch: 12000, train_loss: 2104.046814, test_loss: 860.681396\n",
      "epoch: 12100, train_loss: 2452.093140, test_loss: 838.222900\n",
      "epoch: 12200, train_loss: 3273.813782, test_loss: 796.509094\n",
      "epoch: 12300, train_loss: 2274.055176, test_loss: 827.840637\n",
      "epoch: 12400, train_loss: 2319.709351, test_loss: 856.663940\n",
      "epoch: 12500, train_loss: 1967.546265, test_loss: 860.201965\n",
      "epoch: 12600, train_loss: 1985.353821, test_loss: 879.412231\n",
      "epoch: 12700, train_loss: 1871.012390, test_loss: 936.227417\n",
      "epoch: 12800, train_loss: 1723.568481, test_loss: 884.700012\n",
      "epoch: 12900, train_loss: 3130.695923, test_loss: 909.441467\n",
      "epoch: 13000, train_loss: 2337.107666, test_loss: 848.406677\n",
      "epoch: 13100, train_loss: 1939.150208, test_loss: 817.722839\n",
      "epoch: 13200, train_loss: 2309.308838, test_loss: 919.133484\n",
      "epoch: 13300, train_loss: 2144.237671, test_loss: 800.609985\n",
      "epoch: 13400, train_loss: 2383.171875, test_loss: 807.046082\n",
      "epoch: 13500, train_loss: 1670.373962, test_loss: 857.754395\n",
      "epoch: 13600, train_loss: 2782.542969, test_loss: 788.932922\n",
      "epoch: 13700, train_loss: 1926.075500, test_loss: 813.885620\n",
      "epoch: 13800, train_loss: 2126.017334, test_loss: 797.765503\n",
      "epoch: 13900, train_loss: 1709.599243, test_loss: 781.909973\n",
      "epoch: 14000, train_loss: 1913.008057, test_loss: 814.308228\n",
      "epoch: 14100, train_loss: 1967.774963, test_loss: 792.606445\n",
      "epoch: 14200, train_loss: 1815.845459, test_loss: 799.107727\n",
      "epoch: 14300, train_loss: 2771.978088, test_loss: 808.063782\n",
      "epoch: 14400, train_loss: 2038.216125, test_loss: 807.307068\n",
      "epoch: 14500, train_loss: 1928.808105, test_loss: 810.454407\n",
      "epoch: 14600, train_loss: 2013.666260, test_loss: 790.835876\n",
      "epoch: 14700, train_loss: 1863.915588, test_loss: 756.966675\n",
      "epoch: 14800, train_loss: 1871.864563, test_loss: 738.708130\n",
      "epoch: 14900, train_loss: 3022.160400, test_loss: 751.075134\n",
      "epoch: 15000, train_loss: 2527.048767, test_loss: 817.817749\n",
      "epoch: 15100, train_loss: 1956.069702, test_loss: 780.506165\n",
      "epoch: 15200, train_loss: 2129.949463, test_loss: 678.078186\n",
      "epoch: 15300, train_loss: 1514.919983, test_loss: 775.571106\n",
      "epoch: 15400, train_loss: 1690.819519, test_loss: 741.035583\n",
      "epoch: 15500, train_loss: 1368.445892, test_loss: 743.274048\n",
      "epoch: 15600, train_loss: 2049.548340, test_loss: 787.999451\n",
      "epoch: 15700, train_loss: 2217.810181, test_loss: 798.160034\n",
      "epoch: 15800, train_loss: 1860.965576, test_loss: 751.762878\n",
      "epoch: 15900, train_loss: 1530.237488, test_loss: 724.486023\n",
      "epoch: 16000, train_loss: 1682.587952, test_loss: 752.708435\n",
      "epoch: 16100, train_loss: 1519.333069, test_loss: 679.897705\n",
      "epoch: 16200, train_loss: 1616.569580, test_loss: 800.634216\n",
      "epoch: 16300, train_loss: 1754.045044, test_loss: 776.618408\n",
      "epoch: 16400, train_loss: 2138.407654, test_loss: 698.896912\n",
      "epoch: 16500, train_loss: 1562.748108, test_loss: 709.611206\n",
      "epoch: 16600, train_loss: 1223.863556, test_loss: 731.291260\n",
      "epoch: 16700, train_loss: 2280.373413, test_loss: 714.601135\n",
      "epoch: 16800, train_loss: 2170.604370, test_loss: 718.478882\n",
      "epoch: 16900, train_loss: 1626.064026, test_loss: 716.458435\n",
      "epoch: 17000, train_loss: 1333.770172, test_loss: 628.798706\n",
      "epoch: 17100, train_loss: 2924.899841, test_loss: 699.365479\n",
      "epoch: 17200, train_loss: 1722.017639, test_loss: 674.444885\n",
      "epoch: 17300, train_loss: 1699.173218, test_loss: 678.624023\n",
      "epoch: 17400, train_loss: 1294.107452, test_loss: 632.822144\n",
      "epoch: 17500, train_loss: 1321.028137, test_loss: 694.912109\n",
      "epoch: 17600, train_loss: 2097.396545, test_loss: 632.398010\n",
      "epoch: 17700, train_loss: 1536.775208, test_loss: 691.959290\n",
      "epoch: 17800, train_loss: 2958.720947, test_loss: 662.939209\n",
      "epoch: 17900, train_loss: 1270.210052, test_loss: 611.245789 (Saved)\n",
      "epoch: 18000, train_loss: 2298.841064, test_loss: 714.273987\n",
      "epoch: 18100, train_loss: 1486.113281, test_loss: 643.638611\n",
      "epoch: 18200, train_loss: 1581.868042, test_loss: 622.414917\n",
      "epoch: 18300, train_loss: 1196.879211, test_loss: 619.037598\n",
      "epoch: 18400, train_loss: 1661.549927, test_loss: 625.939331\n",
      "epoch: 18500, train_loss: 1813.260254, test_loss: 637.212463\n",
      "epoch: 18600, train_loss: 1351.452454, test_loss: 597.785522 (Saved)\n",
      "epoch: 18700, train_loss: 1510.141907, test_loss: 541.905579 (Saved)\n",
      "epoch: 18800, train_loss: 1695.451233, test_loss: 663.872498\n",
      "epoch: 18900, train_loss: 1168.685211, test_loss: 579.404602\n",
      "epoch: 19000, train_loss: 1424.965759, test_loss: 677.417114\n",
      "epoch: 19100, train_loss: 1293.068909, test_loss: 572.408569\n",
      "epoch: 19200, train_loss: 1801.432617, test_loss: 603.571960\n",
      "epoch: 19300, train_loss: 1300.492615, test_loss: 611.944458\n",
      "epoch: 19400, train_loss: 2279.964111, test_loss: 601.323059\n",
      "epoch: 19500, train_loss: 1200.510284, test_loss: 569.148743\n",
      "epoch: 19600, train_loss: 1087.558228, test_loss: 631.079895\n",
      "epoch: 19700, train_loss: 1963.652954, test_loss: 617.097534\n",
      "epoch: 19800, train_loss: 2371.735352, test_loss: 630.594055\n",
      "epoch: 19900, train_loss: 1138.584503, test_loss: 586.657288\n",
      "epoch: 20000, train_loss: 1155.531677, test_loss: 550.765503\n",
      "epoch: 20100, train_loss: 1722.652466, test_loss: 557.401794\n",
      "epoch: 20200, train_loss: 1546.523071, test_loss: 603.744080\n",
      "epoch: 20300, train_loss: 1339.754211, test_loss: 512.577148 (Saved)\n",
      "epoch: 20400, train_loss: 946.914291, test_loss: 568.734192\n",
      "epoch: 20500, train_loss: 1232.237305, test_loss: 565.221375\n",
      "epoch: 20600, train_loss: 2395.738464, test_loss: 543.413391\n",
      "epoch: 20700, train_loss: 1124.992279, test_loss: 486.467010 (Saved)\n",
      "epoch: 20800, train_loss: 1160.114288, test_loss: 506.784668\n",
      "epoch: 20900, train_loss: 1254.568054, test_loss: 531.730408\n",
      "epoch: 21000, train_loss: 2290.316406, test_loss: 482.297699 (Saved)\n",
      "epoch: 21100, train_loss: 1116.230194, test_loss: 495.352448\n",
      "epoch: 21200, train_loss: 1503.340515, test_loss: 520.521301\n",
      "epoch: 21300, train_loss: 1315.083191, test_loss: 522.610474\n",
      "epoch: 21400, train_loss: 1217.012329, test_loss: 487.071625\n",
      "epoch: 21500, train_loss: 1232.617676, test_loss: 508.556213\n",
      "epoch: 21600, train_loss: 1020.875488, test_loss: 533.871765\n",
      "epoch: 21700, train_loss: 1058.452393, test_loss: 485.944183\n",
      "epoch: 21800, train_loss: 1610.142334, test_loss: 442.250885 (Saved)\n",
      "epoch: 21900, train_loss: 1472.096619, test_loss: 476.552094\n",
      "epoch: 22000, train_loss: 1229.228943, test_loss: 468.517853\n",
      "epoch: 22100, train_loss: 860.342453, test_loss: 466.976349\n",
      "epoch: 22200, train_loss: 1119.009644, test_loss: 464.245728\n",
      "epoch: 22300, train_loss: 877.154617, test_loss: 550.965088\n",
      "epoch: 22400, train_loss: 1198.804749, test_loss: 395.787323 (Saved)\n",
      "epoch: 22500, train_loss: 809.439087, test_loss: 426.213867\n",
      "epoch: 22600, train_loss: 1262.151917, test_loss: 509.788513\n",
      "epoch: 22700, train_loss: 1171.674194, test_loss: 473.904358\n",
      "epoch: 22800, train_loss: 911.347839, test_loss: 460.372711\n",
      "epoch: 22900, train_loss: 1027.054810, test_loss: 508.141876\n",
      "epoch: 23000, train_loss: 1076.062927, test_loss: 499.374176\n",
      "epoch: 23100, train_loss: 884.721069, test_loss: 413.070831\n",
      "epoch: 23200, train_loss: 1788.359497, test_loss: 398.987549\n",
      "epoch: 23300, train_loss: 932.980652, test_loss: 420.206177\n",
      "epoch: 23400, train_loss: 1694.996857, test_loss: 436.370911\n",
      "epoch: 23500, train_loss: 1022.499359, test_loss: 473.436188\n",
      "epoch: 23600, train_loss: 840.313416, test_loss: 465.738556\n",
      "epoch: 23700, train_loss: 1131.941101, test_loss: 380.586639 (Saved)\n",
      "epoch: 23800, train_loss: 1862.389893, test_loss: 438.674561\n",
      "epoch: 23900, train_loss: 698.026672, test_loss: 394.733826\n",
      "epoch: 24000, train_loss: 896.633575, test_loss: 369.262482 (Saved)\n",
      "epoch: 24100, train_loss: 910.315796, test_loss: 425.537262\n",
      "epoch: 24200, train_loss: 1009.140472, test_loss: 373.591949\n",
      "epoch: 24300, train_loss: 897.228180, test_loss: 394.215729\n",
      "epoch: 24400, train_loss: 1267.348846, test_loss: 402.778168\n",
      "epoch: 24500, train_loss: 1213.357422, test_loss: 353.720276 (Saved)\n",
      "epoch: 24600, train_loss: 733.573273, test_loss: 392.012848\n",
      "epoch: 24700, train_loss: 736.518631, test_loss: 362.557983\n",
      "epoch: 24800, train_loss: 909.456879, test_loss: 441.267426\n",
      "epoch: 24900, train_loss: 848.958069, test_loss: 419.113037\n",
      "epoch: 25000, train_loss: 872.955780, test_loss: 384.772064\n",
      "epoch: 25100, train_loss: 801.009857, test_loss: 354.615753\n",
      "epoch: 25200, train_loss: 1546.440918, test_loss: 336.822418 (Saved)\n",
      "epoch: 25300, train_loss: 888.173462, test_loss: 360.451721\n",
      "epoch: 25400, train_loss: 756.367889, test_loss: 353.564972\n",
      "epoch: 25500, train_loss: 643.859512, test_loss: 364.183685\n",
      "epoch: 25600, train_loss: 799.303680, test_loss: 324.711426 (Saved)\n",
      "epoch: 25700, train_loss: 1442.951965, test_loss: 342.116913\n",
      "epoch: 25800, train_loss: 681.886200, test_loss: 302.062653 (Saved)\n",
      "epoch: 25900, train_loss: 765.928467, test_loss: 312.830780\n",
      "epoch: 26000, train_loss: 781.409058, test_loss: 312.282776\n",
      "epoch: 26100, train_loss: 1322.877930, test_loss: 318.244812\n",
      "epoch: 26200, train_loss: 979.039032, test_loss: 289.965393 (Saved)\n",
      "epoch: 26300, train_loss: 761.691406, test_loss: 292.906921\n",
      "epoch: 26400, train_loss: 700.043915, test_loss: 315.070221\n",
      "epoch: 26500, train_loss: 812.983215, test_loss: 311.015656\n",
      "epoch: 26600, train_loss: 668.606842, test_loss: 285.173676 (Saved)\n",
      "epoch: 26700, train_loss: 1536.261292, test_loss: 276.872162 (Saved)\n",
      "epoch: 26800, train_loss: 612.270889, test_loss: 362.350037\n",
      "epoch: 26900, train_loss: 679.099701, test_loss: 269.818054 (Saved)\n",
      "epoch: 27000, train_loss: 816.874939, test_loss: 282.206177\n",
      "epoch: 27100, train_loss: 500.375580, test_loss: 291.997284\n",
      "epoch: 27200, train_loss: 482.962402, test_loss: 278.055817\n",
      "epoch: 27300, train_loss: 713.151459, test_loss: 275.707245\n",
      "epoch: 27400, train_loss: 640.290497, test_loss: 299.466766\n",
      "epoch: 27500, train_loss: 539.942719, test_loss: 270.070160\n",
      "epoch: 27600, train_loss: 503.544464, test_loss: 240.772919 (Saved)\n",
      "epoch: 27700, train_loss: 454.986305, test_loss: 260.877655\n",
      "epoch: 27800, train_loss: 572.280197, test_loss: 220.921860 (Saved)\n",
      "epoch: 27900, train_loss: 781.214783, test_loss: 227.201202\n",
      "epoch: 28000, train_loss: 731.444611, test_loss: 252.683807\n",
      "epoch: 28100, train_loss: 624.046417, test_loss: 280.454193\n",
      "epoch: 28200, train_loss: 619.548126, test_loss: 239.778534\n",
      "epoch: 28300, train_loss: 460.598251, test_loss: 225.617310\n",
      "epoch: 28400, train_loss: 577.158813, test_loss: 239.614578\n",
      "epoch: 28500, train_loss: 661.625885, test_loss: 217.549911 (Saved)\n",
      "epoch: 28600, train_loss: 551.314880, test_loss: 250.412384\n",
      "epoch: 28700, train_loss: 942.652130, test_loss: 225.317245\n",
      "epoch: 28800, train_loss: 434.486961, test_loss: 211.008392 (Saved)\n",
      "epoch: 28900, train_loss: 472.653168, test_loss: 212.000977\n",
      "epoch: 29000, train_loss: 502.954910, test_loss: 197.812714 (Saved)\n",
      "epoch: 29100, train_loss: 457.415665, test_loss: 215.709320\n",
      "epoch: 29200, train_loss: 714.464752, test_loss: 171.898026 (Saved)\n",
      "epoch: 29300, train_loss: 956.684540, test_loss: 195.111633\n",
      "epoch: 29400, train_loss: 400.262726, test_loss: 179.930618\n",
      "epoch: 29500, train_loss: 648.778168, test_loss: 171.062851 (Saved)\n",
      "epoch: 29600, train_loss: 445.862320, test_loss: 195.624680\n",
      "epoch: 29700, train_loss: 480.330338, test_loss: 157.589264 (Saved)\n",
      "epoch: 29800, train_loss: 434.135803, test_loss: 160.237137\n",
      "epoch: 29900, train_loss: 403.908936, test_loss: 177.631180\n",
      "epoch: 30000, train_loss: 336.864578, test_loss: 180.185898\n",
      "epoch: 30100, train_loss: 315.999981, test_loss: 157.585419 (Saved)\n",
      "epoch: 30200, train_loss: 423.056915, test_loss: 159.683899\n",
      "epoch: 30300, train_loss: 395.300827, test_loss: 168.234390\n",
      "epoch: 30400, train_loss: 445.180374, test_loss: 156.959427 (Saved)\n",
      "epoch: 30500, train_loss: 466.880325, test_loss: 144.846603 (Saved)\n",
      "epoch: 30600, train_loss: 727.893509, test_loss: 157.698486\n",
      "epoch: 30700, train_loss: 466.646332, test_loss: 146.294510\n",
      "epoch: 30800, train_loss: 441.865814, test_loss: 156.642410\n",
      "epoch: 30900, train_loss: 429.709030, test_loss: 132.063934 (Saved)\n",
      "epoch: 31000, train_loss: 394.988495, test_loss: 126.099648 (Saved)\n",
      "epoch: 31100, train_loss: 463.247467, test_loss: 113.222038 (Saved)\n",
      "epoch: 31200, train_loss: 340.572144, test_loss: 109.648399 (Saved)\n",
      "epoch: 31300, train_loss: 603.190918, test_loss: 133.940720\n",
      "epoch: 31400, train_loss: 296.072571, test_loss: 114.942726\n",
      "epoch: 31500, train_loss: 353.931610, test_loss: 142.957214\n",
      "epoch: 31600, train_loss: 293.942421, test_loss: 147.695724\n",
      "epoch: 31700, train_loss: 600.611786, test_loss: 116.485878\n",
      "epoch: 31800, train_loss: 581.169159, test_loss: 115.989624\n",
      "epoch: 31900, train_loss: 314.084015, test_loss: 97.558342 (Saved)\n",
      "epoch: 32000, train_loss: 655.889702, test_loss: 104.755447\n",
      "epoch: 32100, train_loss: 296.166016, test_loss: 92.362267 (Saved)\n",
      "epoch: 32200, train_loss: 282.322556, test_loss: 97.299950\n",
      "epoch: 32300, train_loss: 315.121765, test_loss: 110.228722\n",
      "epoch: 32400, train_loss: 228.980648, test_loss: 96.190414\n",
      "epoch: 32500, train_loss: 245.558350, test_loss: 115.083878\n",
      "epoch: 32600, train_loss: 190.784136, test_loss: 104.531029\n",
      "epoch: 32700, train_loss: 253.057335, test_loss: 89.182121 (Saved)\n",
      "epoch: 32800, train_loss: 549.535156, test_loss: 79.136772 (Saved)\n",
      "epoch: 32900, train_loss: 197.553440, test_loss: 85.043556\n",
      "epoch: 33000, train_loss: 205.371071, test_loss: 74.634995 (Saved)\n",
      "epoch: 33100, train_loss: 207.674175, test_loss: 74.781296\n",
      "epoch: 33200, train_loss: 442.921371, test_loss: 83.566010\n",
      "epoch: 33300, train_loss: 263.472702, test_loss: 84.100929\n",
      "epoch: 33400, train_loss: 180.919556, test_loss: 94.228722\n",
      "epoch: 33500, train_loss: 202.320873, test_loss: 74.475082 (Saved)\n",
      "epoch: 33600, train_loss: 200.265446, test_loss: 71.078453 (Saved)\n",
      "epoch: 33700, train_loss: 274.984863, test_loss: 73.776031\n",
      "epoch: 33800, train_loss: 300.025818, test_loss: 88.157822\n",
      "epoch: 33900, train_loss: 191.851559, test_loss: 78.905617\n",
      "epoch: 34000, train_loss: 271.511490, test_loss: 71.380280\n",
      "epoch: 34100, train_loss: 183.567383, test_loss: 75.191208\n",
      "epoch: 34200, train_loss: 216.614990, test_loss: 72.955421\n",
      "epoch: 34300, train_loss: 163.420128, test_loss: 66.582382 (Saved)\n",
      "epoch: 34400, train_loss: 209.132996, test_loss: 66.709366\n",
      "epoch: 34500, train_loss: 213.004936, test_loss: 67.852257\n",
      "epoch: 34600, train_loss: 155.279026, test_loss: 64.920929 (Saved)\n",
      "epoch: 34700, train_loss: 161.444515, test_loss: 63.926495 (Saved)\n",
      "epoch: 34800, train_loss: 161.638374, test_loss: 67.763748\n",
      "epoch: 34900, train_loss: 298.313629, test_loss: 75.871239\n",
      "epoch: 35000, train_loss: 336.180145, test_loss: 66.092903\n",
      "epoch: 35100, train_loss: 228.432510, test_loss: 61.127705 (Saved)\n",
      "epoch: 35200, train_loss: 144.133331, test_loss: 70.828781\n",
      "epoch: 35300, train_loss: 192.638229, test_loss: 52.332108 (Saved)\n",
      "epoch: 35400, train_loss: 161.739983, test_loss: 57.540363\n",
      "epoch: 35500, train_loss: 291.172264, test_loss: 68.322914\n",
      "epoch: 35600, train_loss: 302.873642, test_loss: 59.590385\n",
      "epoch: 35700, train_loss: 219.174019, test_loss: 55.535530\n",
      "epoch: 35800, train_loss: 136.916832, test_loss: 50.117115 (Saved)\n",
      "epoch: 35900, train_loss: 151.045471, test_loss: 55.802723\n",
      "epoch: 36000, train_loss: 159.354942, test_loss: 55.411877\n",
      "epoch: 36100, train_loss: 234.283691, test_loss: 49.936844 (Saved)\n",
      "epoch: 36200, train_loss: 140.358524, test_loss: 50.727802\n",
      "epoch: 36300, train_loss: 173.487495, test_loss: 48.395260 (Saved)\n",
      "epoch: 36400, train_loss: 162.274178, test_loss: 53.757671\n",
      "epoch: 36500, train_loss: 157.026482, test_loss: 53.756588\n",
      "epoch: 36600, train_loss: 153.273857, test_loss: 51.879658\n",
      "epoch: 36700, train_loss: 235.128845, test_loss: 51.531376\n",
      "epoch: 36800, train_loss: 145.449039, test_loss: 58.229397\n",
      "epoch: 36900, train_loss: 161.838158, test_loss: 51.468342\n",
      "epoch: 37000, train_loss: 155.414658, test_loss: 52.629688\n",
      "epoch: 37100, train_loss: 231.030838, test_loss: 54.333759\n",
      "epoch: 37200, train_loss: 248.325020, test_loss: 54.260796\n",
      "epoch: 37300, train_loss: 148.021141, test_loss: 50.182720\n",
      "epoch: 37400, train_loss: 131.438805, test_loss: 48.797901\n",
      "epoch: 37500, train_loss: 134.935886, test_loss: 50.297577\n",
      "epoch: 37600, train_loss: 191.753799, test_loss: 46.267551 (Saved)\n",
      "epoch: 37700, train_loss: 139.516460, test_loss: 48.224968\n",
      "epoch: 37800, train_loss: 149.531364, test_loss: 48.879780\n",
      "epoch: 37900, train_loss: 129.543453, test_loss: 50.240421\n",
      "epoch: 38000, train_loss: 228.284374, test_loss: 46.844124\n",
      "epoch: 38100, train_loss: 156.471100, test_loss: 48.805061\n",
      "epoch: 38200, train_loss: 124.424793, test_loss: 46.085957 (Saved)\n",
      "epoch: 38300, train_loss: 115.913765, test_loss: 53.617325\n",
      "epoch: 38400, train_loss: 157.430923, test_loss: 48.939461\n",
      "epoch: 38500, train_loss: 113.858109, test_loss: 46.034473 (Saved)\n",
      "epoch: 38600, train_loss: 197.749771, test_loss: 49.372601\n",
      "epoch: 38700, train_loss: 186.110153, test_loss: 52.642025\n",
      "epoch: 38800, train_loss: 147.417442, test_loss: 50.059464\n",
      "epoch: 38900, train_loss: 131.307224, test_loss: 50.022346\n",
      "epoch: 39000, train_loss: 121.289513, test_loss: 51.919582\n",
      "epoch: 39100, train_loss: 122.957470, test_loss: 49.336658\n",
      "epoch: 39200, train_loss: 136.393295, test_loss: 48.092083\n",
      "epoch: 39300, train_loss: 122.251942, test_loss: 46.692707\n",
      "epoch: 39400, train_loss: 144.058807, test_loss: 46.528580\n",
      "epoch: 39500, train_loss: 117.114712, test_loss: 46.510868\n",
      "epoch: 39600, train_loss: 157.102333, test_loss: 51.934101\n",
      "epoch: 39700, train_loss: 131.758209, test_loss: 48.670937\n",
      "epoch: 39800, train_loss: 212.417732, test_loss: 47.658768\n",
      "epoch: 39900, train_loss: 150.244476, test_loss: 47.244682\n",
      "epoch: 40000, train_loss: 179.249947, test_loss: 48.951283\n",
      "epoch: 40100, train_loss: 114.283855, test_loss: 46.923855\n",
      "epoch: 40200, train_loss: 128.231060, test_loss: 48.233597\n",
      "epoch: 40300, train_loss: 118.293907, test_loss: 44.460377 (Saved)\n",
      "epoch: 40400, train_loss: 138.487305, test_loss: 45.316929\n",
      "epoch: 40500, train_loss: 118.986507, test_loss: 49.470119\n",
      "epoch: 40600, train_loss: 118.746590, test_loss: 49.502003\n",
      "epoch: 40700, train_loss: 161.633125, test_loss: 48.101772\n",
      "epoch: 40800, train_loss: 122.154514, test_loss: 48.397316\n",
      "epoch: 40900, train_loss: 108.582623, test_loss: 49.578724\n",
      "epoch: 41000, train_loss: 96.476770, test_loss: 47.599094\n",
      "epoch: 41100, train_loss: 99.739353, test_loss: 49.350639\n",
      "epoch: 41200, train_loss: 95.815176, test_loss: 47.373032\n",
      "epoch: 41300, train_loss: 118.867191, test_loss: 48.865772\n",
      "epoch: 41400, train_loss: 97.010374, test_loss: 48.670891\n",
      "epoch: 41500, train_loss: 107.283192, test_loss: 49.153244\n",
      "epoch: 41600, train_loss: 120.103340, test_loss: 46.729038\n",
      "epoch: 41700, train_loss: 102.330456, test_loss: 45.965473\n",
      "epoch: 41800, train_loss: 113.030060, test_loss: 45.478973\n",
      "epoch: 41900, train_loss: 111.281971, test_loss: 50.648060\n",
      "epoch: 42000, train_loss: 108.129990, test_loss: 48.227615\n",
      "epoch: 42100, train_loss: 113.277859, test_loss: 46.756535\n",
      "epoch: 42200, train_loss: 123.360104, test_loss: 46.583111\n",
      "epoch: 42300, train_loss: 113.615162, test_loss: 46.931843\n",
      "epoch: 42400, train_loss: 150.817467, test_loss: 50.244892\n",
      "epoch: 42500, train_loss: 94.995296, test_loss: 44.871090\n",
      "epoch: 42600, train_loss: 116.578602, test_loss: 48.774746\n",
      "epoch: 42700, train_loss: 154.041534, test_loss: 49.016319\n",
      "epoch: 42800, train_loss: 145.656349, test_loss: 47.574856\n",
      "epoch: 42900, train_loss: 110.508423, test_loss: 49.619400\n",
      "epoch: 43000, train_loss: 90.412180, test_loss: 45.580532\n",
      "epoch: 43100, train_loss: 96.777058, test_loss: 48.366325\n",
      "epoch: 43200, train_loss: 97.699211, test_loss: 47.805630\n",
      "epoch: 43300, train_loss: 101.004124, test_loss: 46.371391\n",
      "epoch: 43400, train_loss: 105.990318, test_loss: 46.598160\n",
      "epoch: 43500, train_loss: 159.044106, test_loss: 49.306995\n",
      "epoch: 43600, train_loss: 104.518269, test_loss: 46.958851\n",
      "epoch: 43700, train_loss: 135.631954, test_loss: 47.128651\n",
      "epoch: 43800, train_loss: 97.180172, test_loss: 47.357071\n",
      "epoch: 43900, train_loss: 97.686058, test_loss: 50.426033\n",
      "epoch: 44000, train_loss: 101.215961, test_loss: 48.239250\n",
      "epoch: 44100, train_loss: 115.416386, test_loss: 46.179722\n",
      "epoch: 44200, train_loss: 96.094063, test_loss: 49.030052\n",
      "epoch: 44300, train_loss: 91.626972, test_loss: 46.785263\n",
      "epoch: 44400, train_loss: 85.538336, test_loss: 45.620823\n",
      "epoch: 44500, train_loss: 111.535782, test_loss: 44.682442\n",
      "epoch: 44600, train_loss: 112.674080, test_loss: 46.239365\n",
      "epoch: 44700, train_loss: 104.470646, test_loss: 47.356640\n",
      "epoch: 44800, train_loss: 95.787949, test_loss: 47.232414\n",
      "epoch: 44900, train_loss: 99.990314, test_loss: 46.578785\n",
      "epoch: 45000, train_loss: 114.702610, test_loss: 48.002827\n",
      "epoch: 45100, train_loss: 94.785767, test_loss: 48.509975\n",
      "epoch: 45200, train_loss: 85.871237, test_loss: 44.969875\n",
      "epoch: 45300, train_loss: 118.481613, test_loss: 44.856907\n",
      "epoch: 45400, train_loss: 107.787918, test_loss: 46.865055\n",
      "epoch: 45500, train_loss: 92.518803, test_loss: 46.625610\n",
      "epoch: 45600, train_loss: 107.122917, test_loss: 45.038994\n",
      "epoch: 45700, train_loss: 115.641502, test_loss: 47.460415\n",
      "epoch: 45800, train_loss: 164.651424, test_loss: 48.032341\n",
      "epoch: 45900, train_loss: 86.688557, test_loss: 46.344738\n",
      "epoch: 46000, train_loss: 121.417343, test_loss: 46.535625\n",
      "epoch: 46100, train_loss: 148.386780, test_loss: 44.938766\n",
      "epoch: 46200, train_loss: 106.824989, test_loss: 48.292656\n",
      "epoch: 46300, train_loss: 89.454720, test_loss: 46.653980\n",
      "epoch: 46400, train_loss: 142.919487, test_loss: 44.646927\n",
      "epoch: 46500, train_loss: 125.893890, test_loss: 43.752590 (Saved)\n",
      "epoch: 46600, train_loss: 127.527138, test_loss: 46.368690\n",
      "epoch: 46700, train_loss: 68.575795, test_loss: 45.010788\n",
      "epoch: 46800, train_loss: 104.428242, test_loss: 46.899055\n",
      "epoch: 46900, train_loss: 125.053665, test_loss: 46.357086\n",
      "epoch: 47000, train_loss: 98.983341, test_loss: 44.585640\n",
      "epoch: 47100, train_loss: 85.237709, test_loss: 45.603062\n",
      "epoch: 47200, train_loss: 97.099083, test_loss: 44.979698\n",
      "epoch: 47300, train_loss: 105.442345, test_loss: 46.301987\n",
      "epoch: 47400, train_loss: 99.703434, test_loss: 46.770390\n",
      "epoch: 47500, train_loss: 96.620548, test_loss: 43.835529\n",
      "epoch: 47600, train_loss: 84.125725, test_loss: 45.787994\n",
      "epoch: 47700, train_loss: 83.308376, test_loss: 45.900707\n",
      "epoch: 47800, train_loss: 88.037167, test_loss: 44.572926\n",
      "epoch: 47900, train_loss: 89.364967, test_loss: 46.318291\n",
      "epoch: 48000, train_loss: 95.931004, test_loss: 43.195755 (Saved)\n",
      "epoch: 48100, train_loss: 74.801893, test_loss: 46.798481\n",
      "epoch: 48200, train_loss: 90.235271, test_loss: 43.383476\n",
      "epoch: 48300, train_loss: 71.302509, test_loss: 47.930225\n",
      "epoch: 48400, train_loss: 97.255470, test_loss: 44.462891\n",
      "epoch: 48500, train_loss: 81.592899, test_loss: 42.181751 (Saved)\n",
      "epoch: 48600, train_loss: 143.730530, test_loss: 42.514507\n",
      "epoch: 48700, train_loss: 129.109524, test_loss: 52.858936\n",
      "epoch: 48800, train_loss: 76.506226, test_loss: 45.098808\n",
      "epoch: 48900, train_loss: 105.463303, test_loss: 45.300228\n",
      "epoch: 49000, train_loss: 83.431530, test_loss: 39.531021 (Saved)\n",
      "epoch: 49100, train_loss: 72.419176, test_loss: 45.252277\n",
      "epoch: 49200, train_loss: 86.615582, test_loss: 45.536106\n",
      "epoch: 49300, train_loss: 93.596806, test_loss: 43.336304\n",
      "epoch: 49400, train_loss: 100.952030, test_loss: 43.183861\n",
      "epoch: 49500, train_loss: 92.072056, test_loss: 43.377789\n",
      "epoch: 49600, train_loss: 95.573856, test_loss: 42.509632\n",
      "epoch: 49700, train_loss: 83.472473, test_loss: 45.012287\n",
      "epoch: 49800, train_loss: 81.405464, test_loss: 47.978077\n",
      "epoch: 49900, train_loss: 84.744175, test_loss: 43.758102\n"
     ]
    }
   ],
   "source": [
    "min_test_loss = np.inf\n",
    "\n",
    "for epoch in range(50000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        save_model = False\n",
    "        if test_loss < min_test_loss:\n",
    "            min_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            save_model = True\n",
    "\n",
    "        if save_model:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f} (Saved)'.format(epoch, train_loss, test_loss))\n",
    "        else:\n",
    "            print('epoch: {}, train_loss: {:.6f}, test_loss: {:.6f}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX path: /home/protanjung/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Saving ONNX model: /home/protanjung/icar-ng-data/model/cm2pixel_model.onnx\n",
      "Exported graph: graph(%onnx::Sub_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(4, 2, strides=[2, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(16, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.weight : Float(64, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc3.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.weight : Float(16, 64, strides=[64, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc4.bias : Float(16, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.weight : Float(4, 16, strides=[16, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc5.bias : Float(4, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.weight : Float(2, 4, strides=[4, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/Constant_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 0 -400 [ CUDAFloatType{2} ], onnx_name=\"/Constant\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:16:0\n",
      "  %/Sub_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Sub[onnx_name=\"/Sub\"](%onnx::Sub_0, %/Constant_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:16:0\n",
      "  %/Constant_1_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 300  850 [ CUDAFloatType{2} ], onnx_name=\"/Constant_1\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:16:0\n",
      "  %/Div_output_0 : Float(1, 2, strides=[2, 1], requires_grad=0, device=cuda:0) = onnx::Div[onnx_name=\"/Div\"](%/Sub_output_0, %/Constant_1_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:16:0\n",
      "  %/fc1/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Div_output_0, %fc1.weight, %fc1.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc1 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh\"](%/fc1/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Tanh_output_0, %fc2.weight, %fc2.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc2 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_1_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_1\"](%/fc2/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc3/Gemm_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Tanh_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc3 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Tanh_2_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = onnx::Tanh[onnx_name=\"/Tanh_2\"](%/fc3/Gemm_output_0), scope: __main__.MultiLayerPerceptron:: # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:1958:0\n",
      "  %/fc4/Gemm_output_0 : Float(1, 16, strides=[16, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc4/Gemm\"](%/Tanh_2_output_0, %fc4.weight, %fc4.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc4 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc5/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc5/Gemm\"](%/fc4/Gemm_output_0, %fc5.weight, %fc5.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc5 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/fc6/Gemm_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc6/Gemm\"](%/fc5/Gemm_output_0, %fc6.weight, %fc6.bias), scope: __main__.MultiLayerPerceptron::/torch.nn.modules.linear.Linear::fc6 # /home/protanjung/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_2_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value= 1192   210 [ CUDAFloatType{2} ], onnx_name=\"/Constant_2\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:23:0\n",
      "  %/Mul_output_0 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Mul[onnx_name=\"/Mul\"](%/fc6/Gemm_output_0, %/Constant_2_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:23:0\n",
      "  %/Constant_3_output_0 : Float(2, strides=[1], requires_grad=0, device=cuda:0) = onnx::Constant[value=  60  274 [ CUDAFloatType{2} ], onnx_name=\"/Constant_3\"](), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:23:0\n",
      "  %29 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/Add\"](%/Mul_output_0, %/Constant_3_output_0), scope: __main__.MultiLayerPerceptron:: # /tmp/ipykernel_123029/541484460.py:23:0\n",
      "  return (%29)\n",
      "\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Saved ONNX model: /home/protanjung/icar-ng-data/model/cm2pixel_model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = os.path.join(model_directory, 'cm2pixel_model.onnx')\n",
    "print('ONNX path: {}'.format(onnx_path))\n",
    "\n",
    "try:\n",
    "    print('Saving ONNX model: {}'.format(onnx_path))\n",
    "    torch.onnx.export(model, torch.randn(1, 2).to(device), onnx_path, verbose=True)\n",
    "    print('Saved ONNX model: {}'.format(onnx_path))\n",
    "except BaseException as e:\n",
    "    print('Failed to save ONNX model: {}'.format(onnx_path))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "tensor([[ 150.,  100.],\n",
      "        [ 100., -100.],\n",
      "        [ 300., -400.],\n",
      "        [ 300., -450.],\n",
      "        [  50., -100.],\n",
      "        [ 200., -200.],\n",
      "        [ 250.,   50.],\n",
      "        [ 150., -100.],\n",
      "        [  50.,    0.],\n",
      "        [ 150.,  150.],\n",
      "        [ 150., -150.],\n",
      "        [ 250., -300.],\n",
      "        [ 300., -100.],\n",
      "        [ 200.,    0.],\n",
      "        [ 200.,  150.],\n",
      "        [ 200.,  100.],\n",
      "        [ 150., -200.],\n",
      "        [   0.,    0.]], device='cuda:0')\n",
      "y\n",
      "tensor([[ 452.,  344.],\n",
      "        [ 870.,  362.],\n",
      "        [1190.,  274.],\n",
      "        [1262.,  276.],\n",
      "        [ 918.,  408.],\n",
      "        [ 982.,  310.],\n",
      "        [ 568.,  304.],\n",
      "        [ 838.,  332.],\n",
      "        [ 642.,  408.],\n",
      "        [ 358.,  348.],\n",
      "        [ 934.,  330.],\n",
      "        [1094.,  290.],\n",
      "        [ 780.,  276.],\n",
      "        [ 642.,  308.],\n",
      "        [ 394.,  324.],\n",
      "        [ 474.,  324.],\n",
      "        [1032.,  332.],\n",
      "        [ 642.,  478.]], device='cuda:0')\n",
      "y_pred\n",
      "tensor([[ 446.3451,  345.9088],\n",
      "        [ 873.2797,  369.6592],\n",
      "        [1190.5955,  275.1100],\n",
      "        [1258.7621,  275.0930],\n",
      "        [ 928.9549,  411.9452],\n",
      "        [ 975.4335,  308.8009],\n",
      "        [ 559.7334,  297.8846],\n",
      "        [ 828.1193,  336.5347],\n",
      "        [ 661.2014,  418.5247],\n",
      "        [ 353.0608,  348.6677],\n",
      "        [ 927.2223,  334.6743],\n",
      "        [1096.1206,  289.1326],\n",
      "        [ 781.2914,  279.1743],\n",
      "        [ 629.2072,  315.1523],\n",
      "        [ 382.9817,  321.6819],\n",
      "        [ 463.8359,  319.3645],\n",
      "        [1027.8458,  333.0319],\n",
      "        [ 647.4142,  469.5945]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "y - y_pred\n",
      "tensor([[  5.6549,  -1.9088],\n",
      "        [ -3.2797,  -7.6592],\n",
      "        [ -0.5955,  -1.1100],\n",
      "        [  3.2379,   0.9070],\n",
      "        [-10.9549,  -3.9452],\n",
      "        [  6.5665,   1.1991],\n",
      "        [  8.2666,   6.1154],\n",
      "        [  9.8807,  -4.5347],\n",
      "        [-19.2014, -10.5247],\n",
      "        [  4.9392,  -0.6677],\n",
      "        [  6.7777,  -4.6743],\n",
      "        [ -2.1206,   0.8674],\n",
      "        [ -1.2914,  -3.1743],\n",
      "        [ 12.7928,  -7.1523],\n",
      "        [ 11.0183,   2.3181],\n",
      "        [ 10.1641,   4.6355],\n",
      "        [  4.1542,  -1.0319],\n",
      "        [ -5.4142,   8.4055]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (x, y) in enumerate(dataloader_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print('x\\n{}'.format(x))\n",
    "    print('y\\n{}'.format(y))\n",
    "    print('y_pred\\n{}'.format(y_pred))\n",
    "    print('y - y_pred\\n{}'.format(y - y_pred))\n",
    "    \n",
    "    break\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
